supabase
Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Overview
Database

Every Supabase project comes with a full Postgres database, a free and open source database which is considered one of the world's most stable and advanced databases.

Features#
Table view#
You don't have to be a database expert to start using Supabase. Our table view makes Postgres as easy to use as a spreadsheet.

Table View.

Relationships#
Dig into the relationships within your data.

Clone tables#
You can duplicate your tables, just like you would inside a spreadsheet.

The SQL editor#
Supabase comes with a SQL Editor. You can also save your favorite queries to run later!

Additional features#
Supabase extends Postgres with realtime functionality using our Realtime Server.
Every project is a full Postgres database, with postgres level access.
Supabase manages your database backups.
Import data directly from a CSV or excel spreadsheet.
Database backups do not include objects stored via the Storage API, as the database only includes metadata about these objects. Restoring an old backup does not restore objects that have been deleted since then.

Extensions#
To expand the functionality of your Postgres database, you can use extensions.
You can enable Postgres extensions with the click of a button within the Supabase dashboard.

Learn more about all the extensions provided on Supabase.

Terminology#
Postgres or PostgreSQL?#
PostgreSQL the database was derived from the POSTGRES Project, a package written at the University of California at Berkeley in 1986. This package included a query language called "PostQUEL".

In 1994, Postgres95 was built on top of POSTGRES code, adding an SQL language interpreter as a replacement for PostQUEL.

Eventually, Postgres95 was renamed to PostgreSQL to reflect the SQL query capability.
After this, many people referred to it as Postgres since it's less prone to confusion. Supabase is all about simplicity, so we also refer to it as Postgres.

Tips#
Read about resetting your database password here and changing the timezone of your server here.

Next steps#
Read more about Postgres
Sign in: supabase.com/dashboard
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Features
Table view
Relationships
Clone tables
The SQL editor
Additional features
Extensions
Terminology
Postgres or PostgreSQL?
Tips
Next steps
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Database | Supabase Docs
We use cookies to collect data and improve our services. Learn more


Accept

Opt out
Privacy settings

Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Fundamentals
Connecting to your database
Connect to your database

Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions.

How to connect to your Postgres databases#
How you connect to your database depends on where you're connecting from:

For frontend applications, use the Data API
For Postgres clients, use a connection string
For single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications like pg_dump or specifying connections for replication) use the direct connection string if your environment supports IPv6
For persistent clients, and support for both IPv4 and IPv6, use pooler session mode
For temporary clients (for example, serverless or edge functions) use pooler transaction mode
Quickstarts#
Prisma

Drizzle

Postgres.js

pgAdmin

PSQL

DBeaver

Metabase

Beekeeper Studio

Data APIs and client libraries#
The Data APIs allow you to interact with your database using REST or GraphQL requests. You can use these APIs to fetch and insert data from the frontend, as long as you have RLS enabled.

REST
GraphQL
For convenience, you can also use the Supabase client libraries, which wrap the Data APIs with a developer-friendly interface and automatically handle authentication:

JavaScript
Flutter
Swift
Python
C#
Kotlin
Direct connection#
The direct connection string connects directly to your Postgres instance. It is ideal for persistent servers, such as virtual machines (VMs) and long-lasting containers. Examples include AWS EC2 machines, Fly.io VMs, and DigitalOcean Droplets.

Direct connections use IPv6 by default. If your environment doesn't support IPv6, use Supavisor session mode or get the IPv4 add-on.

The connection string looks like this:

postgresql://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:5432/postgres
Get your project's direct connection string from your project dashboard by clicking Connect.

Poolers#
Every Supabase project includes a connection pooler. This is ideal for persistent servers when IPv6 is not supported.

Pooler session mode#
The session mode connection string connects to your Postgres instance via a proxy. This is only recommended as an alternative to a Direct Connection, when connecting via an IPv4 network.

The connection string looks like this:

postgres://postgres.apbkobhfnmcqqzqeeqss:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres
Get your project's Session pooler connection string from your project dashboard by clicking Connect.

Pooler transaction mode#
The transaction mode connection string connects to your Postgres instance via a proxy which serves as a connection pooler. This is ideal for serverless or edge functions, which require many transient connections.

Transaction mode does not support prepared statements. To avoid errors, turn off prepared statements for your connection library.

The connection string looks like this:

postgres://postgres:[YOUR-PASSWORD]@db.abcdefghijklmnopqrst.supabase.co:6543/postgres
Get your project's Transaction pooler connection string from your project dashboard by clicking Connect.

Dedicated pooler#
For paying customers, we provision a Dedicated Pooler (PgBouncer) that's co-located with your Postgres database. This will require you to connect with IPv6 or, if that's not an option, you can use the IPv4 add-on.

The Dedicated Pooler ensures best performance and latency, while using up more of your project's compute resources. If your network supports IPv6 or you have the IPv4 add-on, we encourage you to use the Dedicated Pooler over the Shared Pooler.

Get your project's Dedicated pooler connection string from your project dashboard by clicking Connect.

PgBouncer always runs in Transaction mode and the current version does not support prepared statement (will be added in a few weeks).

More about connection pooling#
Connection pooling improves database performance by reusing existing connections between queries. This reduces the overhead of establishing connections and improves scalability.

You can use an application-side pooler or a server-side pooler (Supabase automatically provides one called Supavisor), depending on whether your backend is persistent or serverless.

Application-side poolers#
Application-side poolers are built into connection libraries and API servers, such as Prisma, SQLAlchemy, and PostgREST. They maintain several active connections with Postgres or a server-side pooler, reducing the overhead of establishing connections between queries. When deploying to static architecture, such as long-standing containers or VMs, application-side poolers are satisfactory on their own.

Serverside poolers#
Postgres connections are like a WebSocket. Once established, they are preserved until the client (application server) disconnects. A server might only make a single 10 ms query, but needlessly reserve its database connection for seconds or longer.

Serverside-poolers, such as Supabase's Supavisor in transaction mode, sit between clients and the database and can be thought of as load balancers for Postgres connections.

New migration files trigger migrations on the preview instance.
Connecting to the database directly vs using a Connection Pooler
They maintain hot connections with the database and intelligently share them with clients only when needed, maximizing the amount of queries a single connection can service. They're best used to manage queries from auto-scaling systems, such as edge and serverless functions.

Connecting with SSL#
You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.

You can obtain your connection info and Server root certificate from your application's dashboard:

Connection Info and Certificate.

Resources#
Connection management
Connecting with psql
Importing data into Supabase
Troubleshooting and Postgres connection string FAQs#
Below are answers to common challenges and queries.

What is a “connection refused” error?#
A “Connection refused” error typically means your database isn’t reachable. Ensure your Supabase project is running, confirm your database’s connection string, check firewall settings, and validate network permissions.

What is the “FATAL: Password authentication failed” error?#
This error occurs when your credentials are incorrect. Double-check your username and password from the Supabase dashboard. If the problem persists, reset your database password from the project settings.

How do you connect using IPv4?#
Supabase’s default direct connection supports IPv6 only. To connect over IPv4, consider using the Supavisor session or transaction modes, or a connection pooler (shared or dedicated), which support both IPv4 and IPv6.

Where is the Postgres connection string in Supabase?#
Your connection string is located in the Supabase Dashboard. Click the Connect button at the top of the page.

Can you use Supavisor and PgBouncer together?#
You can technically use both, but it’s not recommended unless you’re specifically trying to increase the total number of concurrent client connections. In most cases, it is better to choose either PgBouncer or Supavisor for pooled or transaction-based traffic. Direct connections remain the best choice for long-lived sessions, and, if IPv4 is required for those sessions, Supavisor session mode can be used as an alternative. Running both poolers simultaneously increases the risk of hitting your database’s maximum connection limit on smaller compute tiers.

How does the default pool size work?#
Supavisor and PgBouncer work independently, but both reference the same pool size setting. For example, if you set the pool size to 30, Supavisor can open up to 30 server-side connections to Postgres each for its session mode port (5432) and transaction mode port (6543), and PgBouncer can also open up to 30. If both poolers are active and reach their roles/modes limits at the same time, you could have as many as 60 backend connections hitting your database, in addition to any direct connections. You can adjust the pool size in Database settings in the dashboard.

What is the difference between client connections and backend connections?#
There are two different limits to understand when working with poolers. The first is client connections, which refers to how many clients can connect to a pooler at the same time. This number is capped by your compute tier’s “max pooler clients” limit, and it applies independently to Supavisor and PgBouncer. The second is backend connections, which is the number of active connections a pooler opens to Postgres. This number is set by the pool size for that pooler.

Total backend load on Postgres =
 Direct connections +
 Supavisor backend connections (≤ supavisor_pool_size) +
 PgBouncer backend connections (≤ pgbouncer_pool_size)
≤ Postgres max connections for your compute instance
What is the max pooler clients limit?#
The “max pooler clients” limit for your compute tier applies separately to Supavisor and PgBouncer. One pooler reaching its client limit does not affect the other. When a pooler reaches this limit, it stops accepting new client connections until existing ones are closed, but the other pooler remains unaffected. You can check your tier’s connection limits in the compute and disk limits documentation.

Where can you see current connection usage?#
You can track connection usage from the Reports section in your project dashboard. There are three key reports:

Database Connections: shows total active connections by role (this includes direct and pooled connections).
Dedicated Pooler Client Connections: shows the number of active client connections to PgBouncer.
Shared Pooler (Supavisor) Client Connections: shows the number of active client connections to Supavisor.
Keep in mind that the Roles page is not real-time, it shows the connection count from the last refresh. If you need up-to-the-second data, set up Grafana or run the query against pg_stat_activity directly in SQL Editor. We have a few helpful queries for checking connections.

-- Count connections by application and user name
select
  count(usename),
  count(application_name),
  application_name,
  usename
from
  pg_stat_ssl
  join pg_stat_activity on pg_stat_ssl.pid = pg_stat_activity.pid
group by usename, application_name;
-- View all connections
 SELECT
   pg_stat_activity.pid,
   ssl AS ssl_connection,
   datname AS database,
   usename AS connected_role,
   application_name,
   client_addr,
   query,
   query_start,
   state,
   backend_start
FROM pg_stat_ssl
JOIN pg_stat_activity
 ON pg_stat_ssl.pid = pg_stat_activity.pid;
Why are there active connections when the app is idle?#
Even if your application isn’t making queries, some Supabase services keep persistent connections to your database. For example, Storage, PostgREST, and our health checker all maintain long-lived connections. You usually see a small baseline of active connections from these services.

Why do connection strings have different ports?#
Different modes use different ports:

Direct connection: 5432 (database server)
PgBouncer: 6543 (database server)
Supavisor transaction mode: 6543 (separate server)
Supavisor session mode: 5432 (separate server)
The port helps route the connection to the right pooler/mode.

Does connection pooling affect latency?#
Because the dedicated pooler is hosted on the same machine as your database, it connects with lower latency than the shared pooler, which is hosted on a separate server. Direct connections have no pooler overhead but require IPv6 unless you have the IPv4 add-on.

How to choose the right connection method?#
Direct connection:

Best for: persistent backend services
Limitation: IPv6 only
Shared pooler:

Best for: general-purpose connections (supports IPv4 and IPv6)
Supavisor session mode → persistent backend that require IPv4
Supavisor transaction mode → serverless functions or short-lived tasks
Dedicated pooler (paid tier):

Best for: high-performance apps that need dedicated resources
Uses PgBouncer
You can follow the decision flow in the connection method diagram to quickly choose the right option for your environment.

Decision tree diagram showing when to connect directly to Postgres or use a connection pooler.
Choosing between direct Postgres connections and connection pooling
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How to connect to your Postgres databases
Quickstarts
Data APIs and client libraries
Direct connection
Poolers
Pooler session mode
Pooler transaction mode
Dedicated pooler
More about connection pooling
Application-side poolers
Serverside poolers
Connecting with SSL
Resources
Troubleshooting and Postgres connection string FAQs
What is a “connection refused” error?
What is the “FATAL: Password authentication failed” error?
How do you connect using IPv4?
Where is the Postgres connection string in Supabase?
Can you use Supavisor and PgBouncer together?
How does the default pool size work?
What is the difference between client connections and backend connections?
What is the max pooler clients limit?
Where can you see current connection usage?
Why are there active connections when the app is idle?
Why do connection strings have different ports?
Does connection pooling affect latency?
How to choose the right connection method?
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Connect to your database | Supabase Docs
We use cookies to collect data and improve our services. Learn more


Accept

Opt out
Privacy settings
Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Fundamentals
Importing data
Import data into Supabase

You can import data into Supabase in multiple ways. The best method depends on your data size and app requirements.

If you're working with small datasets in development, you can experiment quickly using CSV import in the Supabase dashboard. If you're working with a large dataset in production, you should plan your data import to minimize app latency and ensure data integrity.

How to import data into Supabase#
You have multiple options for importing your data into Supabase:

CSV import via the Supabase dashboard
Bulk import using pgloader
Using the Postgres COPY command
Using the Supabase API
If you're importing a large dataset or importing data into production, plan ahead and prepare your database.

Option 1: CSV import via Supabase dashboard#
Supabase dashboard provides a user-friendly way to import data. However, for very large datasets, this method may not be the most efficient choice, given the size limit is 100MB. It's generally better suited for smaller datasets and quick data imports. Consider using alternative methods like pgloader for large-scale data imports.

Navigate to the relevant table in the Table Editor.
Click on + New table (for new, empty projects) or Insert (for existing tables), then choose Import Data from CSV and follow the on-screen instructions to upload your CSV file.
Option 2: Bulk import using pgloader#
pgloader is a powerful tool for efficiently importing data into a Postgres database that supports a wide range of source database engines, including MySQL and MS SQL.

You can use it in conjunction with Supabase by following these steps:

Install pgloader on your local machine or a server. For more info, you can refer to the official pgloader installation page.

$ apt-get install pgloader
Create a configuration file that specifies the source data and the target Supabase database (e.g., config.load).
Here's an example configuration file:

LOAD DATABASE
    FROM sourcedb://USER:PASSWORD@HOST/SOURCE_DB
    INTO postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres
ALTER SCHEMA 'public' OWNER TO 'postgres';
set wal_buffers = '64MB', max_wal_senders = 0, statement_timeout = 0, work_mem to '2GB';
Customize the source and Supabase database URL and options to fit your specific use case:

wal_buffers: This parameter is set to '64MB' to allocate 64 megabytes of memory for write-ahead logging buffers. A larger value can help improve write performance by caching more data in memory before writing it to disk. This can be useful during data import operations to speed up the writing of transaction logs.
max_wal_senders: It is set to 0, to disable replication connections. This is done during the data import process to prevent replication-related conflicts and issues.
statement_timeout: The value is set to 0, which means it's disabled, allowing SQL statements to run without a time limit.
work_mem: It is set to '2GB', allocating 2 GB of memory for query operations. This enhances the performance of complex queries by allowing larger in-memory datasets.
Run pgloader with the configuration file.

pgloader config.load
For databases using the Postgres engine, we recommend using the pg_dump and psql command line tools.

Option 3: Using Postgres copy command#
Read more about Bulk data loading.

Option 4: Using the Supabase API#
The Supabase API allows you to programmatically import data into your tables. You can use various client libraries to interact with the API and perform data import operations. This approach is useful when you need to automate data imports, and it gives you fine-grained control over the process. Refer to our API guide for more details.

When importing data via the Supabase API, it's advisable to refrain from bulk imports. This helps ensure a smooth data transfer process and prevents any potential disruptions.

Read more about Rate Limiting, Resource Allocation, & Abuse Prevention.

Preparing to import data#
Large data imports can affect your database performance. Failed imports can also cause data corruption. Importing data is a safe and common operation, but you should plan ahead if you're importing a lot of data, or if you're working in a production environment.

1. Back up your data#
Backups help you restore your data if something goes wrong. Databases on Pro, Team and Enterprise Plans are automatically backed up on schedule, but you can also take your own backup. See Database Backups for more information.

2. Increase statement timeouts#
By default, Supabase enforces query statement timeouts to ensure fair resource allocation and prevent long-running queries from affecting the overall system. When importing large datasets, you may encounter timeouts. To address this:

Increase the Statement Timeout: You can adjust the statement timeout for your session or connection to accommodate longer-running queries. Be cautious when doing this, as excessively long queries can negatively impact system performance. Read more about Statement Timeouts.
3. Estimate your required disk size#
Large datasets consume disk space. Ensure your Supabase project has sufficient disk capacity to accommodate the imported data. If you know how big your database is going to be, you can manually increase the size in your projects database settings.

Read more about disk management.

4. Disable triggers#
When importing large datasets, it's often beneficial to disable triggers temporarily. Triggers can significantly slow down the import process, especially if they involve complex logic or referential integrity checks. After the import, you can re-enable the triggers.

To disable triggers, use the following SQL commands:

-- Disable triggers on a specific table
ALTER TABLE table_name DISABLE TRIGGER ALL;
-- To re-enable triggers
ALTER TABLE table_name ENABLE TRIGGER ALL;
5. Rebuild indices after data import is complete#
Indexing is crucial for query performance, but building indices while importing a large dataset can be time-consuming. Consider building or rebuilding indices after the data import is complete. This approach can significantly speed up the import process and reduce the overall time required.

To build an index after the data import:

-- Create an index on a table
create index index_name on table_name (column_name);
Read more about Managing Indexes in Postgres.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How to import data into Supabase
Option 1: CSV import via Supabase dashboard
Option 2: Bulk import using pgloader
Option 3: Using Postgres copy command
Option 4: Using the Supabase API
Preparing to import data
1. Back up your data
2. Increase statement timeouts
3. Estimate your required disk size
4. Disable triggers
5. Rebuild indices after data import is complete
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Import data into Supabase | Supabase Docs
We use cookies to collect data and improve our services. Learn more


Accept

Opt out
Privacy settings
Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Fundamentals
Securing your data
Securing your data

Supabase helps you control access to your data. With access policies, you can protect sensitive data and make sure users only access what they're allowed to see.

Connecting your app securely#
Supabase allows you to access your database using the auto-generated Data APIs. This speeds up the process of building web apps, since you don't need to write your own backend services to pass database queries and results back and forth.

You can keep your data secure while accessing the Data APIs from the frontend, so long as you:

Turn on Row Level Security (RLS) for your tables
Use your Supabase anon key when you create a Supabase client
Your anon key is safe to expose with RLS enabled, because row access permission is checked against your access policies and the user's JSON Web Token (JWT). The JWT is automatically sent by the Supabase client libraries if the user is logged in using Supabase Auth.

Never expose your service role key on the frontend
Unlike your anon key, your service role key is never safe to expose because it bypasses RLS. Only use your service role key on the backend. Treat it as a secret (for example, import it as a sensitive environment variable instead of hardcoding it).

More information#
Supabase and Postgres provide you with multiple ways to manage security, including but not limited to Row Level Security. See the Access and Security pages for more information:

Row Level Security
Column Level Security
Hardening the Data API
Managing Postgres roles
Managing secrets with Vault
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Connecting your app securely
More information
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Securing your data | Supabase Docs

Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Working with your database (basics)
Managing tables, views, and data
Tables and Data

Tables are where you store your data.

Tables are similar to excel spreadsheets. They contain columns and rows.
For example, this table has 3 "columns" (id, name, description) and 4 "rows" of data:

id	name	description
1	The Phantom Menace	Two Jedi escape a hostile blockade to find allies and come across a young boy who may bring balance to the Force.
2	Attack of the Clones	Ten years after the invasion of Naboo, the Galactic Republic is facing a Separatist movement.
3	Revenge of the Sith	As Obi-Wan pursues a new threat, Anakin acts as a double agent between the Jedi Council and Palpatine and is lured into a sinister plan to rule the galaxy.
4	Star Wars	Luke Skywalker joins forces with a Jedi Knight, a cocky pilot, a Wookiee and two droids to save the galaxy from the Empire's world-destroying battle station.
There are a few important differences from a spreadsheet, but it's a good starting point if you're new to Relational databases.

Creating tables#
When creating a table, it's best practice to add columns at the same time.

Tables and columns

You must define the "data type" of each column when it is created. You can add and remove columns at any time after creating a table.

Supabase provides several options for creating tables. You can use the Dashboard or create them directly using SQL.
We provide a SQL editor within the Dashboard, or you can connect to your database
and run the SQL queries yourself.


Dashboard

SQL
Go to the Table Editor page in the Dashboard.
Click New Table and create a table with the name todos.
Click Save.
Click New Column and create a column with the name task and type text.
Click Save.
When naming tables, use lowercase and underscores instead of spaces (e.g., table_name, not Table Name).

Columns#
You must define the "data type" when you create a column.

Data types#
Every column is a predefined type. Postgres provides many default types, and you can even design your own (or use extensions) if the default types don't fit your needs. You can use any data type that Postgres supports via the SQL editor. We only support a subset of these in the Table Editor in an effort to keep the experience simple for people with less experience with databases.

Show/Hide default data types

You can "cast" columns from one type to another, however there can be some incompatibilities between types.
For example, if you cast a timestamp to a date, you will lose all the time information that was previously saved.

Primary keys#
A table can have a "primary key" - a unique identifier for every row of data. A few tips for Primary Keys:

It's recommended to create a Primary Key for every table in your database.
You can use any column as a primary key, as long as it is unique for every row.
It's common to use a uuid type or a numbered identity column as your primary key.
create table movies (
  id bigint generated always as identity primary key
);
In the example above, we have:

created a column called id
assigned the data type bigint
instructed the database that this should be generated always as identity, which means that Postgres will automatically assign a unique number to this column.
Because it's unique, we can also use it as our primary key.
We could also use generated by default as identity, which would allow us to insert our own unique values.

create table movies (
  id bigint generated by default as identity primary key
);
Loading data#
There are several ways to load data in Supabase. You can load data directly into the database or using the APIs.
Use the "Bulk Loading" instructions if you are loading large data sets.

Basic data loading#

SQL

JavaScript

Dart

Swift

Python

Kotlin
insert into movies
  (name, description)
values
  (
    'The Empire Strikes Back',
    'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.'
  ),
  (
    'Return of the Jedi',
    'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.'
  );
Bulk data loading#
When inserting large data sets it's best to use PostgreSQL's COPY command.
This loads data directly from a file into a table. There are several file formats available for copying data: text, CSV, binary, JSON, etc.

For example, if you wanted to load a CSV file into your movies table:

"The Empire Strikes Back", "After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda."
"Return of the Jedi", "After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star."
You would connect to your database directly and load the file with the COPY command:

psql -h DATABASE_URL -p 5432 -d postgres -U postgres \
  -c "\COPY movies FROM './movies.csv';"
Additionally use the DELIMITER, HEADER and FORMAT options as defined in the Postgres COPY docs.

psql -h DATABASE_URL -p 5432 -d postgres -U postgres \
  -c "\COPY movies FROM './movies.csv' WITH DELIMITER ',' CSV HEADER"
If you receive an error FATAL: password authentication failed for user "postgres", reset your database password in the Database Settings and try again.

Joining tables with foreign keys#
Tables can be "joined" together using Foreign Keys.

Foreign Keys
This is where the "Relational" naming comes from, as data typically forms some sort of relationship.

In our "movies" example above, we might want to add a "category" for each movie (for example, "Action", or "Documentary").
Let's create a new table called categories and "link" our movies table.

create table categories (
  id bigint generated always as identity primary key,
  name text -- category name
);
alter table movies
  add column category_id bigint references categories;
You can also create "many-to-many" relationships by creating a "join" table.
For example if you had the following situations:

You have a list of movies.
A movie can have several actors.
An actor can perform in several movies.

Dashboard

SQL

Schemas#
Tables belong to schemas. Schemas are a way of organizing your tables, often for security reasons.

Schemas and tables
If you don't explicitly pass a schema when creating a table, Postgres will assume that you want to create the table in the public schema.

We can create schemas for organizing tables. For example, we might want a private schema which is hidden from our API:

create schema private;
Now we can create tables inside the private schema:

create table private.salaries (
  id bigint generated by default as identity primary key,
  salary bigint not null,
  actor_id bigint not null references public.actors
);
Views#
A View is a convenient shortcut to a query. Creating a view does not involve new tables or data. When run, an underlying query is executed, returning its results to the user.

Say we have the following tables from a database of a university:

students

id	name	type
1	Princess Leia	undergraduate
2	Yoda	graduate
3	Anakin Skywalker	graduate
courses

id	title	code
1	Introduction to Postgres	PG101
2	Authentication Theories	AUTH205
3	Fundamentals of Supabase	SUP412
grades

id	student_id	course_id	result
1	1	1	B+
2	1	3	A+
3	2	2	A
4	3	1	A-
5	3	2	A
6	3	3	B-
Creating a view consisting of all the three tables will look like this:

create view transcripts as
    select
        students.name,
        students.type,
        courses.title,
        courses.code,
        grades.result
    from grades
    left join students on grades.student_id = students.id
    left join courses on grades.course_id = courses.id;
grant all on table transcripts to authenticated;
Once done, we can now access the underlying query with:

select * from transcripts;
View security#
By default, views are accessed with their creator's permission ("security definer"). If a privileged role creates a view, others accessing it will use that role's elevated permissions. To enforce row level security policies, define the view with the "security invoker" modifier.

-- alter a security_definer view to be security_invoker
alter view <view name>
set (security_invoker = true);
-- create a view with the security_invoker modifier
create view <view name> with(security_invoker=true) as (
  select * from <some table>
);
When to use views#
Views provide several benefits:

Simplicity
Consistency
Logical Organization
Security
Simplicity#
As a query becomes more complex, it can be a hassle to call it over and over - especially when we run it regularly. In the example above, instead of repeatedly running:

select
  students.name,
  students.type,
  courses.title,
  courses.code,
  grades.result
from
  grades
  left join students on grades.student_id = students.id
  left join courses on grades.course_id = courses.id;
We can run this instead:

select * from transcripts;
Additionally, a view behaves like a typical table. We can safely use it in table JOINs or even create new views using existing views.

Consistency#
Views ensure that the likelihood of mistakes decreases when repeatedly executing a query. In our example above, we may decide that we want to exclude the course Introduction to Postgres. The query would become:

select
  students.name,
  students.type,
  courses.title,
  courses.code,
  grades.result
from
  grades
  left join students on grades.student_id = students.id
  left join courses on grades.course_id = courses.id
where courses.code != 'PG101';
Without a view, we would need to go into every dependent query to add the new rule. This would increase in the likelihood of errors and inconsistencies, as well as introducing a lot of effort for a developer. With views, we can alter just the underlying query in the view transcripts. The change will be applied to all applications using this view.

Logical organization#
With views, we can give our query a name. This is extremely useful for teams working with the same database. Instead of guessing what a query is supposed to do, a well-named view can explain it. For example, by looking at the name of the view transcripts, we can infer that the underlying query might involve the students, courses, and grades tables.

Security#
Views can restrict the amount and type of data presented to a user. Instead of allowing a user direct access to a set of tables, we provide them a view instead. We can prevent them from reading sensitive columns by excluding them from the underlying query.

Materialized views#
A materialized view is a form of view but it also stores the results to disk. In subsequent reads of a materialized view, the time taken to return its results would be much faster than a conventional view. This is because the data is readily available for a materialized view while the conventional view executes the underlying query each time it is called.

Using our example above, a materialized view can be created like this:

create materialized view transcripts as
  select
    students.name,
    students.type,
    courses.title,
    courses.code,
    grades.result
  from
    grades
    left join students on grades.student_id = students.id
    left join courses on grades.course_id = courses.id;
Reading from the materialized view is the same as a conventional view:

select * from transcripts;
Refreshing materialized views#
Unfortunately, there is a trade-off - data in materialized views are not always up to date. We need to refresh it regularly to prevent the data from becoming too stale. To do so:

refresh materialized view transcripts;
It's up to you how regularly refresh your materialized views, and it's probably different for each view depending on its use-case.

Materialized views vs conventional views#
Materialized views are useful when execution times for queries or views are too slow. These could likely occur in views or queries involving multiple tables and billions of rows. When using such a view, however, there should be tolerance towards data being outdated. Some use-cases for materialized views are internal dashboards and analytics.

Creating a materialized view is not a solution to inefficient queries. You should always seek to optimize a slow running query even if you are implementing a materialized view.

Resources#
Official Docs: Create table
Official Docs: Create view
Postgres Tutorial: Create tables
Postgres Tutorial: Add column
Postgres Tutorial: Views
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Creating tables
Columns
Data types
Primary keys
Loading data
Basic data loading
Bulk data loading
Joining tables with foreign keys
Schemas
Views
View security
When to use views
Materialized views
Refreshing materialized views
Materialized views vs conventional views
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Tables and Data | Supabase Docs
Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Database
Overview
Fundamentals
Connecting to your database
Importing data
Securing your data
Working with your database (basics)
Managing tables, views, and data
Working with arrays
Managing indexes
Querying joins and nested tables
JSON and unstructured data
Working with your database (intermediate)
Implementing cascade deletes
Managing enums
Managing database functions
Managing database triggers
Managing database webhooks
Using Full Text Search
Partitioning your tables
Managing connections
Managing event triggers
OrioleDB
Overview
Access and security
Row Level Security
Column Level Security
Hardening the Data API
Custom Claims & RBAC
Managing Postgres Roles
Using Custom Postgres Roles
Managing secrets with Vault
Superuser Access and Unsupported Operations
Configuration, optimization, and testing
Database configuration
Query optimization
Database Advisors
Testing your database
Customizing Postgres config
Debugging
Timeouts
Debugging and monitoring
Debugging performance issues
Supavisor
Troubleshooting
ORM Quickstarts

Prisma
Drizzle
Postgres.js
GUI quickstarts
pgAdmin
PSQL
DBeaver
Metabase
Beekeeper Studio
Database replication
Overview
Setting up replication
Monitoring replication
FAQ
Extensions
Overview
HypoPG: Hypothetical indexes
plv8 (deprecated)
http: RESTful Client
index_advisor: Query optimization
PGAudit: Postgres Auditing
pgjwt (deprecated)
PGroonga: Multilingual Full Text Search
pgRouting: Geospatial Routing
pg_cron: Schedule Recurring Jobs
pg_graphql: GraphQL Support
pg_hashids: Short UIDs
pg_jsonschema: JSON Schema Validation
pg_net: Async Networking
pg_plan_filter: Restrict Total Cost
postgres_fdw: query data from an external Postgres server
pgvector: Embeddings and vector similarity
pg_stat_statements: SQL Planning and Execution Statistics
pg_repack: Storage Optimization
PostGIS: Geo queries
pgmq: Queues
pgsodium (pending deprecation): Encryption Features
pgTAP: Unit Testing
plpgsql_check: PL/pgSQL Linter
timescaledb (deprecated)
uuid-ossp: Unique Identifiers
RUM: inverted index for full-text search
Foreign Data Wrappers
Overview
Connecting to Auth0
Connecting to Airtable
Connecting to AWS Cognito
Connecting to AWS S3
Connecting to AWS S3 Vectors
Connecting to BigQuery
Connecting to Clerk
Connecting to ClickHouse
Connecting to DuckDB
Connecting to Firebase
Connecting to Iceberg
Connecting to Logflare
Connecting to MSSQL
Connecting to Notion
Connecting to Paddle
Connecting to Redis
Connecting to Snowflake
Connecting to Stripe
Examples
Drop All Tables in Schema
Select First Row per Group
Print PostgreSQL Version
Replicating from Supabase to External Postgres
Database
Working with your database (basics)
Managing tables, views, and data
Tables and Data

Tables are where you store your data.

Tables are similar to excel spreadsheets. They contain columns and rows.
For example, this table has 3 "columns" (id, name, description) and 4 "rows" of data:

id	name	description
1	The Phantom Menace	Two Jedi escape a hostile blockade to find allies and come across a young boy who may bring balance to the Force.
2	Attack of the Clones	Ten years after the invasion of Naboo, the Galactic Republic is facing a Separatist movement.
3	Revenge of the Sith	As Obi-Wan pursues a new threat, Anakin acts as a double agent between the Jedi Council and Palpatine and is lured into a sinister plan to rule the galaxy.
4	Star Wars	Luke Skywalker joins forces with a Jedi Knight, a cocky pilot, a Wookiee and two droids to save the galaxy from the Empire's world-destroying battle station.
There are a few important differences from a spreadsheet, but it's a good starting point if you're new to Relational databases.

Creating tables#
When creating a table, it's best practice to add columns at the same time.

Tables and columns

You must define the "data type" of each column when it is created. You can add and remove columns at any time after creating a table.

Supabase provides several options for creating tables. You can use the Dashboard or create them directly using SQL.
We provide a SQL editor within the Dashboard, or you can connect to your database
and run the SQL queries yourself.


Dashboard

SQL
Go to the Table Editor page in the Dashboard.
Click New Table and create a table with the name todos.
Click Save.
Click New Column and create a column with the name task and type text.
Click Save.
When naming tables, use lowercase and underscores instead of spaces (e.g., table_name, not Table Name).

Columns#
You must define the "data type" when you create a column.

Data types#
Every column is a predefined type. Postgres provides many default types, and you can even design your own (or use extensions) if the default types don't fit your needs. You can use any data type that Postgres supports via the SQL editor. We only support a subset of these in the Table Editor in an effort to keep the experience simple for people with less experience with databases.

Show/Hide default data types

You can "cast" columns from one type to another, however there can be some incompatibilities between types.
For example, if you cast a timestamp to a date, you will lose all the time information that was previously saved.

Primary keys#
A table can have a "primary key" - a unique identifier for every row of data. A few tips for Primary Keys:

It's recommended to create a Primary Key for every table in your database.
You can use any column as a primary key, as long as it is unique for every row.
It's common to use a uuid type or a numbered identity column as your primary key.
create table movies (
  id bigint generated always as identity primary key
);
In the example above, we have:

created a column called id
assigned the data type bigint
instructed the database that this should be generated always as identity, which means that Postgres will automatically assign a unique number to this column.
Because it's unique, we can also use it as our primary key.
We could also use generated by default as identity, which would allow us to insert our own unique values.

create table movies (
  id bigint generated by default as identity primary key
);
Loading data#
There are several ways to load data in Supabase. You can load data directly into the database or using the APIs.
Use the "Bulk Loading" instructions if you are loading large data sets.

Basic data loading#

SQL

JavaScript

Dart

Swift

Python

Kotlin
insert into movies
  (name, description)
values
  (
    'The Empire Strikes Back',
    'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.'
  ),
  (
    'Return of the Jedi',
    'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.'
  );
Bulk data loading#
When inserting large data sets it's best to use PostgreSQL's COPY command.
This loads data directly from a file into a table. There are several file formats available for copying data: text, CSV, binary, JSON, etc.

For example, if you wanted to load a CSV file into your movies table:

"The Empire Strikes Back", "After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda."
"Return of the Jedi", "After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star."
You would connect to your database directly and load the file with the COPY command:

psql -h DATABASE_URL -p 5432 -d postgres -U postgres \
  -c "\COPY movies FROM './movies.csv';"
Additionally use the DELIMITER, HEADER and FORMAT options as defined in the Postgres COPY docs.

psql -h DATABASE_URL -p 5432 -d postgres -U postgres \
  -c "\COPY movies FROM './movies.csv' WITH DELIMITER ',' CSV HEADER"
If you receive an error FATAL: password authentication failed for user "postgres", reset your database password in the Database Settings and try again.

Joining tables with foreign keys#
Tables can be "joined" together using Foreign Keys.

Foreign Keys
This is where the "Relational" naming comes from, as data typically forms some sort of relationship.

In our "movies" example above, we might want to add a "category" for each movie (for example, "Action", or "Documentary").
Let's create a new table called categories and "link" our movies table.

create table categories (
  id bigint generated always as identity primary key,
  name text -- category name
);
alter table movies
  add column category_id bigint references categories;
You can also create "many-to-many" relationships by creating a "join" table.
For example if you had the following situations:

You have a list of movies.
A movie can have several actors.
An actor can perform in several movies.

Dashboard

SQL

Schemas#
Tables belong to schemas. Schemas are a way of organizing your tables, often for security reasons.

Schemas and tables
If you don't explicitly pass a schema when creating a table, Postgres will assume that you want to create the table in the public schema.

We can create schemas for organizing tables. For example, we might want a private schema which is hidden from our API:

create schema private;
Now we can create tables inside the private schema:

create table private.salaries (
  id bigint generated by default as identity primary key,
  salary bigint not null,
  actor_id bigint not null references public.actors
);
Views#
A View is a convenient shortcut to a query. Creating a view does not involve new tables or data. When run, an underlying query is executed, returning its results to the user.

Say we have the following tables from a database of a university:

students

id	name	type
1	Princess Leia	undergraduate
2	Yoda	graduate
3	Anakin Skywalker	graduate
courses

id	title	code
1	Introduction to Postgres	PG101
2	Authentication Theories	AUTH205
3	Fundamentals of Supabase	SUP412
grades

id	student_id	course_id	result
1	1	1	B+
2	1	3	A+
3	2	2	A
4	3	1	A-
5	3	2	A
6	3	3	B-
Creating a view consisting of all the three tables will look like this:

create view transcripts as
    select
        students.name,
        students.type,
        courses.title,
        courses.code,
        grades.result
    from grades
    left join students on grades.student_id = students.id
    left join courses on grades.course_id = courses.id;
grant all on table transcripts to authenticated;
Once done, we can now access the underlying query with:

select * from transcripts;
View security#
By default, views are accessed with their creator's permission ("security definer"). If a privileged role creates a view, others accessing it will use that role's elevated permissions. To enforce row level security policies, define the view with the "security invoker" modifier.

-- alter a security_definer view to be security_invoker
alter view <view name>
set (security_invoker = true);
-- create a view with the security_invoker modifier
create view <view name> with(security_invoker=true) as (
  select * from <some table>
);
When to use views#
Views provide several benefits:

Simplicity
Consistency
Logical Organization
Security
Simplicity#
As a query becomes more complex, it can be a hassle to call it over and over - especially when we run it regularly. In the example above, instead of repeatedly running:

select
  students.name,
  students.type,
  courses.title,
  courses.code,
  grades.result
from
  grades
  left join students on grades.student_id = students.id
  left join courses on grades.course_id = courses.id;
We can run this instead:

select * from transcripts;
Additionally, a view behaves like a typical table. We can safely use it in table JOINs or even create new views using existing views.

Consistency#
Views ensure that the likelihood of mistakes decreases when repeatedly executing a query. In our example above, we may decide that we want to exclude the course Introduction to Postgres. The query would become:

select
  students.name,
  students.type,
  courses.title,
  courses.code,
  grades.result
from
  grades
  left join students on grades.student_id = students.id
  left join courses on grades.course_id = courses.id
where courses.code != 'PG101';
Without a view, we would need to go into every dependent query to add the new rule. This would increase in the likelihood of errors and inconsistencies, as well as introducing a lot of effort for a developer. With views, we can alter just the underlying query in the view transcripts. The change will be applied to all applications using this view.

Logical organization#
With views, we can give our query a name. This is extremely useful for teams working with the same database. Instead of guessing what a query is supposed to do, a well-named view can explain it. For example, by looking at the name of the view transcripts, we can infer that the underlying query might involve the students, courses, and grades tables.

Security#
Views can restrict the amount and type of data presented to a user. Instead of allowing a user direct access to a set of tables, we provide them a view instead. We can prevent them from reading sensitive columns by excluding them from the underlying query.

Materialized views#
A materialized view is a form of view but it also stores the results to disk. In subsequent reads of a materialized view, the time taken to return its results would be much faster than a conventional view. This is because the data is readily available for a materialized view while the conventional view executes the underlying query each time it is called.

Using our example above, a materialized view can be created like this:

create materialized view transcripts as
  select
    students.name,
    students.type,
    courses.title,
    courses.code,
    grades.result
  from
    grades
    left join students on grades.student_id = students.id
    left join courses on grades.course_id = courses.id;
Reading from the materialized view is the same as a conventional view:

select * from transcripts;
Refreshing materialized views#
Unfortunately, there is a trade-off - data in materialized views are not always up to date. We need to refresh it regularly to prevent the data from becoming too stale. To do so:

refresh materialized view transcripts;
It's up to you how regularly refresh your materialized views, and it's probably different for each view depending on its use-case.

Materialized views vs conventional views#
Materialized views are useful when execution times for queries or views are too slow. These could likely occur in views or queries involving multiple tables and billions of rows. When using such a view, however, there should be tolerance towards data being outdated. Some use-cases for materialized views are internal dashboards and analytics.

Creating a materialized view is not a solution to inefficient queries. You should always seek to optimize a slow running query even if you are implementing a materialized view.

Resources#
Official Docs: Create table
Official Docs: Create view
Postgres Tutorial: Create tables
Postgres Tutorial: Add column
Postgres Tutorial: Views
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Creating tables
Columns
Data types
Primary keys
Loading data
Basic data loading
Bulk data loading
Joining tables with foreign keys
Schemas
Views
View security
When to use views
Materialized views
Refreshing materialized views
Materialized views vs conventional views
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Tables and Data | Supabase Docs
Working With Arrays

Postgres supports flexible array types. These arrays are also supported in the Supabase Dashboard and in the JavaScript API.

Create a table with an array column#
Create a test table with a text array (an array of strings):


Dashboard

SQL
Go to the Table editor page in the Dashboard.
Click New Table and create a table with the name arraytest.
Click Save.
Click New Column and create a column with the name textarray, type text, and select Define as array.
Click Save.
Insert a record with an array value#

Dashboard

SQL

JavaScript

Swift

Python
Go to the Table editor page in the Dashboard.
Select the arraytest table.
Click Insert row and add ["Harry", "Larry", "Moe"].
Click Save.
View the results#

Dashboard

SQL
Go to the Table editor page in the Dashboard.
Select the arraytest table.
You should see:

| id  | textarray               |
| --- | ----------------------- |
| 1   | ["Harry","Larry","Moe"] |
Query array data#
Postgres uses 1-based indexing (e.g., textarray[1] is the first item in the array).


SQL

JavaScript

Swift
To select the first item from the array and get the total length of the array:

SELECT textarray[1], array_length(textarray, 1) FROM arraytest;
returns:

| textarray | array_length |
| --------- | ------------ |
| Harry     | 3            |
Resources#
Supabase JS Client
Supabase - Get started for free
Postgres Arrays
Edit this page on GitHub-

Managing Indexes in PostgreSQL

An index makes your Postgres queries faster. The index is like a "table of contents" for your data - a reference list which allows queries to quickly locate a row in a given table without needing to scan the entire table (which in large tables can take a long time).

Indexes can be structured in a few different ways. The type of index chosen depends on the values you are indexing. By far the most common index type, and the default in Postgres, is the B-Tree. A B-Tree is the generalized form of a binary search tree, where nodes can have more than two children.

Even though indexes improve query performance, the Postgres query planner may not always make use of a given index when choosing which optimizations to make. Additionally indexes come with some overhead - additional writes and increased storage - so it's useful to understand how and when to use indexes, if at all.

Create an index#
Let's take an example table:

create table persons (
  id bigint generated by default as identity primary key,
  age int,
  height int,
  weight int,
  name text,
  deceased boolean
);
All the queries in this guide can be run using the SQL Editor in the Supabase Dashboard, or via psql if you're connecting directly to the database.

We might want to frequently query users based on their age:

select name from persons where age = 32;
Without an index, Postgres will scan every row in the table to find equality matches on age.

You can verify this by doing an explain on the query:

explain select name from persons where age = 32;
Outputs:

Seq Scan on persons  (cost=0.00..22.75 rows=x width=y)
Filter: (age = 32)
To add a simple B-Tree index you can run:

create index idx_persons_age on persons (age);
It can take a long time to build indexes on large datasets and the default behaviour of create index is to lock the table from writes.

Luckily Postgres provides us with create index concurrently which prevents blocking writes on the table, but does take a bit longer to build.

Here is a simplified diagram of the index we just created (note that in practice, nodes actually have more than two children).

B-Tree index example in Postgres

You can see that in any large data set, traversing the index to locate a given value can be done in much less operations (O(log n)) than compared to scanning the table one value at a time from top to bottom (O(n)).

Partial indexes#
If you are frequently querying a subset of rows then it may be more efficient to build a partial index. In our example, perhaps we only want to match on age where deceased is false. We could build a partial index:

create index idx_living_persons_age on persons (age)
where deceased is false;
Ordering indexes#
By default B-Tree indexes are sorted in ascending order, but sometimes you may want to provide a different ordering. Perhaps our application has a page featuring the top 10 oldest people. Here we would want to sort in descending order, and include NULL values last. For this we can use:

create index idx_persons_age_desc on persons (age desc nulls last);
Reindexing#
After a while indexes can become stale and may need rebuilding. Postgres provides a reindex command for this, but due to Postgres locks being placed on the index during this process, you may want to make use of the concurrent keyword.

reindex index concurrently idx_persons_age;
Alternatively you can reindex all indexes on a particular table:

reindex table concurrently persons;
Take note that reindex can be used inside a transaction, but reindex [index/table] concurrently cannot.

Index Advisor#
Indexes can improve query performance of your tables as they grow. The Supabase Dashboard offers an Index Advisor, which suggests potential indexes to add to your tables.

For more information on the Index Advisor and its suggestions, see the index_advisor extension.

To use the Dashboard Index Advisor:

Go to the Query Performance page.
Click on a query to bring up the Details side panel.
Select the Indexes tab.
Enable Index Advisor if prompted.
Understanding Index Advisor results#
The Indexes tab shows the existing indexes used in the selected query. Note that indexes suggested in the "New Index Recommendations" section may not be used when you create them. Postgres' query planner may intentionally ignore an available index if it determines that the query will be faster without. For example, on a small table, a sequential scan might be faster than an index scan. In that case, the planner will switch to using the index as the table size grows, helping to future proof the query.

If additional indexes might improve your query, the Index Advisor shows the suggested indexes with the estimated improvement in startup and total costs:

Startup cost is the cost to fetch the first row
Total cost is the cost to fetch all the rows
Costs are in arbitrary units, where a single sequential page read costs 1.0 units.

Querying Joins and Nested tables

The data APIs automatically detect relationships between Postgres tables. Since Postgres is a relational database, this is a very common scenario.

One-to-many joins#
Let's use an example database that stores orchestral_sections and instruments:


Tables

SQL
Orchestral sections

id	name
1	strings
2	woodwinds
Instruments

id	name	section_id
1	violin	1
2	viola	1
3	flute	2
4	oboe	2
The APIs will automatically detect relationships based on the foreign keys:


JavaScript

Dart

Swift

Kotlin

Python

GraphQL

URL
const { data, error } = await supabase.from('orchestral_sections').select(`
  id,
  name,
  instruments ( id, name )
`)
TypeScript types for joins#
supabase-js always returns a data object (for success), and an error object (for unsuccessful requests).

These helper types provide the result types from any query, including nested types for database joins.

Given the following schema with a relation between orchestral sections and instruments:

create table orchestral_sections (
  "id" serial primary key,
  "name" text
);
create table instruments (
  "id" serial primary key,
  "name" text,
  "section_id" int references "orchestral_sections"
);
We can get the nested SectionsWithInstruments type like this:

import { QueryResult, QueryData, QueryError } from '@supabase/supabase-js'
const sectionsWithInstrumentsQuery = supabase.from('orchestral_sections').select(`
  id,
  name,
  instruments (
    id,
    name
  )
`)
type SectionsWithInstruments = QueryData<typeof sectionsWithInstrumentsQuery>
const { data, error } = await sectionsWithInstrumentsQuery
if (error) throw error
const sectionsWithInstruments: SectionsWithInstruments = data
Many-to-many joins#
The data APIs will detect many-to-many joins. For example, if you have a database which stored teams of users (where each user could belong to many teams):

create table users (
  "id" serial primary key,
  "name" text
);
create table teams (
  "id" serial primary key,
  "team_name" text
);
create table members (
  "user_id" int references users,
  "team_id" int references teams,
  primary key (user_id, team_id)
);
In these cases you don't need to explicitly define the joining table (members). If we wanted to fetch all the teams and the members in each team:


JavaScript

Dart

Swift

Kotlin

Python

GraphQL

URL
const { data, error } = await supabase.from('teams').select(`
  id,
  team_name,
  users ( id, name )
`)
Specifying the ON clause for joins with multiple foreign keys#
For example, if you have a project that tracks when employees check in and out of work shifts:

-- Employees
create table users (
  "id" serial primary key,
  "name" text
);
-- Badge scans
create table scans (
  "id" serial primary key,
  "user_id" int references users,
  "badge_scan_time" timestamp
);
-- Work shifts
create table shifts (
  "id" serial primary key,
  "user_id" int references users,
  "scan_id_start" int references scans, -- clocking in
  "scan_id_end" int references scans, -- clocking out
  "attendance_status" text
);
In this case, you need to explicitly define the join because the joining column on shifts is ambiguous as they are both referencing the scans table.

To fetch all the shifts with scan_id_start and scan_id_end related to a specific scan, use the following syntax:


JavaScript

Dart

Swift

Kotlin

Python

GraphQL
const { data, error } = await supabase.from('shifts').select(
  `
    *,
    start_scan:scans!scan_id_start (
      id,
      user_id,
      badge_scan_time
    ),
   end_scan:scans!scan_id_end (
     id,
     user_id,
     badge_scan_time
    )
  `
)

Database
Working with your database (basics)
JSON and unstructured data
Managing JSON and unstructured data

Using the JSON data type in Postgres.

Postgres supports storing and querying unstructured data.

JSON vs JSONB#
Postgres supports two types of JSON columns: json (stored as a string) and jsonb (stored as a binary). The recommended type is jsonb for almost all cases.

json stores an exact copy of the input text. Database functions must reparse the content on each execution.
jsonb stores database in a decomposed binary format. While this makes it slightly slower to input due to added conversion overhead, it is significantly faster to process, since no reparsing is needed.
When to use JSON/JSONB#
Generally you should use a jsonb column when you have data that is unstructured or has a variable schema. For example, if you wanted to store responses for various webhooks, you might not know the format of the response when creating the table. Instead, you could store the payload as a jsonb object in a single column.

Don't go overboard with json/jsonb columns. They are a useful tool, but most of the benefits of a relational database come from the ability to query and join structured data, and the referential integrity that brings.

Create JSONB columns#
json/jsonb is just another "data type" for Postgres columns. You can create a jsonb column in the same way you would create a text or int column:


SQL

Dashboard
create table books (
  id serial primary key,
  title text,
  author text,
  metadata jsonb
);
Inserting JSON data#
You can insert JSON data in the same way that you insert any other data. The data must be valid JSON.


SQL

Dashboard

JavaScript

Dart

Swift

Kotlin

Python
insert into books
  (title, author, metadata)
values
  (
    'The Poky Little Puppy',
    'Janette Sebring Lowrey',
    '{"description":"Puppy is slower than other, bigger animals.","price":5.95,"ages":[3,6]}'
  ),
  (
    'The Tale of Peter Rabbit',
    'Beatrix Potter',
    '{"description":"Rabbit eats some vegetables.","price":4.49,"ages":[2,5]}'
  ),
  (
    'Tootle',
    'Gertrude Crampton',
    '{"description":"Little toy train has big dreams.","price":3.99,"ages":[2,5]}'
  ),
  (
    'Green Eggs and Ham',
    'Dr. Seuss',
    '{"description":"Sam has changing food preferences and eats unusually colored food.","price":7.49,"ages":[4,8]}'
  ),
  (
    'Harry Potter and the Goblet of Fire',
    'J.K. Rowling',
    '{"description":"Fourth year of school starts, big drama ensues.","price":24.95,"ages":[10,99]}'
  );
Query JSON data#
Querying JSON data is similar to querying other data, with a few other features to access nested values.

Postgres support a range of JSON functions and operators. For example, the -> operator returns values as jsonb data. If you want the data returned as text, use the ->> operator.


SQL

JavaScript

Swift

Kotlin

Python

Result
select
  title,
  metadata ->> 'description' as description, -- returned as text
  metadata -> 'price' as price,
  metadata -> 'ages' -> 0 as low_age,
  metadata -> 'ages' -> 1 as high_age
from books;
Validating JSON data#
Supabase provides the pg_jsonschema extension that adds the ability to validate json and jsonb data types against JSON Schema documents.

Once you have enabled the extension, you can add a "check constraint" to your table to validate the JSON data:

create table customers (
  id serial primary key,
  metadata json
);
alter table customers
add constraint check_metadata check (
  json_matches_schema(
    '{
        "type": "object",
        "properties": {
            "tags": {
                "type": "array",
                "items": {
                    "type": "string",
                    "maxLength": 16
                }
            }
        }
    }',
    metadata
  )
);

Database
Working with your database (intermediate)
Implementing cascade deletes
Cascade Deletes

There are 5 options for foreign key constraint deletes:

CASCADE: When a row is deleted from the parent table, all related rows in the child tables are deleted as well.
RESTRICT: When a row is deleted from the parent table, the delete operation is aborted if there are any related rows in the child tables.
SET NULL: When a row is deleted from the parent table, the values of the foreign key columns in the child tables are set to NULL.
SET DEFAULT: When a row is deleted from the parent table, the values of the foreign key columns in the child tables are set to their default values.
NO ACTION: This option is similar to RESTRICT, but it also has the option to be “deferred” to the end of a transaction. This means that other cascading deletes can run first, and then this delete constraint will only throw an error if there is referenced data remaining at the end of the transaction.
These options can be specified when defining a foreign key constraint using the "ON DELETE" clause. For example, the following SQL statement creates a foreign key constraint with the CASCADE option:

alter table child_table
add constraint fk_parent foreign key (parent_id) references parent_table (id)
  on delete cascade;
This means that when a row is deleted from the parent_table, all related rows in the child_table will be deleted as well.

RESTRICT vs NO ACTION#
The difference between NO ACTION and RESTRICT is subtle and can be a bit confusing.

Both NO ACTION and RESTRICT are used to prevent deletion of a row in a parent table if there are related rows in a child table. However, there is a subtle difference in how they behave.

When a foreign key constraint is defined with the option RESTRICT, it means that if a row in the parent table is deleted, the database will immediately raise an error and prevent the deletion of the row in the parent table. The database will not delete, update or set to NULL any rows in the referenced tables.

When a foreign key constraint is defined with the option NO ACTION, it means that if a row in the parent table is deleted, the database will also raise an error and prevent the deletion of the row in the parent table. However unlike RESTRICT, NO ACTION has the option to defer the check using INITIALLY DEFERRED. This will only raise the above error if the referenced rows still exist at the end of the transaction.

The difference from RESTRICT is that a constraint marked as NO ACTION INITIALLY DEFERRED is deferred until the end of the transaction, rather than running immediately. If, for example there is another foreign key constraint between the same tables marked as CASCADE, the cascade will occur first and delete the referenced rows, and no error will be thrown by the deferred constraint. Otherwise if there are still rows referencing the parent row by the end of the transaction, an error will be raised just like before. Just like RESTRICT, the database will not delete, update or set to NULL any rows in the referenced tables.

In practice, you can use either NO ACTION or RESTRICT depending on your needs. NO ACTION is the default behavior if you do not specify anything. If you prefer to defer the check until the end of the transaction, use NO ACTION INITIALLY DEFERRED.

Example#
Let's further illustrate the difference with an example. We'll use the following data:

grandparent

id	name
1	Elizabeth
parent

id	name	parent_id
1	Charles	1
2	Diana	1
child

id	name	father	mother
1	William	1	2
To create these tables and their data, we run:

create table grandparent (
  id serial primary key,
  name text
);
create table parent (
  id serial primary key,
  name text,
  parent_id integer references grandparent (id)
    on delete cascade
);
create table child (
  id serial primary key,
  name text,
  father integer references parent (id)
    on delete restrict
);
insert into grandparent
  (id, name)
values
  (1, 'Elizabeth');
insert into parent
  (id, name, parent_id)
values
  (1, 'Charles', 1);
insert into parent
  (id, name, parent_id)
values
  (2, 'Diana', 1);
-- We'll just link the father for now
insert into child
  (id, name, father)
values
  (1, 'William', 1);
RESTRICT#
RESTRICT will prevent a delete and raise an error:

postgres=# delete from grandparent;
ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"
DETAIL: Key (id)=(1) is still referenced from table "child".
Even though the foreign key constraint between parent and grandparent is CASCADE, the constraint between child and father is RESTRICT. Therefore an error is raised and no records are deleted.

NO ACTION#
Let's change the child-father relationship to NO ACTION:

alter table child
drop constraint child_father_fkey;
alter table child
add constraint child_father_fkey foreign key (father) references parent (id)
  on delete no action;
We see that NO ACTION will also prevent a delete and raise an error:

postgres=# delete from grandparent;
ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"
DETAIL: Key (id)=(1) is still referenced from table "child".
NO ACTION INITIALLY DEFERRED#
We'll change the foreign key constraint between child and father to be NO ACTION INITIALLY DEFERRED:

alter table child
drop constraint child_father_fkey;
alter table child
add constraint child_father_fkey foreign key (father) references parent (id)
  on delete no action initially deferred;
Here you will see that INITIALLY DEFFERED seems to operate like NO ACTION or RESTRICT. When we run a delete, it seems to make no difference:

postgres=# delete from grandparent;
ERROR: update or delete on table "parent" violates foreign key constraint "child_father_fkey" on table "child"
DETAIL: Key (id)=(1) is still referenced from table "child".
But, when we combine it with other constraints, then any other constraints take precedence. For example, let's run the same but add a mother column that has a CASCADE delete:

alter table child
add column mother integer references parent (id)
  on delete cascade;
update child
set mother = 2
where id = 1;
Then let's run a delete on the grandparent table:

postgres=# delete from grandparent;
DELETE 1
postgres=# select * from parent;
 id | name | parent_id
----+------+-----------
(0 rows)
postgres=# select * from child;
 id | name | father | mother
----+------+--------+--------
(0 rows)
The mother deletion took precedence over the father, and so William was deleted. After William was deleted, there was no reference to “Charles” and so he was free to be deleted, even though previously he wasn't (without INITIALLY DEFERRED).

Database
Working with your database (intermediate)
Managing enums
Managing Enums in Postgres

Enums in Postgres are a custom data type. They allow you to define a set of values (or labels) that a column can hold. They are useful when you have a fixed set of possible values for a column.

Creating enums#
You can define a Postgres Enum using the create type statement. Here's an example:

create type mood as enum (
  'happy',
  'sad',
  'excited',
  'calm'
);
In this example, we've created an Enum called "mood" with four possible values.

When to use enums#
There is a lot of overlap between Enums and foreign keys. Both can be used to define a set of values for a column. However, there are some advantages to using Enums:

Performance: You can query a single table instead of finding the value from a lookup table.
Simplicity: Generally the SQL is easier to read and write.
There are also some disadvantages to using Enums:

Limited Flexibility: Adding and removing values requires modifying the database schema (i.e.: using migrations) rather than adding data to a table.
Maintenance Overhead: Enum types require ongoing maintenance. If your application's requirements change frequently, maintaining enums can become burdensome.
In general you should only use Enums when the list of values is small, fixed, and unlikely to change often. Things like "a list of continents" or "a list of departments" are good candidates for Enums.

Using enums in tables#
To use the Enum in a table, you can define a column with the Enum type. For example:

create table person (
  id serial primary key,
  name text,
  current_mood mood
);
Here, the current_mood column can only have values from the "mood" Enum.

Inserting data with enums#
You can insert data into a table with Enum columns by specifying one of the Enum values:

insert into person
  (name, current_mood)
values
  ('Alice', 'happy');
Querying data with enums#
When querying data, you can filter and compare Enum values as usual:

select * 
from person 
where current_mood = 'sad';
Managing enums#
You can manage your Enums using the alter type statement. Here are some examples:

Updating enum values#
You can update the value of an Enum column:

update person
set current_mood = 'excited'
where name = 'Alice';
Adding enum values#
To add new values to an existing Postgres Enum, you can use the ALTER TYPE statement. Here's how you can do it:

Let's say you have an existing Enum called mood, and you want to add a new value, content:

alter type mood add value 'content';
Removing enum values#
Even though it is possible, it is unsafe to remove enum values once they have been created. It's better to leave the enum value in place.

Read the Postgres mailing list for more information:

There is no ALTER TYPE DELETE VALUE in Postgres. Even if you delete every occurrence of an Enum value within a table (and vacuumed away those rows), the target value could still exist in upper index pages. If you delete the pg_enum entry you'll break the index.

Getting a list of enum values#
Check your existing Enum values by querying the enum_range function:

select enum_range(null::mood);
Resources#
Official Postgres Docs: Enumerated Types

Database
Working with your database (intermediate)
Managing enums
Managing Enums in Postgres

Enums in Postgres are a custom data type. They allow you to define a set of values (or labels) that a column can hold. They are useful when you have a fixed set of possible values for a column.

Creating enums#
You can define a Postgres Enum using the create type statement. Here's an example:

create type mood as enum (
  'happy',
  'sad',
  'excited',
  'calm'
);
In this example, we've created an Enum called "mood" with four possible values.

When to use enums#
There is a lot of overlap between Enums and foreign keys. Both can be used to define a set of values for a column. However, there are some advantages to using Enums:

Performance: You can query a single table instead of finding the value from a lookup table.
Simplicity: Generally the SQL is easier to read and write.
There are also some disadvantages to using Enums:

Limited Flexibility: Adding and removing values requires modifying the database schema (i.e.: using migrations) rather than adding data to a table.
Maintenance Overhead: Enum types require ongoing maintenance. If your application's requirements change frequently, maintaining enums can become burdensome.
In general you should only use Enums when the list of values is small, fixed, and unlikely to change often. Things like "a list of continents" or "a list of departments" are good candidates for Enums.

Using enums in tables#
To use the Enum in a table, you can define a column with the Enum type. For example:

create table person (
  id serial primary key,
  name text,
  current_mood mood
);
Here, the current_mood column can only have values from the "mood" Enum.

Inserting data with enums#
You can insert data into a table with Enum columns by specifying one of the Enum values:

insert into person
  (name, current_mood)
values
  ('Alice', 'happy');
Querying data with enums#
When querying data, you can filter and compare Enum values as usual:

select * 
from person 
where current_mood = 'sad';
Managing enums#
You can manage your Enums using the alter type statement. Here are some examples:

Updating enum values#
You can update the value of an Enum column:

update person
set current_mood = 'excited'
where name = 'Alice';
Adding enum values#
To add new values to an existing Postgres Enum, you can use the ALTER TYPE statement. Here's how you can do it:

Let's say you have an existing Enum called mood, and you want to add a new value, content:

alter type mood add value 'content';
Removing enum values#
Even though it is possible, it is unsafe to remove enum values once they have been created. It's better to leave the enum value in place.

Read the Postgres mailing list for more information:

There is no ALTER TYPE DELETE VALUE in Postgres. Even if you delete every occurrence of an Enum value within a table (and vacuumed away those rows), the target value could still exist in upper index pages. If you delete the pg_enum entry you'll break the index.

Getting a list of enum values#
Check your existing Enum values by querying the enum_range function:

select enum_range(null::mood);
Resources#
Official Postgres Docs: Enumerated Types
Postgres Triggers

Automatically execute SQL on table events.

In Postgres, a trigger executes a set of actions automatically on table events such as INSERTs, UPDATEs, DELETEs, or TRUNCATE operations.

Creating a trigger#
Creating triggers involve 2 parts:

A Function which will be executed (called the Trigger Function)
The actual Trigger object, with parameters around when the trigger should be run.
An example of a trigger is:

create trigger "trigger_name"
after insert on "table_name"
for each row
execute function trigger_function();
Trigger functions#
A trigger function is a user-defined Function that Postgres executes when the trigger is fired.

Example trigger function#
Here is an example that updates salary_log whenever an employee's salary is updated:

-- Example: Update salary_log when salary is updated
create function update_salary_log()
returns trigger
language plpgsql
as $$
begin
  insert into salary_log(employee_id, old_salary, new_salary)
  values (new.id, old.salary, new.salary);
  return new;
end;
$$;
create trigger salary_update_trigger
after update on employees
for each row
execute function update_salary_log();
Trigger variables#
Trigger functions have access to several special variables that provide information about the context of the trigger event and the data being modified. In the example above you can see the values inserted into the salary log are old.salary and new.salary - in this case old specifies the previous values and new specifies the updated values.

Here are some of the key variables and options available within trigger functions:

TG_NAME: The name of the trigger being fired.
TG_WHEN: The timing of the trigger event (BEFORE or AFTER).
TG_OP: The operation that triggered the event (INSERT, UPDATE, DELETE, or TRUNCATE).
OLD: A record variable holding the old row's data in UPDATE and DELETE triggers.
NEW: A record variable holding the new row's data in UPDATE and INSERT triggers.
TG_LEVEL: The trigger level (ROW or STATEMENT), indicating whether the trigger is row-level or statement-level.
TG_RELID: The object ID of the table on which the trigger is being fired.
TG_TABLE_NAME: The name of the table on which the trigger is being fired.
TG_TABLE_SCHEMA: The schema of the table on which the trigger is being fired.
TG_ARGV: An array of string arguments provided when creating the trigger.
TG_NARGS: The number of arguments in the TG_ARGV array.
Types of triggers#
There are two types of trigger, BEFORE and AFTER:

Trigger before changes are made#
Executes before the triggering event.

create trigger before_insert_trigger
before insert on orders
for each row
execute function before_insert_function();
Trigger after changes are made#
Executes after the triggering event.

create trigger after_delete_trigger
after delete on customers
for each row
execute function after_delete_function();
Execution frequency#
There are two options available for executing triggers:

for each row: specifies that the trigger function should be executed once for each affected row.
for each statement: the trigger is executed once for the entire operation (for example, once on insert). This can be more efficient than for each row when dealing with multiple rows affected by a single SQL statement, as they allow you to perform calculations or updates on groups of rows at once.
Dropping a trigger#
You can delete a trigger using the drop trigger command:

drop trigger "trigger_name" on "table_name";
If your trigger is inside a restricted schema, you won't be able to drop it due to permission restrictions. In those cases, you can drop the function it depends on instead using the CASCADE clause to automatically remove all triggers that call it:

drop function if exists restricted_schema.function_name() cascade;
Make sure you take a backup of the function before removing it in case you're planning to recreate it later.

Resources#
Official Postgres Docs: Triggers
Official Postgres Docs: Overview of Trigger Behavior
Official Postgres Docs: CREATE TRIGGER
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Creating a trigger
Trigger functions
Example trigger function
Trigger variables
Types of triggers
Trigger before changes are made
Trigger after changes are made
Execution frequency
Dropping a trigger
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabas

Database Webhooks

Trigger external payloads on database events.

Database Webhooks allow you to send real-time data from your database to another system whenever a table event occurs.

You can hook into three table events: INSERT, UPDATE, and DELETE. All events are fired after a database row is changed.

Webhooks vs triggers#
Database Webhooks are very similar to triggers, and that's because Database Webhooks are just a convenience wrapper around triggers using the pg_net extension. This extension is asynchronous, and therefore will not block your database changes for long-running network requests.

This video demonstrates how you can create a new customer in Stripe each time a row is inserted into a profiles table:


Creating a webhook#
Create a new Database Webhook in the Dashboard.
Give your Webhook a name.
Select the table you want to hook into.
Select one or more events (table inserts, updates, or deletes) you want to hook into.
Since webhooks are just database triggers, you can also create one from SQL statement directly.

create trigger "my_webhook" after insert
on "public"."my_table" for each row
execute function "supabase_functions"."http_request"(
  'http://host.docker.internal:3000',
  'POST',
  '{"Content-Type":"application/json"}',
  '{}',
  '1000'
);
We currently support HTTP webhooks. These can be sent as POST or GET requests with a JSON payload.

Payload#
The payload is automatically generated from the underlying table record:

type InsertPayload = {
  type: 'INSERT'
  table: string
  schema: string
  record: TableRecord<T>
  old_record: null
}
type UpdatePayload = {
  type: 'UPDATE'
  table: string
  schema: string
  record: TableRecord<T>
  old_record: TableRecord<T>
}
type DeletePayload = {
  type: 'DELETE'
  table: string
  schema: string
  record: null
  old_record: TableRecord<T>
}
Monitoring#
Logging history of webhook calls is available under the net schema of your database. For more info, see the GitHub Repo.

Local development#
When using Database Webhooks on your local Supabase instance, you need to be aware that the Postgres database runs inside a Docker container. This means that localhost or 127.0.0.1 in your webhook URL will refer to the container itself, not your host machine where your application is running.

To target services running on your host machine, use host.docker.internal. If that doesn't work, you may need to use your machine's local IP address instead.

For example, if you want to trigger an edge function when a webhook fires, your webhook URL would be:

http://host.docker.internal:54321/functions/v1/my-function-name
If you're experiencing connection issues with webhooks locally, verify you're using the correct hostname instead of localhost.

Resources#
pg_net: an async networking extension for Postgres
Edit this page on GitHub

Full Text Search

How to use full text search in PostgreSQL.

Postgres has built-in functions to handle Full Text Search queries. This is like a "search engine" within Postgres.

Preparation#
For this guide we'll use the following example data:


Data

SQL
id	title	author	description
1	The Poky Little Puppy	Janette Sebring Lowrey	Puppy is slower than other, bigger animals.
2	The Tale of Peter Rabbit	Beatrix Potter	Rabbit eats some vegetables.
3	Tootle	Gertrude Crampton	Little toy train has big dreams.
4	Green Eggs and Ham	Dr. Seuss	Sam has changing food preferences and eats unusually colored food.
5	Harry Potter and the Goblet of Fire	J.K. Rowling	Fourth year of school starts, big drama ensues.
Usage#
The functions we'll cover in this guide are:

to_tsvector()#
Converts your data into searchable tokens. to_tsvector() stands for "to text search vector." For example:

select to_tsvector('green eggs and ham');
-- Returns 'egg':2 'green':1 'ham':4
Collectively these tokens are called a "document" which Postgres can use for comparisons.

to_tsquery()#
Converts a query string into tokens to match. to_tsquery() stands for "to text search query."

This conversion step is important because we will want to "fuzzy match" on keywords.
For example if a user searches for eggs, and a column has the value egg, we probably still want to return a match.

Postgres provides several functions to create tsquery objects:

to_tsquery() - Requires manual specification of operators (&, |, !)
plainto_tsquery() - Converts plain text to an AND query: plainto_tsquery('english', 'fat rats') → 'fat' & 'rat'
phraseto_tsquery() - Creates phrase queries: phraseto_tsquery('english', 'fat rats') → 'fat' <-> 'rat'
websearch_to_tsquery() - Supports web search syntax with quotes, "or", and negation
Match: @@#
The @@ symbol is the "match" symbol for Full Text Search. It returns any matches between a to_tsvector result and a to_tsquery result.

Take the following example:


SQL

JavaScript

Dart

Swift

Kotlin

Python
select *
from books
where title = 'Harry';
The equality symbol above (=) is very "strict" on what it matches. In a full text search context, we might want to find all "Harry Potter" books and so we can rewrite the
example above:


SQL

JavaScript

Dart

Swift

Kotlin
select *
from books
where to_tsvector(title) @@ to_tsquery('Harry');
Basic full text queries#
Search a single column#
To find all books where the description contain the word big:


SQL

JavaScript

Dart

Swift

Kotlin

Python

Data
select
  *
from
  books
where
  to_tsvector(description)
  @@ to_tsquery('big');
Search multiple columns#
Right now there is no direct way to use JavaScript or Dart to search through multiple columns but you can do it by creating computed columns on the database.

To find all books where description or title contain the word little:


SQL

JavaScript

Dart

Swift

Kotlin

Python

Data
select
  *
from
  books
where
  to_tsvector(description || ' ' || title) -- concat columns, but be sure to include a space to separate them!
  @@ to_tsquery('little');
Match all search words#
To find all books where description contains BOTH of the words little and big, we can use the & symbol:


SQL

JavaScript

Dart

Swift

Kotlin

Python

Data
select
  *
from
  books
where
  to_tsvector(description)
  @@ to_tsquery('little & big'); -- use & for AND in the search query
Match any search words#
To find all books where description contain ANY of the words little or big, use the | symbol:


SQL

JavaScript

Dart

Swift

Kotlin

Python

Data
select
  *
from
  books
where
  to_tsvector(description)
  @@ to_tsquery('little | big'); -- use | for OR in the search query
Notice how searching for big includes results with the word bigger (or biggest, etc).

Partial search#
Partial search is particularly useful when you want to find matches on substrings within your data.

Implementing partial search#
You can use the :* syntax with to_tsquery(). Here's an example that searches for any book titles beginning with "Lit":

select title from books where to_tsvector(title) @@ to_tsquery('Lit:*');
Extending functionality with RPC#
To make the partial search functionality accessible through the API, you can wrap the search logic in a stored procedure.

After creating this function, you can invoke it from your application using the SDK for your platform. Here's an example:


SQL

JavaScript

Dart

Swift

Kotlin

Python
create or replace function search_books_by_title_prefix(prefix text)
returns setof books AS $$
begin
  return query
  select * from books where to_tsvector('english', title) @@ to_tsquery(prefix || ':*');
end;
$$ language plpgsql;
This function takes a prefix parameter and returns all books where the title contains a word starting with that prefix. The :* operator is used to denote a prefix match in the to_tsquery() function.

Handling spaces in queries#
When you want the search term to include a phrase or multiple words, you can concatenate words using a + as a placeholder for space:

select * from search_books_by_title_prefix('Little+Puppy');
Web search syntax with websearch_to_tsquery()#
The websearch_to_tsquery() function provides an intuitive search syntax similar to popular web search engines, making it ideal for user-facing search interfaces.

Basic usage#

SQL

JavaScript
select *
from books
where to_tsvector(description) @@ websearch_to_tsquery('english', 'green eggs');
Quoted phrases#
Use quotes to search for exact phrases:

select * from books
where to_tsvector(description || ' ' || title) @@ websearch_to_tsquery('english', '"Green Eggs"');
-- Matches documents containing "Green" immediately followed by "Eggs"
OR searches#
Use "or" (case-insensitive) to search for multiple terms:

select * from books
where to_tsvector(description) @@ websearch_to_tsquery('english', 'puppy or rabbit');
-- Matches documents containing either "puppy" OR "rabbit"
Negation#
Use a dash (-) to exclude terms:

select * from books
where to_tsvector(description) @@ websearch_to_tsquery('english', 'animal -rabbit');
-- Matches documents containing "animal" but NOT "rabbit"
Complex queries#
Combine multiple operators for sophisticated searches:

select * from books
where to_tsvector(description || ' ' || title) @@
  websearch_to_tsquery('english', '"Harry Potter" or "Dr. Seuss" -vegetables');
-- Matches books by "Harry Potter" or "Dr. Seuss" but excludes those mentioning vegetables
Creating indexes#
Now that you have Full Text Search working, create an index. This allows Postgres to "build" the documents preemptively so that they
don't need to be created at the time we execute the query. This will make our queries much faster.

Searchable columns#
Let's create a new column fts inside the books table to store the searchable index of the title and description columns.

We can use a special feature of Postgres called
Generated Columns
to ensure that the index is updated any time the values in the title and description columns change.


SQL

Data
alter table
  books
add column
  fts tsvector generated always as (to_tsvector('english', description || ' ' || title)) stored;
create index books_fts on books using gin (fts); -- generate the index
select id, fts
from books;
Search using the new column#
Now that we've created and populated our index, we can search it using the same techniques as before:


SQL

JavaScript

Dart

Swift

Kotlin

Python

Data
select
  *
from
  books
where
  fts @@ to_tsquery('little & big');
Query operators#
Visit Postgres: Text Search Functions and Operators
to learn about additional query operators you can use to do more advanced full text queries, such as:

Proximity: <->#
The proximity symbol is useful for searching for terms that are a certain "distance" apart.
For example, to find the phrase big dreams, where the a match for "big" is followed immediately by a match for "dreams":


SQL

JavaScript

Dart

Swift

Kotlin

Python
select
  *
from
  books
where
  to_tsvector(description) @@ to_tsquery('big <-> dreams');
We can also use the <-> to find words within a certain distance of each other. For example to find year and school within 2 words of each other:


SQL

JavaScript

Dart

Swift

Kotlin

Python
select
  *
from
  books
where
  to_tsvector(description) @@ to_tsquery('year <2> school');
Negation: !#
The negation symbol can be used to find phrases which don't contain a search term.
For example, to find records that have the word big but not little:


SQL

JavaScript

Dart

Swift

Kotlin

Python
select
  *
from
  books
where
  to_tsvector(description) @@ to_tsquery('big & !little');
Ranking search results #
Postgres provides ranking functions to sort search results by relevance, helping you present the most relevant matches first. Since ranking functions need to be computed server-side, use RPC functions and generated columns.

Creating a search function with ranking #
First, create a Postgres function that handles search and ranking:

create or replace function search_books(search_query text)
returns table(id int, title text, description text, rank real) as $$
begin
  return query
  select
    books.id,
    books.title,
    books.description,
    ts_rank(to_tsvector('english', books.description), to_tsquery(search_query)) as rank
  from books
  where to_tsvector('english', books.description) @@ to_tsquery(search_query)
  order by rank desc;
end;
$$ language plpgsql;
Now you can call this function from your client:


JavaScript

Dart

Python

SQL
const { data, error } = await supabase.rpc('search_books', { search_query: 'big' })
Ranking with weighted columns #
Postgres allows you to assign different importance levels to different parts of your documents using weight labels. This is especially useful when you want matches in certain fields (like titles) to rank higher than matches in other fields (like descriptions).

Understanding weight labels#
Postgres uses four weight labels: A, B, C, and D, where:

A = Highest importance (weight 1.0)
B = High importance (weight 0.4)
C = Medium importance (weight 0.2)
D = Low importance (weight 0.1)
Creating weighted search columns#
First, create a weighted tsvector column that gives titles higher priority than descriptions:

-- Add a weighted fts column
alter table books
add column fts_weighted tsvector
generated always as (
  setweight(to_tsvector('english', title), 'A') ||
  setweight(to_tsvector('english', description), 'B')
) stored;
-- Create index for the weighted column
create index books_fts_weighted on books using gin (fts_weighted);
Now create a search function that uses this weighted column:

create or replace function search_books_weighted(search_query text)
returns table(id int, title text, description text, rank real) as $$
begin
  return query
  select
    books.id,
    books.title,
    books.description,
    ts_rank(books.fts_weighted, to_tsquery(search_query)) as rank
  from books
  where books.fts_weighted @@ to_tsquery(search_query)
  order by rank desc;
end;
$$ language plpgsql;
Custom weight arrays#
You can also specify custom weights by providing a weight array to ts_rank():

create or replace function search_books_custom_weights(search_query text)
returns table(id int, title text, description text, rank real) as $$
begin
  return query
  select
    books.id,
    books.title,
    books.description,
    ts_rank(
      '{0.0, 0.2, 0.5, 1.0}'::real[], -- Custom weights {D, C, B, A}
      books.fts_weighted,
      to_tsquery(search_query)
    ) as rank
  from books
  where books.fts_weighted @@ to_tsquery(search_query)
  order by rank desc;
end;
$$ language plpgsql;
This example uses custom weights where:

A-labeled terms (titles) have maximum weight (1.0)
B-labeled terms (descriptions) have medium weight (0.5)
C-labeled terms have low weight (0.2)
D-labeled terms are ignored (0.0)
Using the weighted search#

JavaScript

Python

SQL
// Search with standard weighted ranking
const { data, error } = await supabase.rpc('search_books_weighted', { search_query: 'Harry' })
// Search with custom weights
const { data: customData, error: customError } = await supabase.rpc('search_books_custom_weights', {
  search_query: 'Harry',
})
Practical example with results#
Say you search for "Harry". With weighted columns:

"Harry Potter and the Goblet of Fire" (title match) gets weight A = 1.0
Books mentioning "Harry" in description get weight B = 0.4
This ensures that books with "Harry" in the title ranks significantly higher than books that only mention "Harry" in the description, providing more relevant search results for users.

Using ranking with indexes #
When using the fts column you created earlier, ranking becomes more efficient. Create a function that uses the indexed column:

create or replace function search_books_fts(search_query text)
returns table(id int, title text, description text, rank real) as $$
begin
  return query
  select
    books.id,
    books.title,
    books.description,
    ts_rank(books.fts, to_tsquery(search_query)) as rank
  from books
  where books.fts @@ to_tsquery(search_query)
  order by rank desc;
end;
$$ language plpgsql;

JavaScript

Dart

Python

SQL
const { data, error } = await supabase.rpc('search_books_fts', { search_query: 'little & big' })
Using web search syntax with ranking #
You can also create a function that combines websearch_to_tsquery() with ranking for user-friendly search:

create or replace function websearch_books(search_text text)
returns table(id int, title text, description text, rank real) as $$
begin
  return query
  select
    books.id,
    books.title,
    books.description,
    ts_rank(books.fts, websearch_to_tsquery('english', search_text)) as rank
  from books
  where books.fts @@ websearch_to_tsquery('english', search_text)
  order by rank desc;
end;
$$ language plpgsql;

JavaScript

SQL
// Support natural search syntax
const { data, error } = await supabase.rpc('websearch_books', {
  search_text: '"little puppy" or train -vegetables',
})
Resources#
Postgres: Text Search Functions and Operators

Partitioning tables

Table partitioning is a technique that allows you to divide a large table into smaller, more manageable parts called “partitions”.

multi database
Each partition contains a subset of the data based on a specified criteria, such as a range of values or a specific condition. Partitioning can significantly improve query performance and simplify data management for large datasets.

Benefits of table partitioning#
Improved query performance: allows queries to target specific partitions, reducing the amount of data scanned and improving query execution time.
Scalability: With partitioning, you can add or remove partitions as your data grows or changes, enabling better scalability and flexibility.
Efficient data management: simplifies tasks such as data loading, archiving, and deletion by operating on smaller partitions instead of the entire table.
Enhanced maintenance operations: can optimize vacuuming and indexing, leading to faster maintenance tasks.
Partitioning methods#
Postgres supports various partitioning methods based on how you want to partition your data. The commonly used methods are:

Range Partitioning: Data is divided into partitions based on a specified range of values. For example, you can partition a sales table by date, where each partition represents a specific time range (e.g., one partition for each month).
List Partitioning: Data is divided into partitions based on a specified list of values. For instance, you can partition a customer table by region, where each partition contains customers from a specific region (e.g., one partition for customers in the US, another for customers in Europe).
Hash Partitioning: Data is distributed across partitions using a hash function. This method provides a way to evenly distribute data among partitions, which can be useful for load balancing. However, it doesn't allow direct querying based on specific values.
Creating partitioned tables#
Let's consider an example of range partitioning for a sales table based on the order date. We'll create monthly partitions to store data for each month:

create table sales (
    id bigint generated by default as identity,
    order_date date not null,
    customer_id bigint,
    amount bigint,
    -- We need to include all the
    -- partitioning columns in constraints:
    primary key (order_date, id)
)
partition by range (order_date);
create table sales_2000_01
	partition of sales
  for values from ('2000-01-01') to ('2000-02-01');
create table sales_2000_02
	partition of sales
	for values from ('2000-02-01') to ('2000-03-01');
To create a partitioned table you append partition by range (<column_name>) to the table creation statement. The column that you are partitioning with must be included in any unique index, which is the reason why we specify a composite primary key here (primary key (order_date, id)).

Querying partitioned tables#
To query a partitioned table, you have two options:

Querying the parent table
Querying specific partitions
Querying the parent table#
When you query the parent table, Postgres automatically routes the query to the relevant partitions based on the conditions specified in the query. This allows you to retrieve data from all partitions simultaneously.

Example:

select *
from sales
where order_date >= '2000-01-01' and order_date < '2000-03-01';
This query will retrieve data from both the sales_2000_01 and sales_2000_02 partitions.

Querying specific partitions#
If you only need to retrieve data from a specific partition, you can directly query that partition instead of the parent table. This approach is useful when you want to target a specific range or condition within a partition.

select *
from sales_2000_02;
This query will retrieve data only from the sales_2000_02 partition.

When to partition your tables#
There is no real threshold to determine when you should use partitions. Partitions introduce complexity, and complexity should be avoided until it's needed. A few guidelines:

If you are considering performance, avoid partitions until you see performance degradation on non-partitioned tables.
If you are using partitions as a management tool, it's fine to create the partitions any time.
If you don't know how you should partition your data, then it's probably too early.
Examples#
Here are simple examples for each of the partitioning types in Postgres.

Range partitioning#
Let's consider a range partitioning example for a table that stores sales data based on the order date. We'll create monthly partitions to store data for each month.

In this example, the sales table is partitioned into two partitions: sales_january and sales_february. The data in these partitions is based on the specified range of order dates:

create table sales (
    id bigint generated by default as identity,
    order_date date not null,
    customer_id bigint,
    amount bigint,
    -- We need to include all the
    -- partitioning columns in constraints:
    primary key (order_date, id)
)
partition by range (order_date);
create table sales_2000_01
	partition of sales
  for values from ('2000-01-01') to ('2000-02-01');
create table sales_2000_02
	partition of sales
	for values from ('2000-02-01') to ('2000-03-01');
List partitioning#
Let's consider a list partitioning example for a table that stores customer data based on their region. We'll create partitions to store customers from different regions.

In this example, the customers table is partitioned into two partitions: customers_americas and customers_asia. The data in these partitions is based on the specified list of regions:

-- Create the partitioned table
create table customers (
    id bigint generated by default as identity,
    name text,
    country text,
    -- We need to include all the
    -- partitioning columns in constraints:
    primary key (country, id)
)
partition by list(country);
create table customers_americas
	partition of customers
	for values in ('US', 'CANADA');
create table customers_asia
	partition of customers
  for values in ('INDIA', 'CHINA', 'JAPAN');
Hash partitioning#
You can use hash partitioning to evenly distribute data.

In this example, the products table is partitioned into two partitions: products_one and products_two. The data is distributed across these partitions using a hash function:

create table products (
    id bigint generated by default as identity,
    name text,
    category text,
    price bigint
)
partition by hash (id);
create table products_one
	partition of products
  for values with (modulus 2, remainder 1);
create table products_two
	partition of products
  for values with (modulus 2, remainder 0);
Other tools#
There are several other tools available for Postgres partitioning, most notably pg_partman. Native partitioning was introduced in Postgres 10 and is generally thought to have better performance.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Benefits of table partitioning
Partitioning methods
Creating partitioned tables
Querying partitioned tables
Querying the parent table
Querying specific partitions
When to partition your tables
Examples
Range partitioning
List partitioning
Hash partitioning
Other tools
Need some help?

Contact support
Connection management

Using your connections resourcefully

Connections#
Every Compute Add-On has a pre-configured direct connection count and Supavisor pool size. This guide discusses ways to observe and manage them resourcefully.

Configuring Supavisor's pool size#
You can change how many database connections Supavisor can manage by altering the pool size in the "Connection pooling configuration" section of the Database Settings:

Connection Info and Certificate.

The general rule is that if you are heavily using the PostgREST database API, you should be conscientious about raising your pool size past 40%. Otherwise, you can commit 80% to the pool. This leaves adequate room for the Authentication server and other utilities.

These numbers are generalizations and depends on other Supabase products that you use and the extent of their usage. The actual values depend on your concurrent peak connection usage. For instance, if you were only using 80 connections in a week period and your database max connections is set to 500, then realistically you could allocate the difference of 420 (minus a reasonable buffer) to service more demand.

Monitoring connections#
Capturing historical usage#
Dashboard monitoring charts#
Database client connections chart
For Teams and Enterprise plans, Supabase provides Advanced Telemetry charts directly within the Dashboard. The Database client connections chart displays historical connection data broken down by connection type:

Postgres: Direct connections from your application
PostgREST: Connections from the PostgREST API layer
Reserved: Administrative connections for Supabase services
Auth: Connections from Supabase Auth service
Storage: Connections from Supabase Storage service
Other roles: Miscellaneous database connections
This chart helps you monitor connection pool usage, identify connection leaks, and plan capacity. It also shows a reference line for your compute size's maximum connection limit.

For more details on using these monitoring charts, see the Reports guide.

Grafana Dashboard#
Supabase offers a Grafana Dashboard that records and visualizes over 200 project metrics, including connections. For setup instructions, check the metrics docs.

Its "Client Connections" graph displays connections for both Supavisor and Postgres
client connection graph

Observing live connections#
pg_stat_activity is a special view that keeps track of processes being run by your database, including live connections. It's particularly useful for determining if idle clients are hogging connection slots.

Query to get all live connections:

SELECT
  pg_stat_activity.pid as connection_id,
  ssl,
  datname as database,
  usename as connected_role,
  application_name,
  client_addr as IP,
  query,
  query_start,
  state,
  backend_start
FROM pg_stat_ssl
JOIN pg_stat_activity
ON pg_stat_ssl.pid = pg_stat_activity.pid;
Interpreting the query:

Column	Description
connection_id	connection id
ssl	Indicates if SSL is in use
database	Name of the connected database (usually postgres)
usename	Role of the connected user
application_name	Name of the connecting application
client_addr	IP address of the connecting server
query	Last query executed by the connection
query_start	Time when the last query was executed
state	Querying state: active or idle
backend_start	Timestamp of the connection's establishment
The username can be used to identify the source:

Role	API/Tool
supabase_admin	Used by Supabase for monitoring and by Realtime
authenticator	Data API (PostgREST)
supabase_auth_admin	Auth
supabase_storage_admin	Storage
supabase_replication_admin	Synchronizes Read Replicas
postgres	Supabase Dashboard and External Tools (e.g., Prisma, SQLAlchemy, PSQL...)
Custom roles defined by user	External Tools (e.g., Prisma, SQLAlchemy, PSQL...)
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Connections
Configuring Supavisor's pool size
Monitoring connections
Capturing historical usage
Observing live connections
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author 

Database
Working with your database (intermediate)
Managing event triggers
Event Triggers

Automatically execute SQL on database events.

In Postgres, an event trigger is similar to a trigger, except that it is triggered by database level events (and is usually reserved for superusers)

With our Supautils extension (installed automatically for all Supabase projects), the postgres user has the ability to create and manage event triggers.

Some use cases for event triggers are:

Capturing Data Definition Language (DDL) changes - these are changes to your database schema (though the pgAudit extension provides a more complete solution)
Enforcing/monitoring/preventing actions - such as preventing tables from being dropped in Production or enforcing RLS on all new tables
The guide covers two example event triggers:

Preventing accidental dropping of a table
Automatically enabling Row Level Security on new tables in the public schema
Creating an event trigger#
Only the postgres user can create event triggers, so make sure you are authenticated as them. As with triggers, event triggers consist of 2 parts

A Function which will be executed when the triggering event occurs
The actual Event Trigger object, with parameters around when the trigger should be run
Example trigger function - prevent dropping tables#
This example protects any table from being dropped. You can override it by temporarily disabling the event trigger: ALTER EVENT TRIGGER dont_drop_trigger DISABLE;

-- Function
CREATE OR REPLACE FUNCTION dont_drop_function()
  RETURNS event_trigger LANGUAGE plpgsql AS $$
DECLARE
    obj record;
    tbl_name text;
BEGIN
    FOR obj IN SELECT * FROM pg_event_trigger_dropped_objects()
    LOOP
        IF obj.object_type = 'table' THEN
            RAISE EXCEPTION 'ERROR: All tables in this schema are protected and cannot be dropped';
        END IF;
    END LOOP;
END;
$$;
-- Event trigger
CREATE EVENT TRIGGER dont_drop_trigger
ON sql_drop
EXECUTE FUNCTION dont_drop_function();
Example trigger function - auto enable Row Level Security#
CREATE OR REPLACE FUNCTION rls_auto_enable()
RETURNS EVENT_TRIGGER
LANGUAGE plpgsql
SECURITY DEFINER
SET search_path = pg_catalog
AS $$
DECLARE
  cmd record;
BEGIN
  FOR cmd IN
    SELECT *
    FROM pg_event_trigger_ddl_commands()
    WHERE command_tag IN ('CREATE TABLE', 'CREATE TABLE AS', 'SELECT INTO')
      AND object_type IN ('table','partitioned table')
  LOOP
     IF cmd.schema_name IS NOT NULL AND cmd.schema_name IN ('public') AND cmd.schema_name NOT IN ('pg_catalog','information_schema') AND cmd.schema_name NOT LIKE 'pg_toast%' AND cmd.schema_name NOT LIKE 'pg_temp%' THEN
      BEGIN
        EXECUTE format('alter table if exists %s enable row level security', cmd.object_identity);
        RAISE LOG 'rls_auto_enable: enabled RLS on %', cmd.object_identity;
      EXCEPTION
        WHEN OTHERS THEN
          RAISE LOG 'rls_auto_enable: failed to enable RLS on %', cmd.object_identity;
      END;
     ELSE
        RAISE LOG 'rls_auto_enable: skip % (either system schema or not in enforced list: %.)', cmd.object_identity, cmd.schema_name;
     END IF;
  END LOOP;
END;
$$;
DROP EVENT TRIGGER IF EXISTS ensure_rls;
CREATE EVENT TRIGGER ensure_rls
ON ddl_command_end
WHEN TAG IN ('CREATE TABLE', 'CREATE TABLE AS', 'SELECT INTO')
EXECUTE FUNCTION rls_auto_enable();
Event trigger Functions and firing events#
Event triggers can be triggered on:

ddl_command_start - occurs just before a DDL command for almost all objects within a schema
ddl_command_end - occurs just after a DDL command for almost all objects within a schema
sql_drop - occurs just before ddl_command_end for any DDL commands that DROP a database object (note that altering a table can cause it to be dropped)
table_rewrite - occurs just before a table is rewritten using the ALTER TABLE command
Event triggers run for each DDL command specified above and can consume resources which may cause performance issues if not used carefully.

Within each event trigger, helper functions exist to view the objects being modified or the command being run. For example, our example calls pg_event_trigger_dropped_objects() to view the object(s) being dropped. For a more comprehensive overview of these functions, read the official event trigger definition documentation

To view the matrix commands that cause an event trigger to fire, read the official event trigger matrix documentation

Disabling an event trigger#
You can disable an event trigger using the alter event trigger command:

ALTER EVENT TRIGGER dont_drop_trigger DISABLE;
Dropping an event trigger#
You can delete a trigger using the drop event trigger command:

DROP EVENT TRIGGER dont_drop_trigger;
Resources#
Official Postgres Docs: Event Trigger Behaviours
Official Postgres Docs: Event Trigger Firing Matrix
Supabase blog: Postgres Event Triggers without superuser access
Edit this page on GitHub

OrioleDB Overview

The OrioleDB Postgres extension provides a drop-in replacement storage engine for the default heap storage method. It is designed to improve Postgres' scalability and performance.

OrioleDB addresses PostgreSQL's scalability limitations by removing bottlenecks in the shared memory cache under high concurrency. It also optimizes write-ahead-log (WAL) insertion through row-level WAL logging. These changes lead to significant improvements in the industry standard TPC-C benchmark, which approximates a real-world transactional workload. The following benchmark was performed on a c7g.metal instance and shows OrioleDB's performance outperforming the default Postgres heap method with a 3.3x speedup.

TPC-C (warehouses = 500)

OrioleDB is in active development and currently has certain limitations. Currently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available. An Index Access Method bridge to unlock support for all index types used with heap storage is under active development. In the Supabase OrioleDB image the default storage method has been updated to use OrioleDB, granting better performance out of the box.

Concepts#
Index-organized tables#
OrioleDB uses index-organized tables, where table data is stored in the index structure. This design eliminates the need for separate heap storage, reduces overhead and improves lookup performance for primary key queries.

No buffer mapping#
In-memory pages are connected to the storage pages using direct links. This allows OrioleDB to bypass PostgreSQL's shared buffer pool and eliminate the associated complexity and contention in buffer mapping.

Undo log#
Multi-Version Concurrency Control (MVCC) is implemented using an undo log. The undo log stores previous row versions and transaction information, which enables consistent reads while removing the need for table vacuuming completely.

Copy-on-write checkpoints#
OrioleDB implements copy-on-write checkpoints to persist data efficiently. This approach writes only modified data during a checkpoint, reducing the I/O overhead compared to traditional Postgres checkpointing and allowing row-level WAL logging.

Usage#
Creating OrioleDB project#
You can get started with OrioleDB by enabling the extension in your Supabase dashboard.
To get started with OrioleDB you need to create a new Supabase project and choose OrioleDB Public Alpha Postgres version.

Creating OrioleDB project

Creating tables#
To create a table using the OrioleDB storage engine just execute the standard CREATE TABLE statement. By default it will create a table using OrioleDB storage engine. For example:

-- Create a table
create table blog_post (
  id int8 not null,
  title text not null,
  body text not null,
  author text not null,
  published_at timestamptz not null default CURRENT_TIMESTAMP,
  views bigint not null,
  primary key (id)
);
Creating indexes#
OrioleDB tables always have a primary key. If it wasn't defined explicitly, a hidden primary key is created using the ctid column.
Additionally you can create secondary indexes.

Currently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available.

-- Create an index
create index blog_post_published_at on blog_post (published_at);
create index blog_post_views on blog_post (views) where (views > 1000);
Data manipulation#
You can query and modify data in OrioleDB tables using standard SQL statements, including SELECT, INSERT, UPDATE, DELETE and INSERT ... ON CONFLICT.

INSERT INTO blog_post (id, title, body, author, views)
VALUES (1, 'Hello, World!', 'This is my first blog post.', 'John Doe', 1000);
SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;
 id │     title     │            body             │  author  │         published_at          │ views
────┼───────────────┼─────────────────────────────┼──────────┼───────────────────────────────┼───────
  1 │ Hello, World! │ This is my first blog post. │ John Doe │ 2024-11-15 12:04:18.756824+01 │  1000
Viewing query plans#
You can see the execution plan using standard EXPLAIN statement.

EXPLAIN SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;
                                                 QUERY PLAN
────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Limit  (cost=0.15..1.67 rows=10 width=120)
   ->  Index Scan Backward using blog_post_published_at on blog_post  (cost=0.15..48.95 rows=320 width=120)
EXPLAIN SELECT * FROM blog_post WHERE id = 1;
                                    QUERY PLAN
──────────────────────────────────────────────────────────────────────────────────
 Index Scan using blog_post_pkey on blog_post  (cost=0.15..8.17 rows=1 width=120)
   Index Cond: (id = 1)
EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;
                                                                      QUERY PLAN
──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Limit  (cost=0.15..1.67 rows=10 width=120) (actual time=0.052..0.054 rows=1 loops=1)
   ->  Index Scan Backward using blog_post_published_at on blog_post  (cost=0.15..48.95 rows=320 width=120) (actual time=0.050..0.052 rows=1 loops=1)
 Planning Time: 0.186 ms
 Execution Time: 0.088 ms
Resources#

Row Level Security

Secure your data using Postgres Row Level Security.

When you need granular authorization rules, nothing beats Postgres's Row Level Security (RLS).

Row Level Security in Supabase#
Supabase allows convenient and secure data access from the browser, as long as you enable RLS.

RLS must always be enabled on any tables stored in an exposed schema. By default, this is the public schema.

RLS is enabled by default on tables created with the Table Editor in the dashboard. If you create one in raw SQL or with the SQL editor, remember to enable RLS yourself:

alter table <schema_name>.<table_name>
enable row level security;
RLS is incredibly powerful and flexible, allowing you to write complex SQL rules that fit your unique business needs. RLS can be combined with Supabase Auth for end-to-end user security from the browser to the database.

RLS is a Postgres primitive and can provide "defense in depth" to protect your data from malicious actors even when accessed through third-party tooling.

Policies#
Policies are Postgres's rule engine. Policies are easy to understand once you get the hang of them. Each policy is attached to a table, and the policy is executed every time a table is accessed.

You can just think of them as adding a WHERE clause to every query. For example a policy like this ...

create policy "Individuals can view their own todos."
on todos for select
using ( (select auth.uid()) = user_id );
.. would translate to this whenever a user tries to select from the todos table:

select *
from todos
where auth.uid() = todos.user_id;
-- Policy is implicitly added.
Enabling Row Level Security#
You can enable RLS for any table using the enable row level security clause:

alter table "table_name" enable row level security;
Once you have enabled RLS, no data will be accessible via the API when using the public anon key, until you create policies.

`auth.uid()` Returns `null` When Unauthenticated
When a request is made without an authenticated user (e.g., no access token is provided or the session has expired), auth.uid() returns null.

This means that a policy like:

USING (auth.uid() = user_id)
will silently fail for unauthenticated users, because:

null = user_id
is always false in SQL.

To avoid confusion and make your intention clear, we recommend explicitly checking for authentication:

USING (auth.uid() IS NOT NULL AND auth.uid() = user_id)
Authenticated and unauthenticated roles#
Supabase maps every request to one of the roles:

anon: an unauthenticated request (the user is not logged in)
authenticated: an authenticated request (the user is logged in)
These are actually Postgres Roles. You can use these roles within your Policies using the TO clause:

create policy "Profiles are viewable by everyone"
on profiles for select
to authenticated, anon
using ( true );
-- OR
create policy "Public profiles are viewable only by authenticated users"
on profiles for select
to authenticated
using ( true );
Anonymous user vs the anon key
Using the anon Postgres role is different from an anonymous user in Supabase Auth. An anonymous user assumes the authenticated role to access the database and can be differentiated from a permanent user by checking the is_anonymous claim in the JWT.

Creating policies#
Policies are SQL logic that you attach to a Postgres table. You can attach as many policies as you want to each table.

Supabase provides some helpers that simplify RLS if you're using Supabase Auth. We'll use these helpers to illustrate some basic policies:

SELECT policies#
You can specify select policies with the using clause.

Let's say you have a table called profiles in the public schema and you want to enable read access to everyone.

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Public profiles are visible to everyone."
on profiles for select
to anon         -- the Postgres Role (recommended)
using ( true ); -- the actual Policy
Alternatively, if you only wanted users to be able to see their own profiles:

create policy "User can see their own profile only."
on profiles
for select using ( (select auth.uid()) = user_id );
INSERT policies#
You can specify insert policies with the with check clause. The with check expression ensures that any new row data adheres to the policy constraints.

Let's say you have a table called profiles in the public schema and you only want users to be able to create a profile for themselves. In that case, we want to check their User ID matches the value that they are trying to insert:

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can create a profile."
on profiles for insert
to authenticated                          -- the Postgres Role (recommended)
with check ( (select auth.uid()) = user_id );      -- the actual Policy
UPDATE policies#
You can specify update policies by combining both the using and with check expressions.

The using clause represents the condition that must be true for the update to be allowed, and with check clause ensures that the updates made adhere to the policy constraints.

Let's say you have a table called profiles in the public schema and you only want users to be able to update their own profile.

You can create a policy where the using clause checks if the user owns the profile being updated. And the with check clause ensures that, in the resultant row, users do not change the user_id to a value that is not equal to their User ID, maintaining that the modified profile still meets the ownership condition.

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can update their own profile."
on profiles for update
to authenticated                    -- the Postgres Role (recommended)
using ( (select auth.uid()) = user_id )       -- checks if the existing row complies with the policy expression
with check ( (select auth.uid()) = user_id ); -- checks if the new row complies with the policy expression
If no with check expression is defined, then the using expression will be used both to determine which rows are visible (normal USING case) and which new rows will be allowed to be added (WITH CHECK case).

To perform an UPDATE operation, a corresponding SELECT policy is required. Without a SELECT policy, the UPDATE operation will not work as expected.

DELETE policies#
You can specify delete policies with the using clause.

Let's say you have a table called profiles in the public schema and you only want users to be able to delete their own profile:

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can delete a profile."
on profiles for delete
to authenticated                     -- the Postgres Role (recommended)
using ( (select auth.uid()) = user_id );      -- the actual Policy
Views#
Views bypass RLS by default because they are usually created with the postgres user. This is a feature of Postgres, which automatically creates views with security definer.

In Postgres 15 and above, you can make a view obey the RLS policies of the underlying tables when invoked by anon and authenticated roles by setting security_invoker = true.

create view <VIEW_NAME>
with(security_invoker = true)
as select <QUERY>
In older versions of Postgres, protect your views by revoking access from the anon and authenticated roles, or by putting them in an unexposed schema.

Helper functions#
Supabase provides some helper functions that make it easier to write Policies.

auth.uid()#
Returns the ID of the user making the request.

auth.jwt()#
Not all information present in the JWT should be used in RLS policies. For instance, creating an RLS policy that relies on the user_metadata claim can create security issues in your application as this information can be modified by authenticated end users.

Returns the JWT of the user making the request. Anything that you store in the user's raw_app_meta_data column or the raw_user_meta_data column will be accessible using this function. It's important to know the distinction between these two:

raw_user_meta_data - can be updated by the authenticated user using the supabase.auth.update() function. It is not a good place to store authorization data.
raw_app_meta_data - cannot be updated by the user, so it's a good place to store authorization data.
The auth.jwt() function is extremely versatile. For example, if you store some team data inside app_metadata, you can use it to determine whether a particular user belongs to a team. For example, if this was an array of IDs:

create policy "User is in team"
on my_table
to authenticated
using ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));
Keep in mind that a JWT is not always "fresh". In the example above, even if you remove a user from a team and update the app_metadata field, that will not be reflected using auth.jwt() until the user's JWT is refreshed.

Also, if you are using Cookies for Auth, then you must be mindful of the JWT size. Some browsers are limited to 4096 bytes for each cookie, and so the total size of your JWT should be small enough to fit inside this limitation.

MFA#
The auth.jwt() function can be used to check for Multi-Factor Authentication. For example, you could restrict a user from updating their profile unless they have at least 2 levels of authentication (Assurance Level 2):

create policy "Restrict updates."
on profiles
as restrictive
for update
to authenticated using (
  (select auth.jwt()->>'aal') = 'aal2'
);
Bypassing Row Level Security#
Supabase provides special "Service" keys, which can be used to bypass RLS. These should never be used in the browser or exposed to customers, but they are useful for administrative tasks.

Supabase will adhere to the RLS policy of the signed-in user, even if the client library is initialized with a Service Key.

You can also create new Postgres Roles which can bypass Row Level Security using the "bypass RLS" privilege:

alter role "role_name" with bypassrls;
This can be useful for system-level access. You should never share login credentials for any Postgres Role with this privilege.

RLS performance recommendations#
Every authorization system has an impact on performance. While row level security is powerful, the performance impact is important to keep in mind. This is especially true for queries that scan every row in a table - like many select operations, including those using limit, offset, and ordering.

Based on a series of tests, we have a few recommendations for RLS:

Add indexes#
Make sure you've added indexes on any columns used within the Policies which are not already indexed (or primary keys). For a Policy like this:

create policy "rls_test_select" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
You can add an index like:

create index userid
on test_table
using btree (user_id);
Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test1-indexed	171	< 0.1	99.94%	
Detalles
Call functions with select#
You can use select statement to improve policies that use functions. For example, instead of this:

create policy "rls_test_select" on test_table
to authenticated
using ( auth.uid() = user_id );
You can do:

create policy "rls_test_select" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
This method works well for JWT functions like auth.uid() and auth.jwt() as well as security definer Functions. Wrapping the function causes an initPlan to be run by the Postgres optimizer, which allows it to "cache" the results per-statement, rather than calling the function on each row.

You can only use this technique if the results of the query or function do not change based on the row data.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test2a-wrappedSQL-uid	179	9	94.97%	
Detalles
test2b-wrappedSQL-isadmin	11,000	7	99.94%	
Detalles
test2c-wrappedSQL-two-functions	11,000	10	99.91%	
Detalles
test2d-wrappedSQL-sd-fun	178,000	12	99.993%	
Detalles
test2e-wrappedSQL-sd-fun-array	173000	16	99.991%	
Detalles
Add filters to every query#
Policies are "implicit where clauses," so it's common to run select statements without any filters. This is a bad pattern for performance. Instead of doing this (JS client example):

const { data } = supabase
  .from('table')
  .select()
You should always add a filter:

const { data } = supabase
  .from('table')
  .select()
  .eq('user_id', userId)
Even though this duplicates the contents of the Policy, Postgres can use the filter to construct a better query plan.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test3-addfilter	171	9	94.74%	
Detalles
Use security definer functions#
A "security definer" function runs using the same role that created the function. This means that if you create a role with a superuser (like postgres), then that function will have bypassrls privileges. For example, if you had a policy like this:

create policy "rls_test_select" on test_table
to authenticated
using (
  exists (
    select 1 from roles_table
    where (select auth.uid()) = user_id and role = 'good_role'
  )
);
We can instead create a security definer function which can scan roles_table without any RLS penalties:

create function private.has_good_role()
returns boolean
language plpgsql
security definer -- will run as the creator
as $$
begin
  return exists (
    select 1 from roles_table
    where (select auth.uid()) = user_id and role = 'good_role'
  );
end;
$$;
-- Update our policy to use this function:
create policy "rls_test_select"
on test_table
to authenticated
using ( (select private.has_good_role()) );
Security-definer functions should never be created in a schema in the "Exposed schemas" inside your API settings`.

Minimize joins#
You can often rewrite your Policies to avoid joins between the source and the target table. Instead, try to organize your policy to fetch all the relevant data from the target table into an array or set, then you can use an IN or ANY operation in your filter.

For example, this is an example of a slow policy which joins the source test_table to the target team_user:

create policy "rls_test_select" on test_table
to authenticated
using (
  (select auth.uid()) in (
    select user_id
    from team_user
    where team_user.team_id = team_id -- joins to the source "test_table.team_id"
  )
);
We can rewrite this to avoid this join, and instead select the filter criteria into a set:

create policy "rls_test_select" on test_table
to authenticated
using (
  team_id in (
    select team_id
    from team_user
    where user_id = (select auth.uid()) -- no join
  )
);
In this case you can also consider using a security definer function to bypass RLS on the join table:

If the list exceeds 1000 items, a different approach may be needed or you may need to analyze the approach to ensure that the performance is acceptable.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test5-fixed-join	9,000	20	99.78%	
Detalles
Specify roles in your policies#
Always use the Role of inside your policies, specified by the TO operator. For example, instead of this query:

create policy "rls_test_select" on rls_test
using ( auth.uid() = user_id );
Use:

create policy "rls_test_select" on rls_test
to authenticated
using ( (select auth.uid()) = user_id );
This prevents the policy ( (select auth.uid()) = user_id ) from running for any anon users, since the execution stops at the to authenticated step.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test6-To-role	170	< 0.1	99.78%	
Detalles
More resources#
Testing your database
Row Level Security and Supabase Auth
RLS Guide and Best Practices
Community repo on testing RLS using pgTAP and dbdev

Column Level Security

PostgreSQL's Row Level Security (RLS) gives you granular control over who can access rows of data. However, it doesn't give you control over which columns they can access within rows. Sometimes you want to restrict access to specific columns in your database. Column Level Privileges allows you to do just that.

This is an advanced feature. We do not recommend using column-level privileges for most users. Instead, we recommend using RLS policies in combination with a dedicated table for handling user roles.

Restricted roles cannot use the wildcard operator (*) on the affected table. Instead of using SELECT * FROM <restricted_table>; or its API equivalent, you must specify the column names explicitly.

Policies at the row level#
Policies in Row Level Security (RLS) are used to restrict access to rows in a table. Think of them like adding a WHERE clause to every query.

For example, let's assume you have a posts table with the following columns:

id
user_id
title
content
created_at
updated_at
You can restrict updates to just the user who created it using RLS, with the following policy:

create policy "Allow update for owners" on posts for
update
  using ((select auth.uid()) = user_id);
However, this gives the post owner full access to update the row, including all of the columns.

Privileges at the column level#
To restrict access to columns, you can use Privileges.

There are two types of privileges in Postgres:

table-level: Grants the privilege on all columns in the table.
column-level Grants the privilege on a specific column in the table.
You can have both types of privileges on the same table. If you have both, and you revoke the column-level privilege, the table-level privilege will still be in effect.

By default, our table will have a table-level UPDATE privilege, which means that the authenticated role can update all the columns in the table.

revoke
update
  on table public.posts
from
  authenticated;
grant
update
  (title, content) on table public.posts to authenticated;
In the above example, we are revoking the table-level UPDATE privilege from the authenticated role and granting a column-level UPDATE privilege on just the title and content columns.

If we want to restrict access to updating the title column:

revoke
update
  (title) on table public.posts
from
  authenticated;
This time, we are revoking the column-level UPDATE privilege of the title column from the authenticated role. We didn't need to revoke the table-level UPDATE privilege because it's already revoked.

Manage column privileges in the Dashboard#
Column-level privileges are a powerful tool, but they're also quite advanced and in many cases, not the best fit for common access control needs. For that reason, we've intentionally moved the UI for this feature under the Feature Preview section in the dashboard.

You can view and edit the privileges in the Supabase Studio.

Column level privileges

Manage column privileges in migrations#
While you can manage privileges directly from the Dashboard, as your project grows you may want to manage them in your migrations. Read about database migrations in the Local Development guide.

1
Create a migration file
To get started, generate a new migration to store the SQL needed to create your table along with row and column-level privileges.

supabase migration new create_posts_table
2
Add the SQL to your migration file
This creates a new migration: supabase/migrations/<timestamp>
_create_posts_table.sql.

To that file, add the SQL to create this posts table with row and column-level privileges.

create table
posts (
id bigint primary key generated always as identity,
user_id text,
title text,
content text,
created_at timestamptz default now()
updated_at timestamptz default now()
);
-- Add row-level security
create policy "Allow update for owners" on posts for
update
using ((select auth.uid()) = user_id);
-- Add column-level security
revoke
update
(title) on table public.posts
from
authenticated;
Considerations when using column-level privileges#
If you turn off a column privilege you won't be able to use that column at all.
All operations (insert, update, delete) as well as using select * will fail.

Hardening the Data API

Your database's auto-generated Data API exposes the public schema by default. You can change this to any schema in your database, or even disable the Data API completely.

Any tables that are accessible through the Data API must have Row Level Security enabled. Row Level Security (RLS) is enabled by default when you create tables from the Supabase Dashboard. If you create a table using the SQL editor or your own SQL client or migration runner, youmust enable RLS yourself.

Shared responsibility#
Your application's security is your responsibility as a developer. This includes RLS, falling under the Shared Responsibility model. To help you:

Supabase sends daily emails warning of any tables that are exposed to the Data API which do not have RLS enabled.
Supabase provides a Security Advisor and other tools in the Supabase Dashboard to fix any issues.
Private schemas#
We highly recommend creating a private schema for storing tables that you do not want to expose via the Data API. These tables can be accessed via Supabase Edge Functions or any other serverside tool. In this model, you should implement your security model in your serverside code. Although it's not required, we still recommend enabling RLS for private tables and then connecting to your database using a Postgres role with bypassrls privileges.

Managing the public schema#
If your public schema is used by other tools as a default space, you might want to lock down this schema. This helps prevent accidental exposure of data that's automatically added to public.

There are two levels of security hardening for the Data API:

Disabling the Data API entirely. This is recommended if you never need to access your database via Supabase client libraries or the REST and GraphQL endpoints.
Removing the public schema from the Data API and replacing it with a custom schema (such as api).
Disabling the Data API#
You can disable the Data API entirely if you never intend to use the Supabase client libraries or the REST and GraphQL data endpoints. For example, if you only access your database via a direct connection on the server, disabling the Data API gives you the greatest layer of protection.

Go to API Settings in the Supabase Dashboard.
Under Data API Settings, toggle Enable Data API off.
Exposing a custom schema instead of public#
If you want to use the Data API but with increased security, you can expose a custom schema instead of public. By not using public, which is often used as a default space and has laxer default permissions, you get more conscious control over your exposed data.

Any data, views, or functions that should be exposed need to be deliberately put within your custom schema (which we will call api), rather than ending up there by default.

Step 1: Remove public from exposed schemas#
Go to API Settings in the Supabase Dashboard.
Under Data API Settings, remove public from Exposed schemas. Also remove public from Extra search path.
Click Save.
Go to Database Extensions and disable the pg_graphql extension.
Step 2: Create an api schema and expose it#
Connect to your database. You can use psql, the Supabase SQL Editor, or the Postgres client of your choice.

Create a new schema named api:

create schema if not exists api;
Grant the anon and authenticated roles usage on this schema.

grant usage on schema api to anon, authenticated;
Go to API Settings in the Supabase Dashboard.

Under Data API Settings, add api to Exposed schemas. Make sure it is the first schema in the list, so that it will be searched first by default.

Under these new settings, anon and authenticated can execute functions defined in the api schema, but they have no automatic permissions on any tables. On a table-by-table basis, you can grant them permissions. For example:

grant select on table api.<your_table> to anon;
grant select, insert, update, delete on table api.<your_table> to authenticated;
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Shared responsibility
Private schemas
Managing the public schema
Disabling the Data API
Exposing a custom schema instead of public
Step 1: Remove public from exposed schemas
Step 2: Create an api schema and expose it
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Access and security
Custom Claims & RBAC
Custom Claims & Role-based Access Control (RBAC)

Custom Claims are special attributes attached to a user that you can use to control access to portions of your application. For example:

{
  "user_role": "admin",
  "plan": "TRIAL",
  "user_level": 100,
  "group_name": "Super Guild!",
  "joined_on": "2022-05-20T14:28:18.217Z",
  "group_manager": false,
  "items": ["toothpick", "string", "ring"]
}
To implement Role-Based Access Control (RBAC) with custom claims, use a Custom Access Token Auth Hook. This hook runs before a token is issued. You can use it to add additional claims to the user's JWT.

This guide uses the Slack Clone example to demonstrate how to add a user_role claim and use it in your Row Level Security (RLS) policies.

Create a table to track user roles and permissions#
In this example, you will implement two user roles with specific permissions:

moderator: A moderator can delete all messages but not channels.
admin: An admin can delete all messages and channels.
-- Custom types
create type public.app_permission as enum ('channels.delete', 'messages.delete');
create type public.app_role as enum ('admin', 'moderator');
-- USER ROLES
create table public.user_roles (
  id        bigint generated by default as identity primary key,
  user_id   uuid references auth.users on delete cascade not null,
  role      app_role not null,
  unique (user_id, role)
);
comment on table public.user_roles is 'Application roles for each user.';
-- ROLE PERMISSIONS
create table public.role_permissions (
  id           bigint generated by default as identity primary key,
  role         app_role not null,
  permission   app_permission not null,
  unique (role, permission)
);
comment on table public.role_permissions is 'Application permissions for each role.';
For the full schema, see the example application on GitHub.

You can now manage your roles and permissions in SQL. For example, to add the mentioned roles and permissions from above, run:

insert into public.role_permissions (role, permission)
values
  ('admin', 'channels.delete'),
  ('admin', 'messages.delete'),
  ('moderator', 'messages.delete');
Create Auth Hook to apply user role#
The Custom Access Token Auth Hook runs before a token is issued. You can use it to edit the JWT.


PL/pgSQL (best performance)
-- Create the auth hook function
create or replace function public.custom_access_token_hook(event jsonb)
returns jsonb
language plpgsql
stable
as $$
  declare
    claims jsonb;
    user_role public.app_role;
  begin
    -- Fetch the user role in the user_roles table
    select role into user_role from public.user_roles where user_id = (event->>'user_id')::uuid;
    claims := event->'claims';
    if user_role is not null then
      -- Set the claim
      claims := jsonb_set(claims, '{user_role}', to_jsonb(user_role));
    else
      claims := jsonb_set(claims, '{user_role}', 'null');
    end if;
    -- Update the 'claims' object in the original event
    event := jsonb_set(event, '{claims}', claims);
    -- Return the modified or original event
    return event;
  end;
$$;
grant usage on schema public to supabase_auth_admin;
grant execute
  on function public.custom_access_token_hook
  to supabase_auth_admin;
revoke execute
  on function public.custom_access_token_hook
  from authenticated, anon, public;
grant all
  on table public.user_roles
to supabase_auth_admin;
revoke all
  on table public.user_roles
  from authenticated, anon, public;
create policy "Allow auth admin to read user roles" ON public.user_roles
as permissive for select
to supabase_auth_admin
using (true)
Enable the hook#
In the dashboard, navigate to Authentication > Hooks (Beta) and select the appropriate Postgres function from the dropdown menu.

When developing locally, follow the local development instructions.

To learn more about Auth Hooks, see the Auth Hooks docs.

Accessing custom claims in RLS policies#
To utilize Role-Based Access Control (RBAC) in Row Level Security (RLS) policies, create an authorize method that reads the user's role from their JWT and checks the role's permissions:

create or replace function public.authorize(
  requested_permission app_permission
)
returns boolean as $$
declare
  bind_permissions int;
  user_role public.app_role;
begin
  -- Fetch user role once and store it to reduce number of calls
  select (auth.jwt() ->> 'user_role')::public.app_role into user_role;
  select count(*)
  into bind_permissions
  from public.role_permissions
  where role_permissions.permission = requested_permission
    and role_permissions.role = user_role;
  return bind_permissions > 0;
end;
$$ language plpgsql stable security definer set search_path = '';
You can read more about using functions in RLS policies in the RLS guide.

You can then use the authorize method within your RLS policies. For example, to enable the desired delete access, you would add the following policies:

create policy "Allow authorized delete access" on public.channels for delete to authenticated using ( (SELECT authorize('channels.delete')) );
create policy "Allow authorized delete access" on public.messages for delete to authenticated using ( (SELECT authorize('messages.delete')) );
Accessing custom claims in your application#
The auth hook will only modify the access token JWT but not the auth response. Therefore, to access the custom claims in your application, e.g. your browser client, or server-side middleware, you will need to decode the access_token JWT on the auth session.

In a JavaScript client application you can for example use the jwt-decode package:

import { jwtDecode } from 'jwt-decode'
const { subscription: authListener } = supabase.auth.onAuthStateChange(async (event, session) => {
  if (session) {
    const jwt = jwtDecode(session.access_token)
    const userRole = jwt.user_role
  }
})
For server-side logic you can use packages like express-jwt, koa-jwt, PyJWT, dart_jsonwebtoken, Microsoft.AspNetCore.Authentication.JwtBearer, etc.

Conclusion#
You now have a robust system in place to manage user roles and permissions within your database that automatically propagates to Supabase Auth.

More resources#
Auth Hooks
Row Level Security
RLS Functions
Next.js Slack Clone Example
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Create a table to track user roles and permissions
Create Auth Hook to apply user role
Enable the hook
Accessing custom claims in RLS policies
Accessing custom claims in your application
Conclusion
More resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
Access and security
Managing Postgres Roles
Postgres Roles

Managing access to your Postgres database and configuring permissions.

Postgres manages database access permissions using the concept of roles. Generally you wouldn't use these roles for your own application - they are mostly for configuring system access to your database. If you want to configure application access, then you should use Row Level Security (RLS). You can also implement Role-based Access Control on top of RLS.

Users vs roles#
In Postgres, roles can function as users or groups of users. Users are roles with login privileges, while groups (also known as role groups) are roles that don't have login privileges but can be used to manage permissions for multiple users.

Creating roles#
You can create a role using the create role command:

create role "role_name";
Creating users#
Roles and users are essentially the same in Postgres, however if you want to use password-logins for a specific role, then you can use WITH LOGIN PASSWORD:

create role "role_name" with login password 'extremely_secure_password';
Passwords#
Your Postgres database is the core of your Supabase project, so it's important that every role has a strong, secure password at all times. Here are some tips for creating a secure password:

Use a password manager to generate it.
Make a long password (12 characters at least).
Don't use any common dictionary words.
Use both upper and lower case characters, numbers, and special symbols.
Special symbols in passwords#
If you use special symbols in your Postgres password, you must remember to percent-encode your password later if using the Postgres connection string, for example, postgresql://postgres.projectref:p%3Dword@aws-0-us-east-1.pooler.supabase.com:6543/postgres

Changing your project password#
When you created your project you were also asked to enter a password. This is the password for the postgres role in your database. You can update this from the Dashboard under the Database Settings page. You should never give this to third-party service unless you absolutely trust them. Instead, we recommend that you create a new user for every service that you want to give access too. This will also help you with debugging - you can see every query that each role is executing in your database within pg_stat_statements.

Changing the password does not result in any downtime. All connected services, such as PostgREST, PgBouncer, and other Supabase managed services, are automatically updated to use the latest password to ensure availability. However, if you have any external services connecting to the Supabase database using hardcoded username/password credentials, a manual update will be required.

Granting permissions#
Roles can be granted various permissions on database objects using the GRANT command. Permissions include SELECT, INSERT, UPDATE, and DELETE. You can configure access to almost any object inside your database - including tables, views, functions, and triggers.

Revoking permissions#
Permissions can be revoked using the REVOKE command:

REVOKE permission_type ON object_name FROM role_name;
Role hierarchy#
Roles can be organized in a hierarchy, where one role can inherit permissions from another. This simplifies permission management, as you can define permissions at a higher level and have them automatically apply to all child roles.

Role inheritance#
To create a role hierarchy, you first need to create the parent and child roles. The child role will inherit permissions from its parent. Child roles can be added using the INHERIT option when creating the role:

create role "child_role_name" inherit "parent_role_name";
Preventing inheritance#
In some cases, you might want to prevent a role from having a child relationship (typically superuser roles). You can prevent inheritance relations using NOINHERIT:

alter role "child_role_name" noinherit;
Supabase roles#
Postgres comes with a set of predefined roles. Supabase extends this with a default set of roles which are configured on your database when you start a new project:

postgres#
The default Postgres role. This has admin privileges.

anon#
For unauthenticated, public access. This is the role which the API (PostgREST) will use when a user is not logged in.

authenticator#
A special role for the API (PostgREST). It has very limited access, and is used to validate a JWT and then
"change into" another role determined by the JWT verification.

authenticated#
For "authenticated access." This is the role which the API (PostgREST) will use when a user is logged in.

service_role#
For elevated access. This role is used by the API (PostgREST) to bypass Row Level Security.

supabase_auth_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to the auth schema.

supabase_storage_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to the storage schema.

dashboard_user#
For running commands via the Supabase UI.

supabase_admin#
An internal role Supabase uses for administrative tasks, such as running upgrades and automations.

Resources#
Official Postgres docs: Database Roles
Official Postgres docs: Role Membership
Official Postgres docs: Function Permissions
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Users vs roles
Creating roles
Creating users
Passwords
Special symbols in passwords
Changing your project password
Granting permissions
Revoking permissions
Role hierarchy
Role inheritance
Preventing inheritance
Supabase roles
postgres
anon
authenticator
authenticated
service_role
supabase_auth_admin
supabase_storage_admin
dashboard_user
supabase_admin
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Custom Roles

Learn about using custom roles with storage schema

In this guide, you will learn how to create and use custom roles with Storage to manage role-based access to objects and buckets. The same approach can be used to use custom roles with any other Supabase service.

Supabase Storage uses the same role-based access control system as any other Supabase service using RLS (Row Level Security).

Create a custom role#
Let's create a custom role manager to provide full read access to a specific bucket. For a more advanced setup, see the RBAC Guide.

create role 'manager';
-- Important to grant the role to the authenticator and anon role
grant manager to authenticator;
grant anon to manager;
Create a policy#
Let's create a policy that gives full read permissions to all objects in the bucket teams for the manager role.

create policy "Manager can view all files in the bucket 'teams'"
on storage.objects
for select
to manager
using (
 bucket_id = 'teams'
);
Test the policy#
To impersonate the manager role, you will need a valid JWT token with the manager role.
You can quickly create one using the jsonwebtoken library in Node.js.

Signing a new JWT requires your JWT_SECRET. You must store this secret securely. Never expose it in frontend code, and do not check it into version control.

const jwt = require('jsonwebtoken')
const JWT_SECRET = 'your-jwt-secret' // You can find this in your Supabase project settings under API. Store this securely.
const USER_ID = '' // the user id that we want to give the manager role
const token = jwt.sign({ role: 'manager', sub: USER_ID }, JWT_SECRET, {
  expiresIn: '1h',
})
Now you can use this token to access the Storage API.

const { StorageClient } = require('@supabase/storage-js')
const PROJECT_URL = 'https://your-project-id.supabase.co/storage/v1'
const storage = new StorageClient(PROJECT_URL, {
  authorization: `Bearer ${token}`,
})
await storage.from('teams').list()
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Create a custom role
Create a policy
Test the policy
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Sou
Postgres Roles

Managing access to your Postgres database and configuring permissions.

Postgres manages database access permissions using the concept of roles. Generally you wouldn't use these roles for your own application - they are mostly for configuring system access to your database. If you want to configure application access, then you should use Row Level Security (RLS). You can also implement Role-based Access Control on top of RLS.

Users vs roles#
In Postgres, roles can function as users or groups of users. Users are roles with login privileges, while groups (also known as role groups) are roles that don't have login privileges but can be used to manage permissions for multiple users.

Creating roles#
You can create a role using the create role command:

create role "role_name";
Creating users#
Roles and users are essentially the same in Postgres, however if you want to use password-logins for a specific role, then you can use WITH LOGIN PASSWORD:

create role "role_name" with login password 'extremely_secure_password';
Passwords#
Your Postgres database is the core of your Supabase project, so it's important that every role has a strong, secure password at all times. Here are some tips for creating a secure password:

Use a password manager to generate it.
Make a long password (12 characters at least).
Don't use any common dictionary words.
Use both upper and lower case characters, numbers, and special symbols.
Special symbols in passwords#
If you use special symbols in your Postgres password, you must remember to percent-encode your password later if using the Postgres connection string, for example, postgresql://postgres.projectref:p%3Dword@aws-0-us-east-1.pooler.supabase.com:6543/postgres

Changing your project password#
When you created your project you were also asked to enter a password. This is the password for the postgres role in your database. You can update this from the Dashboard under the Database Settings page. You should never give this to third-party service unless you absolutely trust them. Instead, we recommend that you create a new user for every service that you want to give access too. This will also help you with debugging - you can see every query that each role is executing in your database within pg_stat_statements.

Changing the password does not result in any downtime. All connected services, such as PostgREST, PgBouncer, and other Supabase managed services, are automatically updated to use the latest password to ensure availability. However, if you have any external services connecting to the Supabase database using hardcoded username/password credentials, a manual update will be required.

Granting permissions#
Roles can be granted various permissions on database objects using the GRANT command. Permissions include SELECT, INSERT, UPDATE, and DELETE. You can configure access to almost any object inside your database - including tables, views, functions, and triggers.

Revoking permissions#
Permissions can be revoked using the REVOKE command:

REVOKE permission_type ON object_name FROM role_name;
Role hierarchy#
Roles can be organized in a hierarchy, where one role can inherit permissions from another. This simplifies permission management, as you can define permissions at a higher level and have them automatically apply to all child roles.

Role inheritance#
To create a role hierarchy, you first need to create the parent and child roles. The child role will inherit permissions from its parent. Child roles can be added using the INHERIT option when creating the role:

create role "child_role_name" inherit "parent_role_name";
Preventing inheritance#
In some cases, you might want to prevent a role from having a child relationship (typically superuser roles). You can prevent inheritance relations using NOINHERIT:

alter role "child_role_name" noinherit;
Supabase roles#
Postgres comes with a set of predefined roles. Supabase extends this with a default set of roles which are configured on your database when you start a new project:

postgres#
The default Postgres role. This has admin privileges.

anon#
For unauthenticated, public access. This is the role which the API (PostgREST) will use when a user is not logged in.

authenticator#
A special role for the API (PostgREST). It has very limited access, and is used to validate a JWT and then
"change into" another role determined by the JWT verification.

authenticated#
For "authenticated access." This is the role which the API (PostgREST) will use when a user is logged in.

service_role#
For elevated access. This role is used by the API (PostgREST) to bypass Row Level Security.

supabase_auth_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to the auth schema.

supabase_storage_admin#
Used by the Auth middleware to connect to the database and run migration. Access is scoped to the storage schema.

dashboard_user#
For running commands via the Supabase UI.

supabase_admin#
An internal role Supabase uses for administrative tasks, such as running upgrades and automations.

Resources#
Official Postgres docs: Database Roles
Official Postgres docs: Role Membership
Official Postgres docs: Function Permissions
Edit this page on GitHub
Custom Roles

Learn about using custom roles with storage schema

In this guide, you will learn how to create and use custom roles with Storage to manage role-based access to objects and buckets. The same approach can be used to use custom roles with any other Supabase service.

Supabase Storage uses the same role-based access control system as any other Supabase service using RLS (Row Level Security).

Create a custom role#
Let's create a custom role manager to provide full read access to a specific bucket. For a more advanced setup, see the RBAC Guide.

create role 'manager';
-- Important to grant the role to the authenticator and anon role
grant manager to authenticator;
grant anon to manager;
Create a policy#
Let's create a policy that gives full read permissions to all objects in the bucket teams for the manager role.

create policy "Manager can view all files in the bucket 'teams'"
on storage.objects
for select
to manager
using (
 bucket_id = 'teams'
);
Test the policy#
To impersonate the manager role, you will need a valid JWT token with the manager role.
You can quickly create one using the jsonwebtoken library in Node.js.

Signing a new JWT requires your JWT_SECRET. You must store this secret securely. Never expose it in frontend code, and do not check it into version control.

const jwt = require('jsonwebtoken')
const JWT_SECRET = 'your-jwt-secret' // You can find this in your Supabase project settings under API. Store this securely.
const USER_ID = '' // the user id that we want to give the manager role
const token = jwt.sign({ role: 'manager', sub: USER_ID }, JWT_SECRET, {
  expiresIn: '1h',
})
Now you can use this token to access the Storage API.

const { StorageClient } = require('@supabase/storage-js')
const PROJECT_URL = 'https://your-project-id.supabase.co/storage/v1'
const storage = new StorageClient(PROJECT_URL, {
  authorization: `Bearer ${token}`,
})
await storage.from('teams').list()
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Create a custom role
Create a policy
Test the policy
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
Vault

Managing secrets in Postgres.

Vault is a Postgres extension and accompanying Supabase UI that makes it safe and easy to store encrypted secrets and other data in your database. This opens up a lot of possibilities to use Postgres in ways that go beyond what is available in a stock distribution.

Under the hood, the Vault is a table of Secrets that are stored using Authenticated Encryption on disk. They are then available in decrypted form through a Postgres view so that the secrets can be used by applications from SQL. Because the secrets are stored on disk encrypted and authenticated, any backups or replication streams also preserve this encryption in a way that can't be decrypted or forged.

Supabase provides a dashboard UI for the Vault that makes storing secrets easy. Click a button, type in your secret, and save.

You can use Vault to store secrets - everything from Environment Variables to API Keys. You can then use these secrets anywhere in your database: Postgres Functions, Triggers, and Webhooks. From a SQL perspective, accessing secrets is as easy as querying a table (or in this case, a view). The underlying secrets tables will be stored in encrypted form.

Using Vault#
You can manage secrets from the UI or using SQL.

Adding secrets#
There is also a handy function for creating secrets called vault.create_secret():

select vault.create_secret('my_s3kre3t');
The function returns the UUID of the new secret.

Show Result
Secrets can also have an optional unique name and an optional description. These are also arguments to vault.create_secret():

select vault.create_secret('another_s3kre3t', 'unique_name', 'This is the description');
Show Result
Viewing secrets#
If you look in the vault.secrets table, you will see that your data is stored encrypted. To decrypt the data, there is an automatically created view vault.decrypted_secrets. This view will decrypt secret data on the fly:

select * 
from vault.decrypted_secrets 
order by created_at desc 
limit 3;
Show Result
Notice how this view has a decrypted_secret column that contains the decrypted secrets. Views are not stored on disk, they are only run at query time, so the secret remains encrypted on disk, and in any backup dumps or replication streams.

You should ensure that you protect access to this view with the appropriate SQL privilege settings at all times, as anyone that has access to the view has access to decrypted secrets.

Updating secrets#
A secret can be updated with the vault.update_secret() function, this function makes updating secrets easy, just provide the secret UUID as the first argument, and then an updated secret, updated optional unique name, or updated description:

select
  vault.update_secret(
    '7095d222-efe5-4cd5-b5c6-5755b451e223',
    'n3w_upd@ted_s3kret',
    'updated_unique_name',
    'This is the updated description'
  );
Show Result
Deep dive#

As we mentioned, Vault uses Transparent Column Encryption (TCE) to store secrets in an authenticated encrypted form. There are some details around that you may be curious about. What does authenticated mean? Where is the encryption key stored? This section explains those details.

Authenticated encryption with associated data#
The first important feature of TCE is that it uses an Authenticated Encryption with Associated Data encryption algorithm (based on libsodium).

Encryption key location#
Authenticated Encryption means that in addition to the data being encrypted, it is also signed so that it cannot be forged. You can guarantee that the data was encrypted by someone you trust, which you wouldn't get with encryption alone. The decryption function verifies that the signature is valid before decrypting the value.

Associated Data means that you can include any other columns from the same row as part of the signature computation. This doesn't encrypt those other columns - rather it ensures that your encrypted value is only associated with columns from that row. If an attacker were to copy an encrypted value from another row to the current one, the signature would be rejected (assuming you used a unique column in the associated data).

Another important feature is that the encryption key is never stored in the database alongside the encrypted data. Even if an attacker can capture a dump of your entire database, they will see only encrypted data, never the encryption key itself.

This is an important safety precaution - there is little value in storing the encryption key in the database itself as this would be like locking your front door but leaving the key in the lock! Storing the key outside the database fixes this issue.

Where is the key stored? Supabase creates and manages the encryption key in our secured backend systems. We keep this key safe and separate from your data. You remain in control of your key - a separate API endpoint is available that you can use to access the key if you want to decrypt your data outside of Supabase.

Which roles should have access to the vault.secrets table should be carefully considered. There are two ways to grant access, the first is that the postgres user can explicitly grant access to the vault table itself.

Resources#
Read more about Supabase Vault in the blog post
Supabase Vault on GitHub
Column Encryption
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Using Vault
Adding secrets
Viewing secrets
Updating secrets
Deep dive
Authenticated encryption with associated data
Encryption key location
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
Access and security
Superuser Access and Unsupported Operations
Roles, superuser access and unsupported operations

Supabase provides the default postgres role to all instances deployed. Superuser access is not given as it allows destructive operations to be performed on the database.

To ensure you are not impacted by this, additional privileges are granted to the postgres user to allow it to run some operations that are normally restricted to superusers.

However, this does mean that some operations, that typically require superuser privileges, are not available on Supabase. These are documented below:

Unsupported operations#
COPY ... FROM PROGRAM
ALTER USER ... WITH SUPERUSER
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Unsupported operations
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
Configuration, optimization, and testing
Database configuration
Database configuration

Updating the default configuration for your Postgres database.

Postgres provides a set of sensible defaults for you database size. In some cases, these defaults can be updated. We do not recommend changing these defaults unless you know what you're doing.

Timeouts#
See the Timeouts section.

Statement optimization#
All Supabase projects come with the pg_stat_statements extension installed, which tracks planning and execution statistics for all statements executed against it. These statistics can be used in order to diagnose the performance of your project.

This data can further be used in conjunction with the explain functionality of Postgres to optimize your usage.

Managing timezones#
Every hosted Supabase database is set to UTC timezone by default. We strongly recommend keeping it this way, even if your users are in a different location. This is because it makes it much easier to calculate differences between timezones if you adopt the mental model that everything in your database is in UTC time.

On self-hosted databases, the timezone defaults to your local timezone. We recommend changing this to UTC for the same reasons.

Change timezone#

SQL
alter database postgres
set timezone to 'America/New_York';
Full list of timezones#
Get a full list of timezones supported by your database. This will return the following columns:

name: Time zone name
abbrev: Time zone abbreviation
utc_offset: Offset from UTC (positive means east of Greenwich)
is_dst: True if currently observing daylight savings

SQL
select name, abbrev, utc_offset, is_dst
from pg_timezone_names()
order by name;
Search for a specific timezone#
Use ilike (case insensitive search) to find specific timezones.


SQL
select *
from pg_timezone_names()
where name ilike '%york%';
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Timeouts
Statement optimization
Managing timezones
Change timezone
Full list of timezones
Search for a specific timezone
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Configuration, optimization, and testing
Query optimization
Query Optimization

Choosing indexes to improve your query performance.

When working with Postgres, or any relational database, indexing is key to improving query performance. Aligning indexes with common query patterns can speed up data retrieval by an order of magnitude.

This guide is intended to:

help identify parts of a query that have the potential to be improved by indexes
introduce tooling to help identify useful indexes
This is not a comprehensive resource, but rather a helpful starting point for your optimization journey.

If you're new to query optimization, you may be interested in index_advisor, our tool for automatically detecting indexes that improve performance on a given query.

Example query#
Consider the following example query that retrieves customer names and purchase dates from two tables:

select
  a.name,
  b.date_of_purchase
from
  customers as a
  join orders as b on a.id = b.customer_id
where a.sign_up_date > '2023-01-01' and b.status = 'shipped'
order by b.date_of_purchase
limit 10;
In this query, there are several parts that indexes could likely help in optimizing the performance:

where clause:#
The where clause filters rows based on certain conditions, and indexing the columns involved can improve this process:

a.sign_up_date: If filtering by sign_up_date is common, indexing this column can speed up the query.
b.status: Indexing the status may be beneficial if the column has diverse values.
create index idx_customers_sign_up_date on customers (sign_up_date);
create index idx_orders_status on orders (status);
join columns#
Indexes on the columns used for joining tables can help Postgres avoid scanning tables in their entirety when connecting tables.

Indexing a.id and b.customer_id would likely improve the performance of the join in this query.
Note that if a.id is the primary key of the customers table it is already indexed
create index idx_orders_customer_id on orders (customer_id);
order by clause#
Sorting can also be optimized by indexing:

An index on b.date_of_purchase can improve the sorting process, and is particularly beneficial when a subset of rows is being returned with a limit clause.
create index idx_orders_date_of_purchase on orders (date_of_purchase);
Key concepts#
Here are some concepts and tools to keep in mind to help you identify the best index for the job, and measure the impact that your index had:

Analyze the query plan#
Use the explain command to understand the query's execution. Look for slow parts, such as Sequential Scans or high cost numbers. If creating an index does not reduce the cost of the query plan, remove it.

For example:

explain select * from customers where sign_up_date > 25;
Use appropriate index types#
Postgres offers various index types like B-tree, Hash, GIN, etc. Select the type that best suits your data and query pattern. Using the right index type can make a significant difference. For example, using a BRIN index on a field that always increases and lives within a table that updates infrequently - like created_at on an orders table - routinely results in indexes that are +10x smaller than the equivalent default B-tree index. That translates into better scalability.

create index idx_orders_created_at ON customers using brin(created_at);
Partial indexes#
For queries that frequently target a subset of data, a partial index could be faster and smaller than indexing the entire column. A partial index contains a where clause to filter the values included in the index. Note that a query's where clause must match the index for it to be used.

create index idx_orders_status on orders (status)
where status = 'shipped';
Composite indexes#
If filtering or joining on multiple columns, a composite index prevents Postgres from referring to multiple indexes when identifying the relevant rows.

create index idx_customers_sign_up_date_priority on customers (sign_up_date, priority);
Over-Indexing#
Avoid the urge to index columns you operate on infrequently. While indexes can speed up reads, they also slow down writes, so it's important to balance those factors when making indexing decisions.

Statistics#
Postgres maintains a set of statistics about the contents of your tables. Those statistics are used by the query planner to decide when it's is more efficient to use an index vs scanning the entire table. If the collected statistics drift too far from reality, the query planner may make poor decisions. To avoid this risk, you can periodically analyze tables.

analyze customers;
By following this guide, you'll be able to discern where indexes can optimize queries and enhance your Postgres performance. Remember that each database is unique, so always consider the specific context and use case of your queries.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Example query
where clause:
join columns
order by clause
Key concepts
Analyze the query plan
Use appropriate index types
Partial indexes
Composite indexes
Over-Indexing
Statistics
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Configuration, optimization, and testing
Database Advisors
Performance and Security Advisors

Check your database for performance and security issues

You can use the Database Performance and Security Advisors to check your database for issues such as missing indexes and improperly set-up RLS policies.

Using the Advisors#
In the dashboard, navigate to Security Advisor and Performance Advisor under Database. The advisors run automatically. You can also manually rerun them after you've resolved issues.

Available checks#

0001 unindexed foreign keys

0002 auth users exposed

0003 auth rls initplan

0004 no primary key

0005 unused index

0006 multiple permissive policies

0007 policy exists rls disabled

0008 rls enabled no policy

0009 duplicate index

0010 security definer view

0011 function search path mutable

0012 auth allow anonymous sign ins

0013 rls disabled in public

0014 extension in public

0015 rls references user metadata

0016 materialized view in api

0017 foreign table in api

0018 unsupported reg types

0019 insecure queue exposed in api

0020 table bloat

0021 fkey to auth unique

0022 extension versions outdated
Level: INFO

Rationale#
In relational databases, indexing foreign key columns is a standard practice for improving query performance. Indexing these columns is recommended in most cases because it improves query join performance along a declared relationship.

What is a Foreign Key?#
A foreign key is a constraint on a column (or set of columns) that enforces a relationship between two tables. For example, a foreign key from book.author_id to author.id enforces that every value in book.author_id exists in author.id. Once the foriegn key is declared, it is not possible to insert a value into book.author_id that does not exist in author.id. Similarly, Postgres will not allow us to delete a value from author.id that is referenced by book.author_id. This concept is known as referential integrity.

Why Index Foreign Key Columns?#
Given that foreign keys define relationships among tables, it is common to use foreign key columns in join conditions when querying the database. Adding an index to the columns making up the foreign key improves the performance of those joins and reduces database resource consumption.

select
    book.id,
    book.title,
    author.name
from
    book
    join author
        -- Both sides of the following condition should be indexed
        -- for best performance
        on book.author_id = author.id
How to Resolve#
Given a table:

create table book (
    id serial primary key,
    title text not null,
    author_id int references author(id) -- this defines the foreign key
);
To apply the best practice of indexing foreign keys, an index is needed on the book.author_id column. We can create that index using:

create index ix_book_author_id on book(author_id);
In this case we used the default B-tree index type. Be sure to choose an index type that is appropriate for the data types and use case when working with your own tables.

Example#
Let's look at a practical example involving two tables: order_item and customer, where order_item references customer.

Given the schema:

create table customer (
    id serial primary key,
    name text not null
);
create table order_item (
    id serial primary key,
    order_date date not null,
    customer_id integer not null references customer (id)
);
We expect the tables to be joined on the condition

customer.id = order_item.customer_id
As in:

select
    customer.name,
    order_item.order_date
from
    customer
    join order_item
        on customer.id = order_item.customer_id
Using Postgres' "explain plan" functionality, we can see how its query planner expects to execute the query.

Hash Join  (cost=38.58..74.35 rows=2040 width=36)
  Hash Cond: (order_item.customer_id = customer.id)
  ->  Seq Scan on order_item  (cost=0.00..30.40 rows=2040 width=8)
  ->  Hash  (cost=22.70..22.70 rows=1270 width=36)
        ->  Seq Scan on customer  (cost=0.00..22.70 rows=1270 width=36)
Notice that the condition order_item.customer_id = customer.id is being serviced by a Seq Scan, a sequential scan across the order_items table. That means Postgres intends to sequentially iterate over each row in the table to identify the value of customer_id.

Next, if we index order_item.customer_id and recompute the query plan:

create index ix_order_item_customer_id on order_item(customer_id);
explain
select
    customer.name,
    order_item.order_date
from
    customer
    join order_item
        on customer.id = order_item.customer_id
We get the query plan:

Hash Join  (cost=38.58..74.35 rows=2040 width=36)
  Hash Cond: (order_item.customer_id = customer.id)
  ->  Seq Scan on order_item  (cost=0.00..30.40 rows=2040 width=8)
  ->  Hash  (cost=22.70..22.70 rows=1270 width=36)
        ->  Seq Scan on customer  (cost=0.00..22.70 rows=1270 width=36)
Note that nothing changed.

We get an identical result because Postgres' query planner is clever enough to know that a Seq Scan over an empty table is extremely fast, so theres no reason for it to reach out to an index. As more rows are inserted into the order_item table the tradeoff between sequentially scanning and retriving the index steadily tip in favor of the index. Rather than manually finding this inflection point, we can hint to the query planner that we'd like to use indexes by disabling sequentials scans except where they are the only available option. To provides that hint we can use:

set local enable_seqscan = off;
With that change:

set local enable_seqscan = off;
explain
select
    customer.name,
    order_item.order_date
from
    customer
    join order_item
        on customer.id = order_item.customer_id
We get the query plan:

Hash Join  (cost=79.23..159.21 rows=2040 width=36)
  Hash Cond: (order_item.customer_id = customer.id)
  ->  Index Scan using ix_order_item_customer_id on order_item  (cost=0.15..74.75 rows=2040 width=8)
  ->  Hash  (cost=63.20..63.20 rows=1270 width=36)
        ->  Index Scan using customer_pkey on customer  (cost=0.15..63.20 rows=1270 width=36)
The new plan services the order_item.customer_id = customer.id join condition using an Index Scan on ix_order_item_customer_id which is far more efficient at scale.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Using the Advisors
Available checks
Rationale
What is a Foreign Key?
Why Index Foreign Key Columns?
How to Resolve
Example
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Configuration, optimization, and testing
Testing your database
Testing Your Database

To ensure that queries return the expected data, RLS policies are correctly applied and etc., we encourage you to write automated tests. There are essentially two approaches to testing:

Firstly, you can write tests that interface with a Supabase client instance (same way you use Supabase client in your application code) in the programming language(s) you use in your application and using your favorite testing framework.

Secondly, you can test through the Supabase CLI, which is a more low-level approach where you write tests in SQL.

Testing using the Supabase CLI
You can use the Supabase CLI to test your database. The minimum required version of the CLI is v1.11.4. To get started:

Install the Supabase CLI on your local machine
Creating a test#
Create a tests folder inside the supabase folder:

mkdir -p ./supabase/tests/database
Create a new file with the .sql extension which will contain the test.

touch ./supabase/tests/database/hello_world.test.sql
Writing tests#
All sql files use pgTAP as the test runner.

Let's write a simple test to check that our auth.users table has an ID column. Open hello_world.test.sql and add the following code:

begin;
select plan(1); -- only one statement to run
SELECT has_column(
    'auth',
    'users',
    'id',
    'id should exist'
);
select * from finish();
rollback;
Running tests#
To run the test, you can use:

supabase test db
This will produce the following output:

$ supabase test db
supabase/tests/database/hello_world.test.sql .. ok
All tests successful.
Files=1, Tests=1,  1 wallclock secs ( 0.01 usr  0.00 sys +  0.04 cusr  0.02 csys =  0.07 CPU)
Result: PASS
More resources#
Testing RLS policies
pgTAP extension
Official pgTAP documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Creating a test
Writing tests
Running tests
More resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Configuration, optimization, and testing
Customizing Postgres config
Customizing Postgres configs

Each Supabase project is a pre-configured Postgres cluster. You can override some configuration settings to suit your needs. This is an advanced topic, and we don't recommend touching these settings unless it is necessary.

Customizing Postgres configurations provides advanced control over your database, but inappropriate settings can lead to severe performance degradation or project instability.

Viewing settings#
To list all Postgres settings and their descriptions, run:

select * from pg_settings;
Configurable settings#
User-context settings#
The pg_settings table's context column specifies the requirements for changing a setting. By default, those with a user context can be changed at the role or database level with SQL.

To list all user-context settings, run:

select * from pg_settings where context = 'user';
As an example, the statement_timeout setting can be altered:

alter database "postgres" set "statement_timeout" TO '60s';
To verify the change, execute:

show "statement_timeout";
Superuser settings#
Some settings can only be modified by a superuser. Supabase pre-enables the supautils extension, which allows the postgres role to retain certain superuser privileges. It enables modification of the below reserved configurations at the role level:

Setting	Description
auto_explain.*	Configures the auto_explain module. Can be configured to log execution plans for queries expected to exceed x seconds, including function queries.
log_lock_waits	Controls whether a log message is produced when a session waits longer than deadlock_timeout to acquire a lock.
log_min_duration_statement	Causes the duration of each completed statement to be logged if the statement ran for at least the specified amount of time.
log_min_messages	Minimum severity level of messages to log.
log_replication_commands	Logs all replication commands
log_statement	Controls which SQL statements are logged. Valid values are none (off), ddl, mod, and all (all statements).
log_temp_files	Controls logging of temporary file names and sizes.
pg_net.ttl	Sets how long the pg_net extension saves responses
pg_net.batch_size	Sets how many requests the pg_net extension can make per second
pg_stat_statements.*	Configures the pg_stat_statements extension.
pgaudit.*	Configures the PGAudit extension. The log_parameter is still restricted to protect secrets
pgrst.*	PostgREST settings
plan_filter.*	Configures the pg_plan_filter extension
session_replication_role	Sets the session's behavior for triggers and rewrite rules.
track_io_timing	Collects timing statistics for database I/O activity.
wal_compression	This parameter enables compression of WAL using the specified compression method.
For example, to enable log_nested_statements for the postgres role, execute:

alter role "postgres" set "auto_explain.log_nested_statements" to 'on';
To view the change:

select
  rolname,
  rolconfig
from pg_roles
where rolname = 'postgres';
CLI configurable settings#
While many Postgres parameters are configurable directly, some configurations can be changed with the Supabase CLI at the system level.

CLI changes permanently overwrite default settings, so reset all and set to default commands won't revert to the original values.

In order to overwrite the default settings, you must have Owner or Administrator privileges within your organizations.

CLI supported parameters#
If a setting you need is not yet configurable, share your use case with us! Let us know what setting you'd like to control, and we'll consider adding support in future updates.

The following parameters are available for overrides:

checkpoint_timeout
effective_cache_size
hot_standby_feedback
logical_decoding_work_mem (CLI only)
maintenance_work_mem
max_connections (CLI only)
max_locks_per_transaction (CLI only)
max_parallel_maintenance_workers
max_parallel_workers_per_gather
max_parallel_workers
max_replication_slots (CLI only)
max_slot_wal_keep_size (CLI only)
max_standby_archive_delay (CLI only)
max_standby_streaming_delay (CLI only)
max_wal_size (CLI only)
max_wal_senders (CLI only)
max_worker_processes (CLI only)
session_replication_role
shared_buffers (CLI only)
statement_timeout
track_activity_query_size
track_commit_timestamp
wal_keep_size (CLI only)
wal_sender_timeout (CLI only)
work_mem
Managing Postgres configuration with the CLI#
To start:

Install Supabase CLI 1.69.0+.
Log in to your Supabase account using the CLI.
To update Postgres configurations, use the postgres config command:

supabase --experimental \
--project-ref <project-ref> \
postgres-config update --config shared_buffers=250MB
By default, the CLI will merge any provided config overrides with any existing ones. The --replace-existing-overrides flag can be used to instead force all existing overrides to be replaced with the ones being provided:

supabase --experimental \
--project-ref <project-ref> \
postgres-config update --config max_parallel_workers=3 \
--replace-existing-overrides
To delete specific configuration overrides, use the postgres-config delete command:

supabase --experimental \
--project-ref <project-ref> \
postgres-config delete --config shared_buffers,work_mem
By default, CLI v2 (≥ 2.0.0) checks the parameter’s context and requests the correct action (reload or restart):

If the setting can be reloaded (pg_settings.context = 'sighup'), then the Management API will detect this and apply the change with a configuration reload.
If the setting requires a restart (pg_settings.context = 'postmaster'), then both the primary and any read replicas will restart to apply the change.
To check whether a parameter can be reloaded without a restart, see the Postgres docs.

You can verify whether changes have been applied with the following checks:

supabase --version;
-- Check whether the parameters were updated (and if a restart is pending):
select name, setting, context, pending_restart
from pg_settings
where name in ('max_slot_wal_keep_size', 'shared_buffers', 'max_connections');
-- If the timestamp hasn’t changed, no restart occurred
select pg_postmaster_start_time();
You can also pass the --no-restart flag to attempt a reload-only apply. If the parameter cannot be reloaded, the change stays pending until the next restart.

Read Replicas and Custom Config
Postgres requires several parameters to be synchronized between the Primary cluster and Read Replicas.

By default, Supabase ensures that this propagation is executed correctly. However, if the --no-restart behavior is used in conjunction with parameters that cannot be reloaded without a restart, the user is responsible for ensuring that both the primaries and the read replicas get restarted in a timely manner to ensure a stable running state. Leaving the configuration updated, but not utilized (via a restart) in such a case can result in read replica failure if the primary, or a read replica, restarts in isolation (e.g. due to an out-of-memory event, or hardware failure).

supabase --experimental \
--project-ref <project-ref> \
postgres-config delete --config shared_buffers --no-restart
Resetting to default config#
To reset a setting to its default value at the database level:

-- reset a single setting at the database level
alter database "postgres" set "<setting_name>" to default;
-- reset all settings at the database level
alter database "postgres" reset all;
For role level configurations, you can run:

alter role "<role_name>" set "<setting_name>" to default;
Considerations#
Changes through the CLI might restart the database causing momentary disruption to existing database connections; in most cases this should not take more than a few seconds. However, you can use the --no-restart flag to bypass the restart and keep the connections intact. Keep in mind that this depends on the specific configuration changes you're making. if the change requires a restart, using the --no-restart flag will prevent the restart but you won't see those changes take effect until a restart is manually triggered. Additionally, some parameters are required to be the same on Primary and Read Replicas; not restarting in these cases can result in read replica failure if the Primary/Read Replicas restart in isolation.
Custom Postgres Config will always override the default optimizations generated by Supabase. When changing compute add-ons, you should also review and update your custom Postgres Config to ensure they remain compatible and effective with the updated compute.
Some parameters (e.g. wal_keep_size) can increase disk utilization, triggering disk expansion, which in turn can lead to increases in your bill.
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Viewing settings
Configurable settings
User-context settings
Superuser settings
CLI configurable settings
Resetting to default config
Considerations
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Debugging
Timeouts
Timeouts

Extend database timeouts to execute longer transactions

Dashboard and Client queries have a max-configurable timeout of 60 seconds. For longer transactions, use Supavisor or direct connections.

Change Postgres timeout#
You can change the Postgres timeout at the:

Session level
Function level
Global level
Role level
Session level#
Session level settings persist only for the duration of the connection.

Set the session timeout by running:

set statement_timeout = '10min';
Because it applies to sessions only, it can only be used with connections through Supavisor in session mode (port 5432) or a direct connection. It cannot be used in the Dashboard, with the Supabase Client API, nor with Supavisor in Transaction mode (port 6543).

This is most often used for single, long running, administrative tasks, such as creating an HSNW index. Once the setting is implemented, you can view it by executing:

SHOW statement_timeout;
See the full guide on changing session timeouts.

Function level#
This works with the Database REST API when called from the Supabase client libraries:

create or replace function myfunc()
returns void as $$
 select pg_sleep(3); -- simulating some long-running process
$$
language sql
set statement_timeout TO '4s'; -- set custom timeout
This is mostly for recurring functions that need a special exemption for runtimes.

Role level#
This sets the timeout for a specific role.

The default role timeouts are:

anon: 3s
authenticated: 8s
service_role: none (defaults to the authenticator role's 8s timeout if unset)
postgres: none (capped by default global timeout to be 2min)
Run the following query to change a role's timeout:

alter role example_role set statement_timeout = '10min'; -- could also use seconds '10s'
If you are changing the timeout for the Supabase Client API calls, you will need to reload PostgREST to reflect the timeout changes by running the following script:

NOTIFY pgrst, 'reload config';
Unlike global settings, the result cannot be checked with SHOW statement_timeout. Instead, run:

select
  rolname,
  rolconfig
from pg_roles
where
  rolname in (
    'anon',
    'authenticated',
    'postgres',
    'service_role'
    -- ,<ANY CUSTOM ROLES>
  );
Global level#
This changes the statement timeout for all roles and sessions without an explicit timeout already set.

alter database postgres set statement_timeout TO '4s';
Check if your changes took effect:

show statement_timeout;
Although not necessary, if you are uncertain if a timeout has been applied, you can run a quick test:

create or replace function myfunc()
returns void as $$
  select pg_sleep(601); -- simulating some long-running process
$$
language sql;
Identifying timeouts#
The Supabase Dashboard contains tools to help you identify timed-out and long-running queries.

Using the Logs Explorer#
Go to the Logs Explorer, and run the following query to identify timed-out events (statement timeout) and queries that successfully run for longer than 10 seconds (duration).

select
  cast(postgres_logs.timestamp as datetime) as timestamp,
  event_message,
  parsed.error_severity,
  parsed.user_name,
  parsed.query,
  parsed.detail,
  parsed.hint,
  parsed.sql_state_code,
  parsed.backend_type
from
  postgres_logs
  cross join unnest(metadata) as metadata
  cross join unnest(metadata.parsed) as parsed
where
  regexp_contains(event_message, 'duration|statement timeout')
  -- (OPTIONAL) MODIFY OR REMOVE
  and parsed.user_name = 'authenticator' -- <--------CHANGE
order by timestamp desc
limit 100;
Using the Query Performance page#
Go to the Query Performance page and filter by relevant role and query speeds. This only identifies slow-running but successful queries. Unlike the Log Explorer, it does not show you timed-out queries.

Understanding roles in logs#
Each API server uses a designated user for connecting to the database:

Role	API/Tool
supabase_admin	Used by Realtime and for project configuration
authenticator	PostgREST
supabase_auth_admin	Auth
supabase_storage_admin	Storage
supabase_replication_admin	Synchronizes Read Replicas
postgres	Supabase Dashboard and External Tools (e.g., Prisma, SQLAlchemy, PSQL...)
Custom roles	External Tools (e.g., Prisma, SQLAlchemy, PSQL...)
Filter by the parsed.user_name field to only retrieve logs made by specific users:

-- find events based on role/server
... query
where
  -- find events from the relevant role
  parsed.user_name = '<ROLE>'
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Change Postgres timeout
Session level
Function level
Role level
Global level
Identifying timeouts
Using the Logs Explorer
Using the Query Performance page
Understanding roles in logs
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Debugging
Debugging and monitoring
Debugging and monitoring

Database performance is a large topic and many factors can contribute. Some of the most common causes of poor performance include:

An inefficiently designed schema
Inefficiently designed queries
A lack of indexes causing slower than required queries over large tables
Unused indexes causing slow INSERT, UPDATE and DELETE operations
Not enough compute resources, such as memory, causing your database to go to disk for results too often
Lock contention from multiple queries operating on highly utilized tables
Large amount of bloat on your tables causing poor query planning
You can examine your database and queries for these issues using either the Supabase CLI or SQL.

Using the CLI#
The Supabase CLI comes with a range of tools to help inspect your Postgres instances for potential issues. The CLI gets the information from Postgres internals. Therefore, most tools provided are compatible with any Postgres databases regardless if they are a Supabase project or not.

You can find installation instructions for the the Supabase CLI here.

The inspect db command#
The inspection tools for your Postgres database are under then inspect db command. You can get a full list of available commands by running supabase inspect db help.

$ supabase inspect db help
Tools to inspect your Supabase database
Usage:
  supabase inspect db [command]
Available Commands:
  bloat                Estimates space allocated to a relation that is full of dead tuples
  blocking             Show queries that are holding locks and the queries that are waiting for them to be released
  cache-hit            Show cache hit rates for tables and indices
...
Connect to any Postgres database#
Most inspection commands are Postgres agnostic. You can run inspection routines on any Postgres database even if it is not a Supabase project by providing a connection string via --db-url.

For example you can connect to your local Postgres instance:

supabase --db-url postgresql://postgres:postgres@localhost:5432/postgres inspect db bloat
Connect to a Supabase instance#
Working with Supabase, you can link the Supabase CLI with your project:

supabase link --project-ref <project-id>
Then the CLI will automatically connect to your Supabase project whenever you are in the project folder and you no longer need to provide —db-url.

Inspection commands#
Below are the db inspection commands provided, grouped by different use cases.

Some commands might require pg_stat_statements to be enabled or a specific Postgres version to be used.

Disk storage#
These commands are handy if you are running low on disk storage:

bloat - estimates the amount of wasted space
vacuum-stats - gives information on waste collection routines
table-record-counts - estimates the number of records per table
table-sizes - shows the sizes of tables
index-sizes - shows the sizes of individual index
table-index-sizes - shows the sizes of indexes for each table
Query performance#
The commands below are useful if your Postgres database consumes a lot of resources like CPU, RAM or Disk IO. You can also use them to investigate slow queries.

cache-hit - shows how efficient your cache usage is overall
unused-indexes - shows indexes with low index scans
index-usage - shows information about the efficiency of indexes
seq-scans - show number of sequential scans recorded against all tables
long-running-queries - shows long running queries that are executing right now
outliers - shows queries with high execution time but low call count and queries with high proportion of execution time spent on synchronous I/O
Locks#
locks - shows statements which have taken out an exclusive lock on a relation
blocking - shows statements that are waiting for locks to be released
Connections#
role-connections - shows number of active connections for all database roles (Supabase-specific command)
replication-slots - shows information about replication slots on the database
Notes on pg_stat_statements#
Following commands require pg_stat_statements to be enabled: calls, locks, cache-hit, blocking, unused-indexes, index-usage, bloat, outliers, table-record-counts, replication-slots, seq-scans, vacuum-stats, long-running-queries.

When using pg_stat_statements also take note that it only stores the latest 5,000 statements. Moreover, consider resetting the analysis after optimizing any queries by running select pg_stat_statements_reset();

Learn more about pg_stats here.

Using SQL#
If you're seeing an insufficient privilege error when viewing the Query Performance page from the dashboard, run this command:

$ grant pg_read_all_stats to postgres;
Postgres cumulative statistics system#
Postgres collects data about its own operations using the cumulative statistics system. In addition to this, every Supabase project has the pg_stat_statements extension enabled by default. This extension records query execution performance details and is the best way to find inefficient queries. This information can be combined with the Postgres query plan analyzer to develop more efficient queries.

Here are some example queries to get you started.

Most frequently called queries#
select
  auth.rolname,
  statements.query,
  statements.calls,
  -- -- Postgres 13, 14, 15
  statements.total_exec_time + statements.total_plan_time as total_time,
  statements.min_exec_time + statements.min_plan_time as min_time,
  statements.max_exec_time + statements.max_plan_time as max_time,
  statements.mean_exec_time + statements.mean_plan_time as mean_time,
  -- -- Postgres <= 12
  -- total_time,
  -- min_time,
  -- max_time,
  -- mean_time,
  statements.rows / statements.calls as avg_rows
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by statements.calls desc
limit 100;
This query shows:

query statistics, ordered by the number of times each query has been executed
the role that ran the query
the number of times it has been called
the average number of rows returned
the cumulative total time the query has spent running
the min, max and mean query times.
This provides useful information about the queries you run most frequently. Queries that have high max_time or mean_time times and are being called often can be good candidates for optimization.

Slowest queries by execution time#
select
  auth.rolname,
  statements.query,
  statements.calls,
  -- -- Postgres 13, 14, 15
  statements.total_exec_time + statements.total_plan_time as total_time,
  statements.min_exec_time + statements.min_plan_time as min_time,
  statements.max_exec_time + statements.max_plan_time as max_time,
  statements.mean_exec_time + statements.mean_plan_time as mean_time,
  -- -- Postgres <= 12
  -- total_time,
  -- min_time,
  -- max_time,
  -- mean_time,
  statements.rows / statements.calls as avg_rows
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by max_time desc
limit 100;
This query will show you statistics about queries ordered by the maximum execution time. It is similar to the query above ordered by calls, but this one highlights outliers that may have high executions times. Queries which have high or mean execution times are good candidates for optimization.

Most time consuming queries#
select
  auth.rolname,
  statements.query,
  statements.calls,
  statements.total_exec_time + statements.total_plan_time as total_time,
  to_char(
    (
      (statements.total_exec_time + statements.total_plan_time) / sum(
        statements.total_exec_time + statements.total_plan_time
      ) over ()
    ) * 100,
    'FM90D0'
  ) || '%' as prop_total_time
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by total_time desc
limit 100;
This query will show you statistics about queries ordered by the cumulative total execution time. It shows the total time the query has spent running as well as the proportion of total execution time the query has taken up.

Queries which are the most time consuming are not necessarily bad, you may have a very efficient and frequently ran queries that end up taking a large total % time, but it can be useful to help spot queries that are taking up more time than they should.

Hit rate#
Generally for most applications a small percentage of data is accessed more regularly than the rest. To make sure that your regularly accessed data is available, Postgres tracks your data access patterns and keeps this in its shared_buffers cache.

Applications with lower cache hit rates generally perform more poorly since they have to hit the disk to get results rather than serving them from memory. Very poor hit rates can also cause you to burst past your Disk IO limits causing significant performance issues.

You can view your cache and index hit rate by executing the following query:

select
  'index hit rate' as name,
  (sum(idx_blks_hit)) / nullif(sum(idx_blks_hit + idx_blks_read), 0) * 100 as ratio
from pg_statio_user_indexes
union all
select
  'table hit rate' as name,
  sum(heap_blks_hit) / nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100 as ratio
from pg_statio_user_tables;
This shows the ratio of data blocks fetched from the Postgres shared_buffers cache against the data blocks that were read from disk/OS cache.

If either of your index or table hit rate are < 99% then this can indicate your compute plan is too small for your current workload and you would benefit from more memory. Upgrading your compute is easy and can be done from your project dashboard.

Optimizing poor performing queries#
Postgres has built in tooling to help you optimize poorly performing queries. You can use the query plan analyzer on any expensive queries that you have identified:

explain analyze <query-statement-here>;
When you include analyze in the explain statement, the database attempts to execute the query and provides a detailed query plan along with actual execution times. So, be careful using explain analyze with insert/update/delete queries, because the query will actually run, and could have unintended side-effects.

If you run just explain without the analyze keyword, the database will only perform query planning without actually executing the query. This approach can be beneficial when you want to inspect the query plan without affecting the database or if you encounter timeouts in your queries.

Using the query plan analyzer to optimize your queries is a large topic, with a number of online resources available:

Official docs.
The Art of PostgreSQL.
Postgres Wiki.
Enterprise DB.
You can pair the information available from pg_stat_statements with the detailed system metrics available via your metrics endpoint to better understand the behavior of your DB and the queries you're executing against it.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Using the CLI
The inspect db command
Connect to any Postgres database
Connect to a Supabase instance
Inspection commands
Notes on pg_stat_statements
Using SQL
Postgres cumulative statistics system
Most frequently called queries
Slowest queries by execution time
Most time consuming queries
Hit rate
Optimizing poor performing queries
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Debugging
Debugging performance issues
Debugging performance issues

Debug slow-running queries using the Postgres execution planner.

explain() is a method that provides the Postgres EXPLAIN execution plan of a query. It is a powerful tool for debugging slow queries and understanding how Postgres will execute a given query. This feature is applicable to any query, including those made through rpc() or write operations.

Enabling explain()#
explain() is disabled by default to protect sensitive information about your database structure and operations. We recommend using explain() in a non-production environment. Run the following SQL to enable explain():

-- enable explain
alter role authenticator
set pgrst.db_plan_enabled to 'true';
-- reload the config
notify pgrst, 'reload config';
Using explain()#
To get the execution plan of a query, you can chain the explain() method to a Supabase query:

const { data, error } = await supabase
  .from('instruments')
  .select()
  .explain()
Example data#
To illustrate, consider the following setup of a instruments table:

create table instruments (
  id int8 primary key,
  name text
);
insert into books
  (id, name)
values
  (1, 'violin'),
  (2, 'viola'),
  (3, 'cello');
Expected response#
The response would typically look like this:

Aggregate  (cost=33.34..33.36 rows=1 width=112)
  ->  Limit  (cost=0.00..18.33 rows=1000 width=40)
        ->  Seq Scan on instruments  (cost=0.00..22.00 rows=1200 width=40)
By default, the execution plan is returned in TEXT format. However, you can also retrieve it as JSON by specifying the format parameter.

Production use with pre-request protection#
If you need to enable explain() in a production environment, ensure you protect your database by restricting access to the explain() feature. You can do so by using a pre-request function that filters requests based on the IP address:

create or replace function filter_plan_requests()
returns void as $$
declare
  headers   json := current_setting('request.headers', true)::json;
  client_ip text := coalesce(headers->>'cf-connecting-ip', '');
  accept    text := coalesce(headers->>'accept', '');
  your_ip   text := '123.123.123.123'; -- replace this with your IP
begin
  if accept like 'application/vnd.pgrst.plan%' and client_ip != your_ip then
    raise insufficient_privilege using
      message = 'Not allowed to use application/vnd.pgrst.plan';
  end if;
end; $$ language plpgsql;
alter role authenticator set pgrst.db_pre_request to 'filter_plan_requests';
notify pgrst, 'reload config';
Replace '123.123.123.123' with your actual IP address.

Disabling explain#
To disable the explain() method after use, execute the following SQL commands:

-- disable explain
alter role authenticator
set pgrst.db_plan_enabled to 'false';
-- if you used the above pre-request
alter role authenticator
set pgrst.db_pre_request to '';
-- reload the config
notify pgrst, 'reload config';
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enabling explain()
Using explain()
Example data
Expected response
Production use with pre-request protection
Disabling explain
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Debugging
Supavisor
Supavisor

Troubleshooting Supavisor errors

Supavisor logs are available under Pooler Logs in the Dashboard. The following are common errors and their solutions:

Error Type	Description	Resolution Link
Max client connections reached	This error happens when the number of connections to Supavisor is more than the allowed limit of your compute add-on.	Follow this guide to resolve.
Connection failed {:error, :eaddrnotavail} to 'db.xxx.supabase.co':5432	Supavisor cannot connect to the customer database. This is usually caused if the target database is unable to respond.	N/A
Connection failed {:error, :nxdomain} to 'db.xxx.supabase.co':5432	Supavisor cannot connect to the customer database. This is usually caused if the target database is unable to respond.	N/A
Connection closed when state was authentication	This error happens when either the database doesn’t exist or if the user doesn't have the right credentials.	N/A
Subscribe error: {:error, :worker_not_found}	This log event is emitted when the client tries to connect to the database, but Supavisor does not have the necessary information to route the connection. Try reconnecting to the database as it can take some time for the project information to propagate to Supavisor.	N/A
Subscribe error: {:error, {:badrpc, {:error, {:erpc, :timeout}}}}	This is a timeout error when the communication between different Supavisor nodes takes longer than expected. Try reconnecting to the database.	N/A
Terminating with reason :client_termination when state was :busy	This error happens when the client terminates the connection before the connection with the database is completed.	N/A
Error: received invalid response to GSSAPI negotiation: S	This error happens due to gssencmode parameter not set to disabled.	Follow this guide to resolve.
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database Troubleshooting
Search or browse troubleshooting guides for common database issues, including connection problems, query optimization, and configuration.

Search and filter

Error codes

Tags
Search by keyword
Reset filters
Matching troubleshooting entries
42501 : permission denied for table http_request_queue
42501
Database
Sep 9
All about Supabase Egress
Platform
Database
Functions
Storage
Realtime
Auth
Supavisor
Nov 6
Avoiding timeouts in long running queries
Database
Platform
Oct 30
Canceling statement due to "statement timeout"
Database
Sep 16
Certain operations are too complex to perform directly using the client libraries.
Database
Sep 9
Change Project Region
Platform
Database
Auth
Mar 12
Errors when creating / updating / deleting users
500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure,
Auth
Database
Studio
Nov 10
Database API 42501 errors
401 42501, 403 42501
Database
Sep 9
Database Error: remaining connection slots are reserved for non-replication superuser connections
Database
Jan 18
Deprecated RLS features
Database
Nov 12
Disabling Prepared statements
Database
Supavisor
Feb 4
Discovering and Interpreting API Errors in the Logs
Database
Platform
Sep 9
Do I need to expose "security definer" Functions in Row Level Security Policies?
Database
Nov 12
How to download logical backups in Supabase with physical backups enabled?
Database
Sep 9
Enabling the IPv4 add-on FAQ
Database
Platform
Supavisor
Nov 12
Error: "Connection refused" when trying to connect to Supabase database
Database
Cli
Sep 9
Error: index row size exceeds btree version 4 maximum for index
Database
Feb 4
error: no pg_hba.conf entry for host "xx.xxx.xxx.xxx", user "postgres", database "postgres", SSL off
Database
Platform
Sep 9
Error: prepared statement "XXX" already exists
Database
Jan 17
High swap usage
Database
Platform
Sep 9
Failed to restore from backup: All subscriptions and replication slots must be dropped before a backup can be restored.
Database
Jan 18
Error: Failed to run sql query: Connection terminated due to connection timeout
500
Database
Studio
Nov 12
Fixing 520 Errors in the Database REST API
520
Database
Sep 9
Grafana not displaying data
Database
Sep 9
High CPU and Slow Queries with `ERROR: must be a superuser to terminate superuser process`
Cli
Database
Storage
Nov 12
High latency with supabase client
Database
Platform
Feb 4
How can I revoke execution of a PostgreSQL function?
Database
Functions
Jan 17
How do I reset my Supabase database password?
Database
Sep 9
How do I update connection pool settings in my dashboard?
Database
Supavisor
Sep 9
How long does it take to restore a database from a Point-in-Time backup (PITR)?
Database
Jan 18
How Postgres chooses which index to use
Database
Feb 21
How to change max database connections
Database
Supavisor
Cli
Sep 9
How to check if my queries are being blocked by other queries?
Database
Jan 17
How to delete a role in Postgres
Database
Sep 9
How to Interpret and Explore the Postgres Logs
Database
Platform
Sep 9
How to View Database Metrics
Database
Platform
Sep 9
Increase vector lookup speeds by applying an HSNW index
Database
Ai
Platform
Oct 30
Inserting into Sequence/Serial Table Causes "duplicate key violates unique constraint" Error
Database
Sep 9
"insufficient privilege" when accessing pg_stat_statements
Database
Sep 16
Interpreting Supabase Grafana CPU charts
Platform
Database
Sep 9
Interpreting Supabase Grafana IO charts
Database
Platform
Sep 9
Memory and Swap usage explained
Database
Platform
Feb 4
How to monitor Postgres and Supavisor connections
Database
Supavisor
Aug 4
NEW variable is null in a trigger function.
Database
Functions
Jan 16
Partitioning an existing table with same name
Database
Feb 4
pg_cron debugging guide
Database
Sep 9
PGRST106: "The schema must be one of the following..." error when querying an exposed schema
PGRST106
Auth
Database
Nov 12
Prisma Error Management
Database
Sep 9
Reload/refresh postgrest schema
400 bad_request
Database
Oct 15
Resolving 42P01: relation does not exist error
42P01, 42501
Database
Sep 9
Resolving 500 Status Authentication Errors
500
Auth
Database
Sep 9
Resolving database hostname and managing your IP address
Database
Sep 9
RLS Performance and Best Practices
Auth
Database
Apr 4
RLS Simplified
Database
Auth
Feb 21
Seeing "no pg_hba.conf entry for host" errors in Postgres and they come from an IP address that I don't recognize
Database
Platform
Feb 4
Slow Execution of ALTER TABLE on Large Table when changing column type
Database
Sep 9
Steps to improve query performance with indexes
Database
Platform
Cli
Sep 9
Supabase & Your Network: IPv4 and IPv6 compatibility
Database
Platform
Supavisor
Sep 9
Supabase CLI: "failed SASL auth" or "invalid SCRAM server-final-message"
Auth
Cli
Database
Supavisor
Nov 12
Interpreting Supabase Grafana Memory Charts
Database
Feb 21
Supavisor and Connection Terminology Explained
Supavisor
Database
Feb 4
Supavisor FAQ
Supavisor
Database
Oct 30
Transferring from cloud to self-host in Supabase
Database
Self-hosting
Jan 15
Understanding PostgreSQL EXPLAIN Output
Database
Sep 9
Understanding PostgreSQL Logging Levels and How They Impact Your Project
Database
Sep 9
Understanding the Usage Summary on the Dashboard
Platform
Studio
Database
Functions
Sep 9
Using SQLAlchemy with Supabase
Database
Supavisor
Self-hosting
Functions
Oct 30
Webhook debugging guide
Database
Sep 9
Why are there gaps in my Postgres id sequence?
Database
Feb 21
Why do I see Auth & API requests in the dashboard? My app has no users
Auth
Platform
Database
Realtime
Functions
Feb 4
Why is my camelCase name not working in Postgres functions or RLS policies?
Database
Jan 15
Why is my select returning an empty data array and I have data in the table?
Database
Auth
Feb 21
Why is my service role key client getting RLS errors or not returning data?
Auth
Database
Feb 4
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database

More
Prisma
Connecting with Prisma
Prisma

This quickly shows how to connect your Prisma application to Supabase Postgres. If you encounter any problems, reference the Prisma troubleshooting docs.

If you plan to solely use Prisma instead of the Supabase Data API (PostgREST), turn it off in the API Settings.

1
Create a custom user for Prisma
In the SQL Editor, create a Prisma DB user with full privileges on the public schema.
This gives you better control over Prisma's access and makes it easier to monitor using Supabase tools like the Query Performance Dashboard and Log Explorer.
password manager
For security, consider using a password generator for the Prisma role.

-- Create custom user
create user "prisma" with password 'custom_password' bypassrls createdb;
-- extend prisma's privileges to postgres (necessary to view changes in Dashboard)
grant "prisma" to "postgres";
-- Grant it necessary permissions over the relevant schemas (public)
grant usage on schema public to prisma;
grant create on schema public to prisma;
grant all on all tables in schema public to prisma;
grant all on all routines in schema public to prisma;
grant all on all sequences in schema public to prisma;
alter default privileges for role postgres in schema public grant all on tables to prisma;
alter default privileges for role postgres in schema public grant all on routines to prisma;
alter default privileges for role postgres in schema public grant all on sequences to prisma;

-- alter prisma password if needed
alter user "prisma" with password 'new_password';

2
Create a Prisma Project
Create a new Prisma Project on your computer

Create a new directory

mkdir hello-prisma
cd hello-prisma
Initiate a new Prisma project


npm

pnpm

yarn

bun
npm init -y
npm install prisma typescript ts-node @types/node --save-dev
npx tsc --init
npx prisma init
3
Add your connection information to your .env file
On your project dashboard, click Connect
Find your Supavisor Session pooler string. It should end with 5432. It will be used in your .env file.
If you're in an IPv6 environment or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.

If you plan on deploying Prisma to a serverless or auto-scaling environment, you'll also need your Supavisor transaction mode string.
The string is identical to the session mode string but uses port 6543 at the end.

server-based deployments

serverless deployments
In your .env file, set the DATABASE_URL variable to your connection string

# Used for Prisma Migrations and within your application
DATABASE_URL="postgres://[DB-USER].[PROJECT-REF]:[PRISMA-PASSWORD]@[DB-REGION].pooler.supabase.com:5432/postgres"
Change your string's [DB-USER] to prisma and add the password you created in step 1

postgres://prisma.[PROJECT-REF]...
4
Create your migrations
If you have already modified your Supabase database, synchronize it with your migration file. Otherwise create new tables for your database


New Projects

Populated Projects
Create new tables in your prisma.schema file

model Post {
  id        Int     @id @default(autoincrement())
  title     String
  content   String?
  published Boolean @default(false)
  author    User?   @relation(fields: [authorId], references: [id])
  authorId  Int?
}
model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
  posts Post[]
}
commit your migration


npm

pnpm

yarn

bun
npx prisma migrate dev --name first_prisma_migration
5
Install the prisma client
Install the Prisma client and generate its model


npm

pnpm

yarn

bun
npm install @prisma/client
npx prisma generate
6
Test your API
Create a index.ts file and run it to test your connection

const { PrismaClient } = require('@prisma/client');
const prisma = new PrismaClient();
async function main() {
  //change to reference a table in your schema
  const val = await prisma.<SOME_TABLE_NAME>.findMany({
    take: 10,
  });
  console.log(val);
}
main()
  .then(async () => {
    await prisma.$disconnect();
  })
  .catch(async (e) => {
    console.error(e);
    await prisma.$disconnect();
  process.exit(1);
});
Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database

More
Prisma
Prisma troubleshooting
Troubleshooting prisma errors

This guide addresses common Prisma errors that you might encounter while using Supabase.

A full list of errors can be found in Prisma's official docs.

Understanding connection string parameters: #
Unlike other libraries, Prisma lets you configure its settings through special options appended to your connection string.

These options, called "query parameters," can be used to address specific errors.

# Example of query parameters
connection_string.../postgres?KEY1=VALUE&KEY2=VALUE&KEY3=VALUE
Errors
... prepared statement already exists#
Supavisor in transaction mode (port 6543) does not support prepared statements, which Prisma will try to create in the background.

Solution: #
Add pgbouncer=true to the connection string. This turns off prepared statements in Prisma.
.../postgres?pgbouncer=true
Can't reach database server at:#
Prisma couldn't establish a connection with Postgres or Supavisor before the timeout

Possible causes: #
Database overload: The database server is under heavy load, causing Prisma to struggle to connect.
Malformed connection string: The connection string used by Prisma is incorrect or incomplete.
Transient network issues: Temporary network problems are disrupting the connection.
Solutions: #
Check database health: Use the Reports Dashboard to monitor CPU, memory, and I/O usage. If the database is overloaded, consider increasing your compute size or optimizing your queries.
Verify connection string: Double-check the connection string in your Prisma configuration to ensure it matches in your project connect page.
Increase connection timeout: Try increasing the connect_timeout parameter in your Prisma configuration to give it more time to establish a connection.
.../postgres?connect_timeout=30
Timed out fetching a new connection from the connection pool:#
Prisma is unable to allocate connections to pending queries fast enough to meet demand.

Possible causes: #
Overwhelmed server: The server hosting Prisma is under heavy load, limiting its ability to manage connections. By default, Prisma will create the default num_cpus * 2 + 1 worth of connections. A common cause for server strain is increasing the connection_limit significantly past the default.
Insufficient pool size: The Supavisor pooler does not have enough connections available to quickly satisfy Prisma's requests.
Slow queries: Prisma's queries are taking too long to execute, preventing it from releasing connections for reuse.
Solutions: #
Increase the pool timeout: Increase the pool_timeout parameter in your Prisma configuration to give the pooler more time to allocate connections.
Reduce the connection limit: If you've explicitly increased the connection_limit parameter in your Prisma configuration, try reducing it to a more reasonable value.
Increase pool size: If you are connecting with Supavisor, try increasing the pool size in the Database Settings.
Optimize queries: Improve the efficiency of your queries to reduce execution time.
Increase compute size: Like the preceding option, this is a strategy to reduce query execution time.
Server has closed the connection#
According to this GitHub Issue for Prisma, this error may be related to large return values for queries. It may also be caused by significant database strain.

Solutions: #
Limit row return sizes: Try to limit the total amount of rows returned for particularly large requests.
Minimize database strain:Check the Reports Page for database strain. If there is obvious strain, consider optimizing or increasing compute size
Drift detected: Your database schema is not in sync with your migration history#
Prisma relies on migration files to ensure your database aligns with Prisma's model. External schema changes are detected as "drift", which Prisma will try to overwrite, potentially causing data loss.

Possible causes: #
Supabase Managed Schemas: Supabase may update managed schemas like auth and storage to introduce new features. Granting Prisma access to these schemas can lead to drift during updates.
External Schema Modifications: Your team or another tool might have modified the database schema outside of Prisma, causing drift.
Solution: #
Baselining migrations: baselining re-syncs Prisma by capturing the current database schema as the starting point for future migrations.
Max client connections reached#
Postgres or Supavisor rejected a request for more connections

Possible causes:#
When working in transaction mode (port 6543): The error "Max client connections reached" occurs when clients try to form more connections with the pooler than it can support.
When working in session mode (port 5432): The max amount of clients is restricted to the "Pool Size" value in the Database Settings. If the "Pool Size" is set to 15, even if the pooler can handle 200 client connections, it will still be effectively capped at 15 for each unique "database-role+database" combination.
When working with direct connections: Postgres is already servicing the max amount of connections
Solutions #
Transaction Mode for serverless apps: If you are using serverless functions (Supabase Edge, Vercel, AWS Lambda), switch to transaction mode (port 6543). It handles more connections than session mode or direct connections.
Reduce the number of Prisma connections: A single client-server can establish multiple connections with a pooler. Typically, serverless setups do not need many connections. Starting with fewer, like five or three, or even just one, is often sufficient. In serverless setups, begin with connection_limit=1, increasing cautiously if needed to avoid maxing out connections.
Increase pool size: If you are connecting with Supavisor, try increasing the pool size in the Database Settings.
Disconnect appropriately: Close Prisma connections when they are no longer needed.
Decrease query time: Reduce query complexity or add strategic indexes to your tables to speed up queries.
Increase compute size: Sometimes the best option is to increase your compute size, which also increases your max client size and query execution speed
Cross schema references are only allowed when the target schema is listed in the schemas property of your data-source#
A Prisma migration is referencing a schema it is not permitted to manage.

Possible causes: #
A migration references a schema that Prisma is not permitted to manage
Solutions: #
Multi-Schema support: If the external schema isn't Supabase managed, modify your prisma.schema file to enable the multi-Schema preview
generator client {
  provider        = "prisma-client-js"
  previewFeatures = ["multiSchema"]  //Add line
}
datasource db {
  provider  = "postgresql"
  url       = env("DATABASE_URL")
  directUrl = env("DIRECT_URL")
  schemas   = ["public", "other_schema"] //list out relevant schemas
}
Supabase managed schemas: Schemas managed by Supabase, such as auth and storage, may be changed to support new features. Referencing these schemas directly will cause schema drift in the future. It is best to remove references to these schemas from your migrations.
An alternative strategy to reference these tables is to duplicate values into Prisma managed table with triggers. Below is an example for duplicating values from auth.users into a table called profiles.

Show/Hide Details
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Understanding connection string parameters:
... prepared statement already exists
Solution:
Can't reach database server at:
Possible causes:
Solutions:
Timed out fetching a new connection from the connection pool:
Possible causes:
Solutions:
Server has closed the connection
Solutions:
Drift detected: Your database schema is not in sync with your migration history
Possible causes:
Solution:
Max client connections reached
Possible causes:
Solutions
Cross schema references are only allowed when the target schema is listed in the schemas property of your data-source
Possible causes:
Solutions:
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
ORM Quickstarts
Drizzle
Drizzle

Connecting with Drizzle#
Drizzle ORM is a TypeScript ORM for SQL databases designed with maximum type safety in mind. You can use their ORM to connect to your database.

If you plan on solely using Drizzle instead of the Supabase Data API (PostgREST), you can turn off the latter in the API Settings.

1
Install
Install Drizzle and related dependencies.

npm i drizzle-orm postgres
npm i -D drizzle-kit
2
Create your models
Create a schema.ts file and define your models.

import { pgTable, serial, text, varchar } from "drizzle-orm/pg-core";
export const users = pgTable('users', {
  id: serial('id').primaryKey(),
  fullName: text('full_name'),
  phone: varchar('phone', { length: 256 }),
});
3
Connect
Connect to your database using the Connection Pooler.

From the project Connect panel, copy the URI from the "Shared Pooler" option and save it as the DATABASE_URL environment variable. Remember to replace the password placeholder with your actual database password.

In local SUPABASE_DB_URL require to be adapted to work with Docker resolver

import 'dotenv/config'
import { drizzle } from 'drizzle-orm/postgres-js'
import postgres from 'postgres'
let connectionString = process.env.DATABASE_URL
if (host.includes('postgres:postgres@supabase_db_')) {
  const url = URL.parse(host)!
  url.hostname = url.hostname.split('_')[1]
  connectionString = url.href
}
// Disable prefetch as it is not supported for "Transaction" pool mode
export const client = postgres(connectionString, { prepare: false })
export const db = drizzle(client);
Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
ORM Quickstarts
Postgres.js
Postgres.js

Connecting with Postgres.js#
Postgres.js is a full-featured Postgres client for Node.js and Deno.

1
Install
Install Postgres.js and related dependencies.

npm i postgres

2
Connect
Create a db.js file with the connection details.

To get your connection details, go to the Connect panel. Choose Transaction pooler if you're on a platform with transient connections, such as a serverless function, and Session pooler if you have a long-lived connection. Copy the URI and save it as the environment variable DATABASE_URL.

// db.js
import postgres from 'postgres'
const connectionString = process.env.DATABASE_URL
const sql = postgres(connectionString)
export default sql
3
Execute commands
Use the connection to execute commands.

import sql from './db.js'
async function getUsersOver(age) {
  const users = await sql`
    select name, age
    from users
    where age > ${ age }
  `
  // users = Result [{ name: "Walter", age: 80 }, { name: 'Murray', age: 68 }, ...]
  return users
}
Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
GUI quickstarts
pgAdmin
Connecting with pgAdmin

What is pgAdmin?#
pgAdmin is a GUI tool for managing Postgres databases. You can use it to connect to your database via SSL.

Connecting pgAdmin with your Postgres database#
1
Register
Register a new Postgres server.

Register a new postgres server.
2
Name
Name your server.

Name Postgres Server.

3
Connect
Add the connection info. Click the "Connect" button at the top of the page to open the connect Modal. Scroll down to "session pooler", click "view parameters" to toggle the parameters menu open and copy your connection parameters. Fill in your Database password that you made when creating your project (It can be reset in Database Settings above if you don't have it).

Add Connection Info.

4
SSL
Download your SSL certificate from Dashboard's Database Settings.

In pgAdmin, navigate to the Parameters tab and select connection parameter as Root Certificate. Next navigate to the Root certificate input, it will open up a file-picker modal. Select the certificate you downloaded earlier and save the server details. pgAdmin should now be able to connect to your Postgres via SSL.

Add Connection Info.

Why connect to pgAdmin#
Connecting your Postgres instance to pgAdmin gives you a free, cross-platform GUI that makes tasks such as browsing objects, writing queries with autocomplete, running backups, and monitoring performance much faster and safer than using psql alone.

It acts as a single control panel where you can manage multiple servers, inspect locks and slow queries in real time, and perform maintenance operations with a click.

For scripted migrations or ultra-light remote work you’ll still lean on plain SQL or CLI tools, but most teams find pgAdmin invaluable for exploration and routine administration.

Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
GUI quickstarts
PSQL
Connecting with PSQL

psql is a command-line tool that comes with Postgres.

Connecting with SSL#
You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.

You can obtain your connection info and Server root certificate from your application's dashboard:

Connection Info and Certificate.

Download your SSL certificate to /path/to/prod-supabase.cer.

Find your connection settings. Go to the project Connect panel and copy the URL from the Session pooler section, and copy the parameters into the connection string:

psql "sslmode=verify-full sslrootcert=/path/to/prod-supabase.cer host=[CLOUD_PROVIDER]-0-[REGION].pooler.supabase.com dbname=postgres user=postgres.[PROJECT_REF]"
Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
GUI quickstarts
DBeaver
Connecting with DBeaver

If you do not have DBeaver, you can download it from its website.

1
Create a new database connection
Create a new database connection

new database connection

2
Select PostgreSQL
Selection Menu

3
Get Your Credentials
On your project dashboard, click Connect, note your session pooler's:

host
username
You will also need your database's password. If you forgot it, you can generate a new one in the settings.

If you're in an IPv6 environment or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.

database credentials

4
Fill out credentials
In DBeaver's Main menu, add your host, username, and password

filling out form

5
Download certificate
In the Database Settings, download your SSL certificate.

filling out form

6
Secure your connection
In DBeaver's SSL tab, add your SSL certificate

filling out form

7
Connect
Test your connection and then click finish. You should now be able to interact with your database with DBeaver

connected dashboard

Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
GUI quickstarts
Metabase
Connecting to Metabase

Metabase is an Open Source data visualization tool. You can use it to explore your data stored in Supabase.

1
Register
Create a Metabase account or deploy locally with Docker

Deploying with Docker:

docker pull metabase/metabase:latest

Then run:

docker run -d -p 3000:3000 --name metabase metabase/metabase

The server should be available at http://localhost:3000/setup

2
Connect to Postgres
Connect your Postgres server to Metabase.

On your project dashboard, click Connect
View parameters under "Session pooler"
connection notice
If you're in an IPv6 environment or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.

Enter your database credentials into Metabase
Example credentials:Name Postgres Server.

3
Explore
Explore your data in Metabase

explore data

Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
GUI quickstarts
Beekeeper Studio
Connecting with Beekeeper Studio

Beekeeper Studio Community is a free GUI tool for interacting with databases.

1
Create a new connection
In Beekeeper, create a new Postgres connection.

Postgres connection

2
Get your connection credentials
Get your connection credentials from the Connect panel. You will need:

host
username
password
port
Add your credentials to Beekeeper's connection form

Credentials

3
Download your SSL Certificate
Download your SSL certificate from the Dashboard's Database Settings
SSL

Add your SSL to the connection formSSL

4
Test and connect
Test your connection and then connect

SSL

Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Database replication
Overview
Replication and change data capture

Replication is the process of copying changes from your database to another location. It's also referred to as change data capture (CDC): capturing all the changes that occur to your data.

Use cases#
You might use replication for:

Analytics and Data Warehousing: Replicate your operational database to analytics platforms like BigQuery for complex analysis without impacting your application's performance.
Data Integration: Keep your data synchronized across different systems and services in your tech stack.
Backup and Disaster Recovery: Maintain up-to-date copies of your data in different locations.
Read Scaling: Distribute read operations across multiple database instances to improve performance.
Replication in Postgres#
Postgres comes with built-in support for replication via publications and replication slots. Refer to the Concepts and terms section to learn how replication works.

Setting up and monitoring replication in Supabase#
Setting up replication
Monitoring replication
If you want to set up a read replica, see Read Replicas instead. If you want to sync your data in real time to a client such as a browser or mobile app, see Realtime instead. For configuring replication to an ETL destination, use the Dashboard.

Concepts and terms#
Write-Ahead Log (WAL)#
Postgres uses a system called the Write-Ahead Log (WAL) to manage changes to the database. As you make changes, they are appended to the WAL (which is a series of files (also called "segments"), where the file size can be specified). Once one segment is full, Postgres will start appending to a new segment. After a period of time, a checkpoint occurs and Postgres synchronizes the WAL with your database. Once the checkpoint is complete, then the WAL files can be removed from disk and free up space.

Logical replication and WAL#
Logical replication is a method of replication where Postgres uses the WAL files and transmit those changes to another Postgres database, or a system that supports reading WAL files.

LSN#
LSN is a Log Sequence Number that is used to identify the position of a WAL file in the WAL directory. It is often used to determine the progress of replication in subscribers and calculate the lag of a replication slot.

Logical replication architecture#
When setting up logical replication, three key components are involved:

publication - A set of tables on your primary database that will be published
replication slot - A slot used for replicating the data from a single publication. The slot, when created, will specify the output format of the changes
subscription - A subscription is created from an external system (i.e. another Postgres database) and must specify the name of the publication. If you do not specify a replication slot, one is automatically created
Logical replication output format#
Logical replication is typically output in 2 forms, pgoutput and wal2json. The output method is how Postgres sends changes to any active replication slot.

Logical replication configuration#
When using logical replication, Postgres is then configured to keep WAL files around for longer than it needs them. If the files are removed too quickly, then your replication slot will become inactive and, if the database receives a large number of changes in a short time, then the replication slot can become lost as it was not able to keep up.

In order to mitigate this, Postgres has many options and settings that can be tweaked to manage the WAL usage effectively. Not all of these settings are user configurable as they can impact the stability of your database. For those that are, these should be considered as advanced configuration and not changed without understanding that they can cause additional disk space and resources to be used, as well as incur additional costs.

Setting	Description	User-facing	Default
max_replication_slots	Max count of replication slots allowed	No	
wal_keep_size	Minimum size of WAL files to keep for replication	No	
max_slot_wal_keep_size	Max WAL size that can be reserved by replication slots	No	
checkpoint_timeout	Max time between WAL checkpoints	No	
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Use cases
Replication in Postgres
Setting up and monitoring replication in Supabase
Concepts and terms
Write-Ahead Log (WAL)
Logical replication and WAL
LSN
Logical replication architecture
Logical replication output format
Logical replication configuration
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Database replication
Setting up replication
Setting up replication and CDC with Supabase

Prerequisites#
To set up replication, the following is recommended:

Instance size of XL or greater
IPv4 add-on enabled
To create a replication slot, you will need to use the postgres user and follow the instructions in our guide.

If you are running Postgres 17 or higher, you can create a new user and grant them replication permissions with the postgres user. For versions below 17, you will need to use the postgres user.

If you are replicating to an external system and using any of the tools below, check their documentation first and we have added additional information where the setup with Supabase can vary.


Airbyte

Estuary

Fivetran

Materialize

Stitch
Estuary has the following documentation for setting up Postgres as a source.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Prerequisites
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Database replication
Monitoring replication
Monitoring replication

Monitoring replication lag is important and there are 3 ways to do this:

Dashboard - Under the Reports of the dashboard, you can view the replication lag of your project
Database -
pg_stat_subscription (subscriber) - if PID is null, then the subscription is not active
pg_stat_subscription_stats - look here for error_count to see if there were issues applying or syncing (if yes, check the logs for why)
pg_replication_slots - use this to check if the slot is active and you can also calculate the lag from here
Metrics - Using the prometheus endpoint for your project
replication_slots_max_lag_bytes - this is the more important one
pg_stat_replication_replay_lag - lag to replay WAL files from the source DB on the target DB (throttled by disk or high activity)
pg_stat_replication_send_lag - lag in sending WAL files from the source DB (a high lag means that the publisher is not being asked to send new WAL files OR a network issues)
Primary#
Replication status and lag#
The pg_stat_replication table shows the status of any replicas connected to the primary database.

select pid, application_name, state, sent_lsn, write_lsn, flush_lsn, replay_lsn, sync_state
from pg_stat_replication;
Replication slot status#
A replication slot can be in one of three states:

active - The slot is active and is receiving data
inactive - The slot is not active and is not receiving data
lost - The slot is lost and is not receiving data
The state can be checked using the pg_replication_slots table:

select slot_name, active, state from pg_replication_slots;
WAL size#
The WAL size can be checked using the pg_ls_waldir() function:

select * from pg_ls_waldir();
Check LSN#
select pg_current_wal_lsn();
Subscriber#
Subscription status#
The pg_subscription table shows the status of any subscriptions on a replica and the pg_subscription_rel table shows the status of each table within a subscription.

The srsubstate column in pg_subscription_rel can be one of the following:

i - Initializing - The subscription is being initialized
d - Data Synchronizing - The subscription is synchronizing data for the first time (i.e. doing the initial copy)
s - Synchronized - The subscription is synchronized
r - Replicating - The subscription is replicating data
SELECT
    sub.subname AS subscription_name,
    relid::regclass AS table_name,
    srel.srsubstate AS replication_state,
    CASE srel.srsubstate
        WHEN 'i' THEN 'Initializing'
        WHEN 'd' THEN 'Data Synchronizing'
        WHEN 's' THEN 'Synchronized'
        WHEN 'r' THEN 'Replicating'
        ELSE 'Unknown'
    END AS state_description,
    srel.srsyncedlsn AS last_synced_lsn
FROM
    pg_subscription sub
JOIN
    pg_subscription_rel srel ON sub.oid = srel.srsubid
ORDER BY
    table_name;
Check LSN#
select pg_last_wal_replay_lsn();
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Primary
Replication status and lag
Replication slot status
WAL size
Check LSN
Subscriber
Subscription status
Check LSN
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
Database
Database replication
FAQ
FAQs

Which connection string should be used?
Always use the direct connection string for logical replication.

Connections through a pooler, such as Supavisor, will not work.

The tool in use does not support IPv6
You can enable the IPv4 add-on for your project.

What is XMIN and should it be used?
Xmin is a different form of replication from logical replication and should only be used if logical replication is not available for your database (i.e. older versions of Postgres).

Xmin performs replication by checking the xmin system column and determining if that row has already been synchronized.

It does not capture deletion of data and is not recommended, particularly for larger databases.

Can replication be configured in the Dashboard?
You can view publications in the Dashboard but all steps to configure replication must be done using the SQL Editor or a CLI tool of your choice.

How to configure database settings for replication?
Yes. Using the Supabase CLI, you can configure database settings to optimize them for your replication needs. These values can vary depending on the activity of your database size and activity.

What are some important configuration options?
Some of the more important options to be aware of are:

max_wal_size
max_slot_wal_keep_size
wal_keep_size
max_wal_senders
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Extensions
Overview
Postgres Extensions Overview

Extensions are exactly as they sound - they "extend" the database with functionality which isn't part of the Postgres core.
Supabase has pre-installed some of the most useful open source extensions.

Enable and disable extensions#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click Extensions in the sidebar.
Enable or disable an extension.
Most extensions are installed under the extensions schema, which is accessible to public by default. To avoid namespace pollution, we do not recommend creating other entities in the extensions schema.

If you need to restrict user access to tables managed by extensions, we recommend creating a separate schema for installing that specific extension.

Some extensions can only be created under a specific schema, for example, postgis_tiger_geocoder extension creates a schema named tiger. Before enabling such extensions, make sure you have not created a conflicting schema with the same name.

In addition to the pre-configured extensions, you can also install your own SQL extensions directly in the database using Supabase's SQL editor. The SQL code for the extensions, including plpgsql extensions, can be added through the SQL editor.

Upgrade extensions#
If a new version of an extension becomes available on Supabase, you need to initiate a software upgrade in the Infrastructure Settings to access it. Software upgrades can also be initiated by restarting your server in the General Settings.

Full list of extensions#
Supabase is pre-configured with over 50 extensions and you can install additional extensions through the database.dev package manager.

You can install pure SQL extensions directly in the database using the SQL editor or any Postgres client.

If you would like to request an extension, add (or upvote) it in the GitHub Discussion.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable and disable extensions
Upgrade extensions
Full list of extensions
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Extensions
HypoPG: Hypothetical indexes
HypoPG: Hypothetical indexes

HypoPG is Postgres extension for creating hypothetical/virtual indexes. HypoPG allows users to rapidly create hypothetical/virtual indexes that have no resource cost (CPU, disk, memory) that are visible to the Postgres query planner.

The motivation for HypoPG is to allow users to quickly search for an index to improve a slow query without consuming server resources or waiting for them to build.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for hypopg and enable the extension.
Speeding up a query#
Given the following table and a simple query to select from the table by id:

create table account (
  id int,
  address text
);
insert into account(id, address)
select
  id,
  id || ' main street'
from
  generate_series(1, 10000) id;
We can generate an explain plan for a description of how the Postgres query planner
intends to execute the query.

explain select * from account where id=1;
                      QUERY PLAN
-------------------------------------------------------
 Seq Scan on account  (cost=0.00..180.00 rows=1 width=13)
   Filter: (id = 1)
(2 rows)
Using HypoPG, we can create a hypothetical index on the account(id) column to check if it would be useful to the query planner and then re-run the explain plan.

Note that the virtual indexes created by HypoPG are only visible in the Postgres connection that they were created in. Supabase connects to Postgres through a connection pooler so the hypopg_create_index statement and the explain statement should be executed in a single query.

select * from hypopg_create_index('create index on account(id)');
explain select * from account where id=1;
                                     QUERY PLAN
------------------------------------------------------------------------------------
 Index Scan using <13504>btree_account_id on hypo  (cost=0.29..8.30 rows=1 width=13)
   Index Cond: (id = 1)
(2 rows)
The query plan has changed from a Seq Scan to an Index Scan using the newly created virtual index, so we may choose to create a real version of the index to improve performance on the target query:

create index on account(id);
Functions#
hypo_create_index(text): A function to create a hypothetical index.
hypopg_list_indexes: A View that lists all hypothetical indexes that have been created.
hypopg(): A function that lists all hypothetical indexes that have been created with the same format as pg_index.
hypopg_get_index_def(oid): A function to display the create index statement that would create the index.
hypopg_get_relation_size(oid): A function to estimate how large a hypothetical index would be.
hypopg_drop_index(oid): A function to remove a given hypothetical index by oid.
hypopg_reset(): A function to remove all hypothetical indexes.
Resources#
Official HypoPG documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Speeding up a query
Functions
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Extensions
plv8 (deprecated)
plv8: JavaScript Language

The plv8 extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See the Upgrading to Postgres 17 notes for more information.

The plv8 extension allows you use JavaScript within Postgres.

Overview#
While Postgres natively runs SQL, it can also run other procedural languages.
plv8 allows you to run JavaScript code - specifically any code that runs on the V8 JavaScript engine.

It can be used for database functions, triggers, queries and more.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "plv8" and enable the extension.
Create plv8 functions#
Functions written in plv8 are written just like any other Postgres functions, only
with the language identifier set to plv8.

create or replace function function_name()
returns void as $$
    // V8 JavaScript
    // code
    // here
$$ language plv8;
You can call plv8 functions like any other Postgres function:


SQL

JavaScript

Kotlin
select function_name();
Examples#
Scalar functions#
A scalar function is anything that takes in some user input and returns a single result.

create or replace function hello_world(name text)
returns text as $$
    let output = `Hello, ${name}!`;
    return output;
$$ language plv8;
Executing SQL#
You can execute SQL within plv8 code using the plv8.execute function.

create or replace function update_user(id bigint, first_name text)
returns smallint as $$
    var num_affected = plv8.execute(
        'update profiles set first_name = $1 where id = $2',
        [first_name, id]
    );
    return num_affected;
$$ language plv8;
Set-returning functions#
A set-returning function is anything that returns a full set of results - for example, rows in a table.

create or replace function get_messages()
returns setof messages as $$
    var json_result = plv8.execute(
        'select * from messages'
    );
    return json_result;
$$ language plv8;
select * from get_messages();
Resources#
Official plv8 documentation
plv8 GitHub Repository
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Enable the extension
Create plv8 functions
Examples
Scalar functions
Executing SQL
Set-returning functions
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
	
Database
Extensions
http: RESTful Client
http: RESTful Client

The http extension allows you to call RESTful endpoints within Postgres.

Quick demo#

Overview#
Let's cover some basic concepts:

REST: stands for REpresentational State Transfer. It's a way to request data from external services.
RESTful APIs are servers which accept HTTP "calls". The calls are typically:
GET − Read only access to a resource.
POST − Creates a new resource.
DELETE − Removes a resource.
PUT − Updates an existing resource or creates a new resource.
You can use the http extension to make these network requests from Postgres.

Usage#
Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for http and enable the extension.
Available functions#
While the main usage is http('http_request'), there are 5 wrapper functions for specific functionality:

http_get()
http_post()
http_put()
http_delete()
http_head()
Returned values#
A successful call to a web URL from the http extension returns a record with the following fields:

status: integer
content_type: character varying
headers: http_header[]
content: character varying. Typically you would want to cast this to jsonb using the format content::jsonb
Examples#
Simple GET example#
select
  "status", "content"::jsonb
from
  extensions.http_get('https://jsonplaceholder.typicode.com/todos/1');
Simple POST example#
select
  "status", "content"::jsonb
from
  extensions.http_post(
    'https://jsonplaceholder.typicode.com/posts',
    '{ "title": "foo", "body": "bar", "userId": 1 }',
    'application/json'
  );
Resources#
Official http GitHub Repository
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Quick demo
Overview
Usage
Enable the extension
Available functions
Returned values
Examples
Simple GET example
Simple POST example
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Extensions
index_advisor: Query optimization
index_advisor: query optimization

Index advisor is a Postgres extension for recommending indexes to improve query performance.

Features:

Supports generic parameters e.g. $1, $2
Supports materialized views
Identifies tables/columns obfuscated by views
Skips duplicate indexes
index_advisor is accessible directly through Supabase Studio by navigating to the Query Performance Report and selecting a query and then the "indexes" tab.

Supabase Studio index_advisor integration.

Alternatively, you can use index_advisor directly via SQL.

For example:

select
    *
from
  index_advisor('select book.id from book where title = $1');
 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                   | errors
---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------
 0.00                | 1.17               | 25.88             | 6.40             | {"CREATE INDEX ON public.book USING btree (title)"},| {}
(1 row)
Installation#
To get started, enable index_advisor by running

create extension index_advisor;
API#
Index advisor exposes a single function index_advisor(query text) that accepts a query and searches for a set of SQL DDL create index statements that improve the query's execution time.

The function's signature is:

index_advisor(query text)
returns
    table  (
        startup_cost_before jsonb,
        startup_cost_after jsonb,
        total_cost_before jsonb,
        total_cost_after jsonb,
        index_statements text[],
        errors text[]
    )
Usage#
As a minimal example, the index_advisor function can be given a single table query with a filter on an unindexed column.

create extension if not exists index_advisor cascade;
create table book(
  id int primary key,
  title text not null
);
select
  *
from
  index_advisor('select book.id from book where title = $1');
 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                   | errors
---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------
 0.00                | 1.17               | 25.88             | 6.40             | {"CREATE INDEX ON public.book USING btree (title)"},| {}
(1 row)
and will return a row recommending an index on the unindexed column.

More complex queries may generate additional suggested indexes:

create extension if not exists index_advisor cascade;
create table author(
    id serial primary key,
    name text not null
);
create table publisher(
    id serial primary key,
    name text not null,
    corporate_address text
);
create table book(
    id serial primary key,
    author_id int not null references author(id),
    publisher_id int not null references publisher(id),
    title text
);
create table review(
    id serial primary key,
    book_id int references book(id),
    body text not null
);
select
    *
from
    index_advisor('
        select
            book.id,
            book.title,
            publisher.name as publisher_name,
            author.name as author_name,
            review.body review_body
        from
            book
            join publisher
                on book.publisher_id = publisher.id
            join author
                on book.author_id = author.id
            join review
                on book.id = review.book_id
        where
            author.id = $1
            and publisher.id = $2
    ');
 startup_cost_before | startup_cost_after | total_cost_before | total_cost_after |                  index_statements                         | errors
---------------------+--------------------+-------------------+------------------+-----------------------------------------------------------+--------
 27.26               | 12.77              | 68.48             | 42.37            | {"CREATE INDEX ON public.book USING btree (author_id)",   | {}
                                                                                    "CREATE INDEX ON public.book USING btree (publisher_id)",
                                                                                    "CREATE INDEX ON public.review USING btree (book_id)"}
(3 rows)
Limitations#
index_advisor will only recommend single column, B-tree indexes. More complex indexes will be supported in future releases.
when a generic argument's type is not discernible from context, an error is returned in the errors field. To resolve those errors, add explicit type casting to the argument. e.g. $1::int.
Resources#
index_advisor repo
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Installation
API
Usage
Limitations
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

Database
Extensions
PGAudit: Postgres Auditing
PGAudit: Postgres Auditing

PGAudit extends Postgres's built-in logging abilities. It can be used to selectively track activities within your database.

This helps you with:

Compliance: Meeting audit requirements for regulations
Security: Detecting suspicious database activity
Troubleshooting: Identifying and fixing database issues
Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pgaudit and enable the extension.
Configure the extension#
PGAudit can be configured with different levels of precision.

PGAudit logging precision:

Session: Logs activity within a connection, such as a psql connection.
User: Logs activity by a particular database user (for example, anon or postgres).
Global: Logs activity across the entire database.
Object: Logs events related to specific database objects (for example, the auth.users table).
Although Session, User, and Global modes differ in their precision, they're all considered variants of Session Mode and are configured with the same input categories.

Session mode categories#
These modes can monitor predefined categories of database operations:

Category	What it Logs	Description
read	Data retrieval (SELECT, COPY)	Tracks what data is being accessed.
write	Data modification (INSERT, DELETE, UPDATE, TRUNCATE, COPY)	Tracks changes made to your database.
function	FUNCTION, PROCEDURE, and DO/END block executions	Tracks routine/function executions
role	User management actions (CREATE, DROP, ALTER on users and privileges)	Tracks changes to user permissions and access.
ddl	Schema changes (CREATE, DROP, ALTER statements)	Monitors modifications to your database structure (tables, indexes, etc.).
misc	Less common commands (FETCH, CHECKPOINT)	Captures obscure actions for deeper analysis if needed.
all	Everything above	Comprehensive logging for complete audit trails.
Below is a limited example of how to assign PGAudit to monitor specific categories.

-- log all CREATE, ALTER, and DROP events
... pgaudit.log = 'ddl';
-- log all CREATE, ALTER, DROP, and SELECT events
... pgaudit.log = 'read, ddl';
-- log nothing
... pgaudit.log = 'none';
Session logging#
When you are connecting in a session environment, such as a psql connection, you can configure PGAudit to record events initiated within the session.

The Dashboard is a transactional environment and won't sustain a session.

Inside a session, by default, PGAudit will log nothing:

-- returns 'none'
show pgaudit.log;
In the session, you can set the pgaudit.log variable to record events:

-- log CREATE, ALTER, and DROP events
set pgaudit.log = 'ddl';
-- log all CREATE, ALTER, DROP, and SELECT events
set pgaudit.log = 'read, ddl';
-- log nothing
set pgaudit.log = 'none';
User logging#
There are some cases where you may want to monitor a database user's actions. For instance, let's say you connected your database to Zapier and created a custom role for it to use:

create user "zapier" with password '<new password>';
You may want to log all actions initiated by zapier, which can be done with the following command:

alter role "zapier" set pgaudit.log to 'all';
To remove the settings, execute the following code:

-- disables role's log
alter role "zapier" set pgaudit.log to 'none';
-- check to make sure the changes are finalized:
select
  rolname,
  rolconfig
from pg_roles
where rolname = 'zapier';
-- should return a rolconfig path with "pgaudit.log=none" present
Global logging#
Use global logging cautiously. It can generate many logs and make it difficult to find important events. Consider limiting the scope of what is logged by using session, user, or object logging where possible.

The below SQL configures PGAudit to record all events associated with the postgres role. Since it has extensive privileges, this effectively monitors all database activity.

alter role "postgres" set pgaudit.log to 'all';
To check if the postgres role is auditing, execute the following command:

select
  rolname,
  rolconfig
from pg_roles
where rolname = 'postgres';
-- should return a rolconfig path with "pgaudit.log=all" present
To remove the settings, execute the following code:

alter role "postgres" set pgaudit.log to 'none';
Object logging#
To fine-tune what object events PGAudit will record, you must create a custom database role with limited permissions:

create role "some_audit_role" noinherit;
No other Postgres user can assume or login via this role. It solely exists to securely define what PGAudit will record.

Once the role is created, you can direct PGAudit to log by assigning it to the pgaudit.role variable:

alter role "postgres" set pgaudit.role to 'some_audit_role';
You can then assign the role to monitor only approved object events, such as select statements that include a specific table:

grant select on random_table to "some_audit_role";
With this privilege granted, PGAudit will record all select statements that reference the random_table, regardless of who or what actually initiated the event. All assignable privileges can be viewed in the Postgres documentation.

If you would no longer like to use object logging, you will need to unassign the pgaudit.role variable:

-- change pgaudit.role to no longer reference some_audit_role
alter role "postgres" set pgaudit.role to '';
-- view if pgaudit.role changed with the following command:
select
  rolname,
  rolconfig
from pg_roles
where rolname = 'postgres';
-- should return a rolconfig path with "pgaudit.role="
Interpreting Audit Logs#
PGAudit was designed for storing logs as CSV files with the following headers:

Referenced from the PGAudit official docs

header	Description
AUDIT_TYPE	SESSION or OBJECT
STATEMENT_ID	Unique statement ID for this session. Sequential even if some statements are not logged.
SUBSTATEMENT_ID	Sequential ID for each sub-statement within the main statement. Continuous even if some are not logged.
CLASS	..., READ, ROLE (see pgaudit.log).
COMMAND	..., ALTER TABLE, SELECT.
OBJECT_TYPE	TABLE, INDEX, VIEW, etc. Available for SELECT, DML, and most DDL statements.
OBJECT_NAME	The fully qualified object name (for example, public.account). Available for SELECT, DML, and most DDL.
STATEMENT	Statement executed on the backend.
PARAMETER	If pgaudit.log_parameter is set, this field contains the statement parameters as quoted CSV, or <none>. Otherwise, it's <not logged>.
A log made from the following create statement:

create table account (
  id int primary key,
  name text,
  description text
);
Generates the following log in the Dashboard's Postgres Logs:

AUDIT: SESSION,1,1,DDL,CREATE TABLE,TABLE,public.account,create table account(
  id int,
  name text,
  description text
); <not logged>
Finding and filtering audit logs#
Logs generated by PGAudit can be found in Postgres Logs. To find a specific log, you can use the log explorer. Below is a basic example to extract logs referencing CREATE TABLE events

select
  cast(t.timestamp as datetime) as timestamp,
  event_message
from
  postgres_logs as t
  cross join unnest(metadata) as m
  cross join unnest(m.parsed) as p
where event_message like 'AUDIT%CREATE TABLE%'
order by timestamp desc
limit 100;
Practical examples#
Monitoring API events#
API requests are already recorded in the API Edge Network logs.

To monitor all writes initiated by the PostgREST API roles:

alter role "authenticator" set pgaudit.log to 'write';
-- the above is the practical equivalent to:
-- alter role "anon" set pgaudit.log TO 'write';
-- alter role "authenticated" set pgaudit.log TO 'write';
-- alter role "service_role" set pgaudit.log TO 'write';
Monitoring the auth.users table#
In the worst case scenario, where a privileged roles' password is exposed, you can use PGAudit to monitor if the auth.users table was targeted. It should be stated that API requests are already monitored in the API Edge Network and this is more about providing greater clarity about what is happening at the database level.

Logging auth.user should be done in Object Mode and requires a custom role:

-- create logging role
create role "auth_auditor" noinherit;
-- give role permission to observe relevant table events
grant select on auth.users to "auth_auditor";
grant delete on auth.users to "auth_auditor";
-- assign auth_auditor to pgaudit.role
alter role "postgres" set pgaudit.role to 'auth_auditor';
With the above code, any query involving reading or deleting from the auth.users table will be logged.

Best practices#
Disabling excess logging#
PGAudit, if not configured mindfully, can log all database events, including background tasks. This can generate an undesirably large amount of logs in a few hours.

The first step to solve this problem is to identify which database users PGAudit is observing:

-- find all users monitored by pgaudit
select
  rolname,
  rolconfig
from pg_roles
where
  exists (
    select
      1
    from UNNEST(rolconfig) as c
    where c like '%pgaudit.role%' or c like '%pgaudit.log%'
  );
To prevent PGAudit from monitoring the problematic roles, you'll want to change their pgaudit.log values to none and pgaudit.role values to empty quotes ''

-- Use to disable object level logging
  alter role "<role name>" set pgaudit.role to '';
  -- Use to disable global and user level logging
  alter role "<role name>" set pgaudit.log to 'none';
FAQ#
Using PGAudit to debug database functions#
Technically yes, but it is not the best approach. It is better to check out our function debugging guide instead.

Downloading database logs#
In the Logs Dashboard you can download logs as CSVs.

Logging observed table rows#
By default, PGAudit records queries, but not the returned rows. You can modify this behavior with the pgaudit.log_rows variable:

--enable
alter role "postgres" set pgaudit.log_rows to 'on';
-- disable
alter role "postgres" set pgaudit.log_rows to 'off';
You should not do this unless you are absolutely certain it is necessary for your use case. It can expose sensitive values to your logs that ideally should not be preserved. Furthermore, if done in excess, it can noticeably reduce database performance.

Logging function parameters#
We don't currently support configuring pgaudit.log_parameter because it may log secrets in encrypted columns if you are using pgsodium orVault.

You can upvote this feature request with your use-case if you'd like this restriction lifted.

Does PGAudit support system wide configurations?#
PGAudit allows settings to be applied to 3 different database scopes:

Scope	Description	Configuration File/Command
System	Entire server	ALTER SYSTEM commands
Database	Specific database	ALTER DATABASE commands
Role	Specific user/role	ALTER ROLE commands
Supabase limits full privileges for file system and database variables, meaning PGAudit modifications can only occur at the role level. Assigning PGAudit to the postgres role grants it nearly complete visibility into the database, making role-level adjustments a practical alternative to configuring at the database or system level.

PGAudit's official documentation focuses on system and database level configs, but its docs officially supports role level configs, too.

Resources#
Official PGAudit documentation
Database Function Logging
Supabase Logging
Self-Hosting Logs
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Configure the extension
Session mode categories
Session logging
User logging
Global logging
Object logging
Interpreting Audit Logs
Finding and filtering audit logs
Practical examples
Monitoring API events
Monitoring the auth.users table
Best practices
Disabling excess logging
FAQ
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status

pgjwt: JSON Web Tokens

The pgjwt extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See the Upgrading to Postgres 17 notes for more information.

The pgjwt (Postgres JSON Web Token) extension allows you to create and parse JSON Web Tokens (JWTs) within a Postgres database. JWTs are commonly used for authentication and authorization in web applications and services.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pgjwt and enable the extension.
API#
sign(payload json, secret text, algorithm text default 'HSA256'): Signs a JWT containing payload with secret using algorithm.
verify(token text, secret text, algorithm text default 'HSA256'): Decodes a JWT token that was signed with secret using algorithm.
Where:

payload is an encrypted JWT represented as a string.
secret is the private/secret passcode which is used to sign the JWT and verify its integrity.
algorithm is the method used to sign the JWT using the secret.
token is an encrypted JWT represented as a string.
Usage#
Once the extension is installed, you can use its functions to create and parse JWTs. Here's an example of how you can use the sign function to create a JWT:

select
  extensions.sign(
    payload   := '{"sub":"1234567890","name":"John Doe","iat":1516239022}',
    secret    := 'secret',
    algorithm := 'HS256'
  );
The pgjwt_encode function returns a string that represents the JWT, which can then be safely transmitted between parties.

sign
---------------------------------
 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpX
 VCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiw
 ibmFtZSI6IkpvaG4gRG9lIiwiaWF0Ijo
 xNTE2MjM5MDIyfQ.XbPfbIHMI6arZ3Y9
 22BhjWgQzWXcXNrz0ogtVhfEd2o
(1 row)
To parse a JWT and extract its claims, you can use the verify function. Here's an example:

select
  extensions.verify(
    token := 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiRm9vIn0.Q8hKjuadCEhnCPuqIj9bfLhTh_9QSxshTRsA5Aq4IuM',
    secret    := 'secret',
    algorithm := 'HS256'
  );
Which returns the decoded contents and some associated metadata.

header            |    payload     | valid
-----------------------------+----------------+-------
 {"alg":"HS256","typ":"JWT"} | {"name":"Foo"} | t
(1 row)
Resources#
Official pgjwt documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
API
Usage
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
PGroonga: Multilingual Full Text Search
PGroonga: Multilingual Full Text Search

PGroonga is a Postgres extension adding a full text search indexing method based on Groonga. While native Postgres supports full text indexing, it is limited to alphabet and digit based languages. PGroonga offers a wider range of character support making it viable for a superset of languages supported by Postgres including Japanese, Chinese, etc.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pgroonga and enable the extension.
Creating a full text search index#
Given a table with a text column:

create table memos (
  id serial primary key,
  content text
);
We can index the column for full text search with a pgroonga index:

create index ix_memos_content ON memos USING pgroonga(content);
To test the full text index, we'll add some data.

insert into memos(content)
values
  ('PostgreSQL is a relational database management system.'),
  ('Groonga is a fast full text search engine that supports all languages.'),
  ('PGroonga is a PostgreSQL extension that uses Groonga as index.'),
  ('There is groonga command.');
The Postgres query planner is smart enough to know that, for extremely small tables, it's faster to scan the whole table rather than loading an index. To force the index to be used, we can disable sequential scans:

-- For testing only. Don't do this in production
set enable_seqscan = off;
Now if we run an explain plan on a query filtering on memos.content:

explain select * from memos where content like '%engine%';
                               QUERY PLAN
-----------------------------------------------------------------------------
Index Scan using ix_memos_content on memos  (cost=0.00..1.11 rows=1 width=36)
  Index Cond: (content ~~ '%engine%'::text)
(2 rows)
The pgroonga index is used to retrieve the result set:

| id  | content                                                                  |
| --- | ------------------------------------------------------------------------ |
| 2   | 'Groonga is a fast full text search engine that supports all languages.' |
Full text search#
The &@~ operator performs full text search. It returns any matching results. Unlike LIKE operator, pgroonga can search any text that contains the keyword case insensitive.

Take the following example:

select * from memos where content &@~ 'groonga';
And the result:

id | content  
----+------------------------------------------------------------------------
2 | Groonga is a fast full text search engine that supports all languages.
3 | PGroonga is a PostgreSQL extension that uses Groonga as index.
4 | There is groonga command.
(3 rows)
Match all search words#
To find all memos where content contains BOTH of the words postgres and pgroonga, we can just use space to separate each words:

select * from memos where content &@~ 'postgres pgroonga';
And the result:

id | content  
----+----------------------------------------------------------------
3 | PGroonga is a PostgreSQL extension that uses Groonga as index.
(1 row)
Match any search words#
To find all memos where content contain ANY of the words postgres or pgroonga, use the upper case OR:

select * from memos where content &@~ 'postgres OR pgroonga';
And the result:

id | content  
----+----------------------------------------------------------------
1 | PostgreSQL is a relational database management system.
3 | PGroonga is a PostgreSQL extension that uses Groonga as index.
(2 rows)
Search that matches words with negation#
To find all memos where content contain the word postgres but not pgroonga, use - symbol:

select * from memos where content &@~ 'postgres -pgroonga';
And the result:

id | content  
----+--------------------------------------------------------
1 | PostgreSQL is a relational database management system.
(1 row)
Resources#
Official PGroonga documentation
Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Enable the extension
Creating a full text search index
Full text search
Match all search words
Match any search words
Search that matches words with negation
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pgRouting: Geospatial Routing
pgrouting: Geospatial Routing

pgRouting is Postgres and PostGIS extension adding geospatial routing functionality.

The core functionality of pgRouting is a set of path finding algorithms including:

All Pairs Shortest Path, Johnson’s Algorithm
All Pairs Shortest Path, Floyd-Warshall Algorithm
Shortest Path A*
Bi-directional Dijkstra Shortest Path
Bi-directional A* Shortest Path
Shortest Path Dijkstra
Driving Distance
K-Shortest Path, Multiple Alternative Paths
K-Dijkstra, One to Many Shortest Path
Traveling Sales Person
Turn Restriction Shortest Path (TRSP)
Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pgrouting and enable the extension.
Example#
As an example, we'll solve the traveling salesperson problem using the pgRouting's pgr_TSPeuclidean function from some PostGIS coordinates.

A summary of the traveling salesperson problem is, given a set of city coordinates, solve for a path that goes through each city and minimizes the total distance traveled.

First we populate a table with some X, Y coordinates

create table wi29 (
  id bigint,
  x float,
  y float,
  geom gis.geometry
);
insert into wi29 (id, x, y)
values
  (1,20833.3333,17100.0000),
  (2,20900.0000,17066.6667),
  (3,21300.0000,13016.6667),
  (4,21600.0000,14150.0000),
  (5,21600.0000,14966.6667),
  (6,21600.0000,16500.0000),
  (7,22183.3333,13133.3333),
  (8,22583.3333,14300.0000),
  (9,22683.3333,12716.6667),
  (10,23616.6667,15866.6667),
  (11,23700.0000,15933.3333),
  (12,23883.3333,14533.3333),
  (13,24166.6667,13250.0000),
  (14,25149.1667,12365.8333),
  (15,26133.3333,14500.0000),
  (16,26150.0000,10550.0000),
  (17,26283.3333,12766.6667),
  (18,26433.3333,13433.3333),
  (19,26550.0000,13850.0000),
  (20,26733.3333,11683.3333),
  (21,27026.1111,13051.9444),
  (22,27096.1111,13415.8333),
  (23,27153.6111,13203.3333),
  (24,27166.6667,9833.3333),
  (25,27233.3333,10450.0000),
  (26,27233.3333,11783.3333),
  (27,27266.6667,10383.3333),
  (28,27433.3333,12400.0000),
  (29,27462.5000,12992.2222);
Next we use the pgr_TSPeuclidean function to find the best path.

select
    *
from
     pgr_TSPeuclidean($$select * from wi29$$)
seq | node |       cost       |     agg_cost     
-----+------+------------------+------------------
   1 |    1 |                0 |                0
   2 |    2 |  74.535614157127 |  74.535614157127
   3 |    6 | 900.617093380362 | 975.152707537489
   4 |   10 | 2113.77757765045 | 3088.93028518793
   5 |   11 | 106.718669615254 | 3195.64895480319
   6 |   12 | 1411.95293791574 | 4607.60189271893
   7 |   13 | 1314.23824873744 | 5921.84014145637
   8 |   14 | 1321.76283931305 | 7243.60298076942
   9 |   17 | 1202.91366735569 |  8446.5166481251
  10 |   18 | 683.333268292684 | 9129.84991641779
  11 |   15 | 1108.05137466134 | 10237.9012910791
  12 |   19 | 772.082339448903 |  11009.983630528
  13 |   22 | 697.666150054665 | 11707.6497805827
  14 |   23 | 220.141999627513 | 11927.7917802102
  15 |   21 | 197.926372783442 | 12125.7181529937
  16 |   29 | 440.456596290771 | 12566.1747492844
  17 |   28 | 592.939989005405 | 13159.1147382898
  18 |   26 | 648.288376333318 | 13807.4031146231
  19 |   20 | 509.901951359278 | 14317.3050659824
  20 |   25 | 1330.83095428717 | 15648.1360202696
  21 |   27 |  74.535658878487 | 15722.6716791481
  22 |   24 | 559.016994374947 |  16281.688673523
  23 |   16 | 1243.87392358622 | 17525.5625971092
  24 |    9 |  4088.0585364911 | 21613.6211336004
  25 |    7 |  650.85409697993 | 22264.4752305803
  26 |    3 | 891.004385199336 | 23155.4796157796
  27 |    4 | 1172.36699411442 |  24327.846609894
  28 |    8 | 994.708187806297 | 25322.5547977003
  29 |    5 | 1188.01888359478 | 26510.5736812951
  30 |    1 | 2266.91173136004 | 28777.4854126552
Resources#
Official pgRouting documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Example
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_cron: Schedule Recurring Jobs
pg_cron: Schedule Recurring Jobs with Cron Syntax in Postgres

See the Supabase Cron docs.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_graphql: GraphQL Support
pg_graphql: GraphQL for PostgreSQL

pg_graphql is Postgres extension for interacting with the database using GraphQL instead of SQL.

The extension reflects a GraphQL schema from the existing SQL schema and exposes it through a SQL function, graphql.resolve(...). This enables any programming language that can connect to Postgres to query the database via GraphQL with no additional servers, processes, or libraries.

The pg_graphql resolve method is designed to interop with PostgREST, the tool that underpins the Supabase API, such that the graphql.resolve function can be called via RPC to safely and performantly expose the GraphQL API over HTTP/S.

For more information about how the SQL schema is reflected into a GraphQL schema, see the pg_graphql API docs.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "pg_graphql" and enable the extension.
Usage#
Given a table

create table "Blog"(
  id serial primary key,
  name text not null,
  description text
);
insert into "Blog"(name)
values ('My Blog');
The reflected GraphQL schema can be queried immediately as

select
  graphql.resolve($$
    {
      blogCollection(first: 1) {
        edges {
          node {
            id,
            name
          }
        }
      }
    }
  $$);
returning the JSON

{
  "data": {
    "blogCollection": {
      "edges": [
        {
          "node": {
            "id": 1
            "name": "My Blog"
          }
        }
      ]
    }
  }
}
Note that pg_graphql fully supports schema introspection so you can connect any GraphQL IDE or schema inspection tool to see the full set of fields and arguments available in the API.

API#
graphql.resolve: A SQL function for executing GraphQL queries.
Resources#
Official pg_graphql documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Usage
API
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_hashids: Short UIDs
pg_hashids: Short UIDs

pg_hashids provides a secure way to generate short, unique, non-sequential ids from numbers. The hashes are intended to be small, easy-to-remember identifiers that can be used to obfuscate data (optionally) with a password, alphabet, and salt. For example, you may wish to hide data like user IDs, order numbers, or tracking codes in favor of pg_hashid's unique identifiers.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "pg_hashids" and enable the extension.
Usage#
Suppose we have a table that stores order information, and we want to give customers a unique identifier without exposing the sequential id column. To do this, we can use pg_hashid's id_encode function.

create table orders (
  id serial primary key,
  description text,
  price_cents bigint
);
insert into orders (description, price_cents)
values ('a book', 9095);
select
  id,
  id_encode(id) as short_id,
  description,
  price_cents
from
  orders;
  id | short_id | description | price_cents
----+----------+-------------+-------------
  1 | jR       | a book      |        9095
(1 row)
To reverse the short_id back into an id, there is an equivalent function named id_decode.

Resources#
Official pg_hashids documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Usage
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_jsonschema: JSON Schema Validation
pg_jsonschema: JSON Schema Validation

JSON Schema is a language for annotating and validating JSON documents. pg_jsonschema is a Postgres extension that adds the ability to validate PostgreSQL's built-in json and jsonb data types against JSON Schema documents.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pg_jsonschema and enable the extension.
Functions#
json_matches_schema(schema json, instance json): Checks if a json instance conforms to a JSON Schema schema.
jsonb_matches_schema(schema json, instance jsonb): Checks if a jsonb instance conforms to a JSON Schema schema.
Usage#
Since pg_jsonschema exposes its utilities as functions, we can execute them with a select statement:

select
  extensions.json_matches_schema(
    schema := '{"type": "object"}',
    instance := '{}'
  );
pg_jsonschema is generally used in tandem with a check constraint as a way to constrain the contents of a json/b column to match a JSON Schema.

create table customer(
    id serial primary key,
    ...
    metadata json,
    check (
        json_matches_schema(
            '{
                "type": "object",
                "properties": {
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "maxLength": 16
                        }
                    }
                }
            }',
            metadata
        )
    )
);
-- Example: Valid Payload
insert into customer(metadata)
values ('{"tags": ["vip", "darkmode-ui"]}');
-- Result:
--   INSERT 0 1
-- Example: Invalid Payload
insert into customer(metadata)
values ('{"tags": [1, 3]}');
-- Result:
--   ERROR:  new row for relation "customer" violates check constraint "customer_metadata_check"
--   DETAIL:  Failing row contains (2, {"tags": [1, 3]}).
Resources#
Official pg_jsonschema documentation
Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Enable the extension
Functions
Usage
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_net: Async Networking
pg_net: Async Networking

The pg_net API is in beta. Functions signatures may change.

pg_net enables Postgres to make asynchronous HTTP/HTTPS requests in SQL. It differs from the http extension in that it is asynchronous by default. This makes it useful in blocking functions (like triggers).

It eliminates the need for servers to continuously poll for database changes and instead allows the database to proactively notify external resources about significant events.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "pg_net" and enable the extension.
http_get#
Creates an HTTP GET request returning the request's ID. HTTP requests are not started until the transaction is committed.

Signature #
This is a Postgres SECURITY DEFINER function.

net.http_get(
    -- url for the request
    url text,
    -- key/value pairs to be url encoded and appended to the `url`
    params jsonb default '{}'::jsonb,
    -- key/values to be included in request headers
    headers jsonb default '{}'::jsonb,
    -- the maximum number of milliseconds the request may take before being canceled
    timeout_milliseconds int default 2000
)
    -- request_id reference
    returns bigint
    strict
    volatile
    parallel safe
    language plpgsql
Usage #
select
    net.http_get('https://news.ycombinator.com')
    as request_id;
request_id
----------
         1
(1 row)
http_post#
Creates an HTTP POST request with a JSON body, returning the request's ID. HTTP requests are not started until the transaction is committed.

The body's character set encoding matches the database's server_encoding setting.

Signature #
This is a Postgres SECURITY DEFINER function

net.http_post(
    -- url for the request
    url text,
    -- body of the POST request
    body jsonb default '{}'::jsonb,
    -- key/value pairs to be url encoded and appended to the `url`
    params jsonb default '{}'::jsonb,
    -- key/values to be included in request headers
    headers jsonb default '{"Content-Type": "application/json"}'::jsonb,
    -- the maximum number of milliseconds the request may take before being canceled
    timeout_milliseconds int default 2000
)
    -- request_id reference
    returns bigint
    volatile
    parallel safe
    language plpgsql
Usage #
select
    net.http_post(
        url:='https://httpbin.org/post',
        body:='{"hello": "world"}'::jsonb
    ) as request_id;
request_id
----------
         1
(1 row)
http_delete#
Creates an HTTP DELETE request, returning the request's ID. HTTP requests are not started until the transaction is committed.

Signature #
This is a Postgres SECURITY DEFINER function

net.http_delete(
    -- url for the request
    url text,
    -- key/value pairs to be url encoded and appended to the `url`
    params jsonb default '{}'::jsonb,
    -- key/values to be included in request headers
    headers jsonb default '{}'::jsonb,
    -- the maximum number of milliseconds the request may take before being canceled
    timeout_milliseconds int default 2000
)
    -- request_id reference
    returns bigint
    strict
    volatile
    parallel safe
    language plpgsql
    security definer
Usage #
select
    net.http_delete(
        'https://dummy.restapiexample.com/api/v1/delete/2'
    ) as request_id;
----------
         1
(1 row)
Analyzing responses#
Waiting requests are stored in the net.http_request_queue table. Upon execution, they are deleted.

CREATE UNLOGGED TABLE
    net.http_request_queue (
        id bigint NOT NULL DEFAULT nextval('net.http_request_queue_id_seq'::regclass),
        method text NOT NULL,
        url text NOT NULL,
        headers jsonb NOT NULL,
        body bytea NULL,
        timeout_milliseconds integer NOT NULL
    )
Once a response is returned, by default, it is stored for 6 hours in the net._http_response table.

CREATE UNLOGGED TABLE
    net._http_response (
        id bigint NULL,
        status_code integer NULL,
        content_type text NULL,
        headers jsonb NULL,
        content text NULL,
        timed_out boolean NULL,
        error_msg text NULL,
        created timestamp with time zone NOT NULL DEFAULT now()
    )
The responses can be observed with the following query:

select * from net._http_response;
The data can also be observed in the net schema with the Supabase Dashboard's SQL Editor

Debugging requests#
Inspecting request data#
The Postman Echo API returns a response with the same body and content
as the request. It can be used to inspect the data being sent.

Sending a post request to the echo API

select
    net.http_post(
        url := 'https://postman-echo.com/post',
        body := '{"key1": "value", "key2": 5}'::jsonb
    ) as request_id;
Inspecting the echo API response content to ensure it contains the right body

select
    "content"
from net._http_response
where id = <request_id>
-- returns information about the request
-- including the body sent: {"key": "value", "key": 5}
Alternatively, by wrapping a request in a database function, sent row data can be logged or returned for inspection and debugging.

create or replace function debugging_example (row_id int)
returns jsonb as $$
declare
    -- Store payload data
    row_data_var jsonb;
begin
    -- Retrieve row data and convert to JSON
    select to_jsonb("<example_table>".*) into row_data_var
    from "<example_table>"
    where "<example_table>".id = row_id;
    -- Initiate HTTP POST request to URL
    perform
        net.http_post(
            url := 'https://postman-echo.com/post',
            -- Use row data as payload
            body := row_data_var
        ) as request_id;
    -- Optionally Log row data or other data for inspection in Supabase Dashboard's Postgres Logs
    raise log 'Logging an entire row as JSON (%)', row_data_var;
    -- return row data to inspect
    return row_data_var;
-- Handle exceptions here if needed
exception
    when others then
        raise exception 'An error occurred: %', SQLERRM;
end;
$$ language plpgsql;
-- calling function
select debugging_example(<row_id>);
Inspecting failed requests#
Finds all failed requests

select
  *
from net._http_response
where "status_code" >= 400 or "error_msg" is not null
order by "created" desc;
Configuration#
Must be on pg_net v0.12.0 or above to reconfigure
Supabase supports reconfiguring pg*net starting from v0.12.0+. For the latest release, initiate a Postgres upgrade in the Infrastructure Settings.

The extension is configured to reliably execute up to 200 requests per second. The response messages are stored for only 6 hours to prevent needless buildup. The default behavior can be modified by rewriting config variables.

Get current settings#
select
  "name",
  "setting"
from pg_settings
where "name" like 'pg_net%';
Alter settings#
Change variables:

alter role "postgres" set pg_net.ttl to '24 hours';
alter role "postgres" set pg_net.batch_size to 500;
Then reload the settings and restart the pg_net background worker with:

select net.worker_restart();
Examples#
Invoke a Supabase Edge Function#
Make a POST request to a Supabase Edge Function with auth header and JSON body payload:

select
    net.http_post(
        url:='https://project-ref.supabase.co/functions/v1/function-name',
        headers:='{"Content-Type": "application/json", "Authorization": "Bearer <YOUR_ANON_KEY>"}'::jsonb,
        body:='{"name": "pg_net"}'::jsonb
    ) as request_id;
Call an endpoint every minute with pg_cron#
The pg_cron extension enables Postgres to become its own cron server. With it you can schedule regular calls with up to a minute precision to endpoints.

select cron.schedule(
	'cron-job-name',
	'* * * * *', -- Executes every minute (cron syntax)
	$$
	    -- SQL query
	    select "net"."http_post"(
            -- URL of Edge function
            url:='https://project-ref.supabase.co/functions/v1/function-name',
            headers:='{"Authorization": "Bearer <YOUR_ANON_KEY>"}'::jsonb,
            body:='{"name": "pg_net"}'::jsonb
	    ) as "request_id";
	$$
);
Execute pg_net in a trigger#
Make a call to an external endpoint when a trigger event occurs.

-- function called by trigger
create or replace function <function_name>()
    returns trigger
    language plpgSQL
as $$
begin
    -- calls pg_net function net.http_post
    -- sends request to postman API
    perform "net"."http_post"(
      'https://postman-echo.com/post'::text,
      jsonb_build_object(
        'old_row', to_jsonb(old.*),
        'new_row', to_jsonb(new.*)
      ),
      headers:='{"Content-Type": "application/json"}'::jsonb
    ) as request_id;
    return new;
END $$;
-- trigger for table update
create trigger <trigger_name>
    after update on <table_name>
    for each row
    execute function <function_name>();
Send multiple table rows in one request#
with "selected_table_rows" as (
    select
        -- Converts all the rows into a JSONB array
        jsonb_agg(to_jsonb(<table_name>.*)) as JSON_payload
    from <table_name>
    -- good practice to LIMIT the max amount of rows
)
select
    net.http_post(
        url := 'https://postman-echo.com/post'::text,
        body := JSON_payload
    ) AS request_id
FROM "selected_table_rows";
More examples can be seen on the Extension's GitHub page

Limitations#
To improve speed and performance, the requests and responses are stored in unlogged tables, which are not preserved during a crash or unclean shutdown.
By default, response data is saved for only 6 hours
Can only make POST requests with JSON data. No other data formats are supported
Intended to handle at most 200 requests per second. Increasing the rate can introduce instability
Does not have support for PATCH/PUT requests
Can only work with one database at a time. It defaults to the postgres database.
Resources#
Source code: github.com/supabase/pg_net
Official Docs: github.com/supabase/pg_net
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
http_get
Signature
Usage
http_post
Signature
Usage
http_delete
Signature
Usage
Analyzing responses
Debugging requests
Inspecting request data
Inspecting failed requests
Configuration
Get current settings
Alter settings
Examples
Invoke a Supabase Edge Function
Call an endpoint every minute with pg_cron
Execute pg_net in a trigger
Send multiple table rows in one request
Limitations
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_plan_filter: Restrict Total Cost
pg_plan_filter: Restrict Total Cost

pg_plan_filter is Postgres extension to block execution of statements where query planner's estimate of the total cost exceeds a threshold. This is intended to give database administrators a way to restrict the contribution an individual query has on database load.

Enable the extension#
The extension is already enabled by default via shared_preload_libraries setting.

You can follow the instructions below.

API#
plan_filter.statement_cost_limit: restricts the maximum total cost for executed statements
plan_filter.limit_select_only: restricts to select statements

Note that limit_select_only = true is not the same as read-only because select statements may modify data, for example, through a function call.

Example#
To demonstrate total cost filtering, we'll compare how plan_filter.statement_cost_limit treats queries that are under and over its cost limit. First, we set up a table with some data:

create table book(
  id int primary key
);
-- CREATE TABLE
insert into book(id) select * from generate_series(1, 10000);
-- INSERT 0 10000
Next, we can review the explain plans for a single record select, and a whole table select.

explain select * from book where id =1;
                                QUERY PLAN
---------------------------------------------------------------------------
 Index Only Scan using book_pkey on book  (cost=0.28..2.49 rows=1 width=4)
   Index Cond: (id = 1)
(2 rows)
explain select * from book;
                       QUERY PLAN
---------------------------------------------------------
 Seq Scan on book  (cost=0.00..135.00 rows=10000 width=4)
(1 row)
Now we can choose a statement_cost_filter value between the total cost for the single select (2.49) and the whole table select (135.0) so one statement will succeed and one will fail.

set plan_filter.statement_cost_limit = 50; -- between 2.49 and 135.0
select * from book where id = 1;
 id
----
  1
(1 row)
-- SUCCESS
select * from book;
ERROR:  plan cost limit exceeded
HINT:  The plan for your query shows that it would probably have an excessive run time. This may be due to a logic error in the SQL, or it maybe just a very costly query. Rewrite your query or increase the configuration parameter "plan_filter.statement_cost_limit".
-- FAILURE
Resources#
Official pg_plan_filter documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
API
Example
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth
V
Database
Extensions
postgres_fdw: query data from an external Postgres server
postgres_fdw

The extension enables Postgres to query tables and views on a remote Postgres server.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "postgres_fdw" and enable the extension.
Create a connection to another database#
1
Create a foreign server
Define the remote database address

create server "<foreign_server_name>"
    foreign data wrapper postgres_fdw
    options (
        host '<host>',
        port '<port>',
        dbname '<dbname>'
    );
2
Create a server mapping
Set the user credentials for the remote server

create user mapping for "<dbname>"
server "<foreign_server_name>"
options (
    user '<db_user>',
    password '<password>'
);
3
Import tables
Import tables from the foreign database

Example: Import all tables from a schema

import foreign schema "<foreign_schema>"
from server "<foreign_server>"
into "<host_schema>";
Example: Import specific tables

import foreign schema "<foreign_schema>"
limit to (
    "<table_name1>",
    "<table_name2>"
)
from server "<foreign_server>"
into "<host_schema>";
4
Query foreign table
select * from "<foreign_table>"
Configuring execution options#
Fetch_size#
Maximum rows fetched per operation. For example, fetching 200 rows with fetch_size set to 100 requires 2 requests.

alter server "<foreign_server_name>"
options (fetch_size '10000');
Batch_size#
Maximum rows inserted per cycle. For example, inserting 200 rows with batch_size set to 100 requires 2 requests.

alter server "<foreign_server_name>"
options (batch_size '1000');
Extensions#
Lists shared extensions. Without them, queries involving unlisted extension functions or operators may fail or omit references.

alter server "<foreign_server_name>"
options (extensions 'vector, postgis');
For more server options, check the extension's official documentation

Resources#
Official postgres_fdw documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Create a connection to another database
Configuring execution options
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth
Database
Extensions
pgvector: Embeddings and vector similarity
pgvector: Embeddings and vector similarity

pgvector is a Postgres extension for vector similarity search. It can also be used for storing embeddings.

The name of pgvector's Postgres extension is vector.

Learn more about Supabase's AI & Vector offering.

Concepts#
Vector similarity#
Vector similarity refers to a measure of the similarity between two related items. For example, if you have a list of products, you can use vector similarity to find similar products. To do this, you need to convert each product into a "vector" of numbers, using a mathematical model. You can use a similar model for text, images, and other types of data. Once all of these vectors are stored in the database, you can use vector similarity to find similar items.

Embeddings#
This is particularly useful if you're building AI applications with large language models. You can create and store embeddings for retrieval augmented generation (RAG).

Usage#
Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "vector" and enable the extension.
Usage#
Create a table to store vectors#
create table posts (
  id serial primary key,
  title text not null,
  body text not null,
  embedding extensions.vector(384)
);
Storing a vector / embedding#
In this example we'll generate a vector using Transformer.js, then store it in the database using the Supabase client.

import { pipeline } from '@xenova/transformers'
const generateEmbedding = await pipeline('feature-extraction', 'Supabase/gte-small')
const title = 'First post!'
const body = 'Hello world!'
// Generate a vector using Transformers.js
const output = await generateEmbedding(body, {
  pooling: 'mean',
  normalize: true,
})
// Extract the embedding output
const embedding = Array.from(output.data)
// Store the vector in Postgres
const { data, error } = await supabase.from('posts').insert({
  title,
  body,
  embedding,
})
Specific usage cases#
Queries with filtering#
If you use an IVFFlat or HNSW index and naively filter the results based on the value of another column, you may get fewer rows returned than requested.

For example, the following query may return fewer than 5 rows, even if 5 corresponding rows exist in the database. This is because the embedding index may not return 5 rows matching the filter.

SELECT * FROM items WHERE category_id = 123 ORDER BY embedding <-> '[3,1,2]' LIMIT 5;
To get the exact number of requested rows, use iterative search to continue scanning the index until enough results are found.

More pgvector and Supabase resources#
Supabase Clippy: ChatGPT for Supabase Docs
Storing OpenAI embeddings in Postgres with pgvector
A ChatGPT Plugins Template built with Supabase Edge Runtime
Template for building your own custom ChatGPT style doc search
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Concepts
Vector similarity
Embeddings
Usage
Enable the extension
Usage
Create a table to store vectors
Storing a vector / embedding
Specific usage cases
Queries with filtering
More pgvector and Supabase resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_stat_statements: SQL Planning and Execution Statistics
pg_stat_statements: Query Performance Monitoring

pg_stat_statements is a database extension that exposes a view, of the same name, to track statistics about SQL statements executed on the database. The following table shows some of the available statistics and metadata:

Column Name	Column Type	Description
userid	oid (references pg_authid.oid)	OID of user who executed the statement
dbid	oid (references pg_database.oid)	OID of database in which the statement was executed
toplevel	bool	True if the query was executed as a top-level statement (always true if pg_stat_statements.track is set to top)
queryid	bigint	Hash code to identify identical normalized queries.
query	text	Text of a representative statement
plans	bigint	Number of times the statement was planned (if pg_stat_statements.track_planning is enabled, otherwise zero)
total_plan_time	double precision	Total time spent planning the statement, in milliseconds (if pg_stat_statements.track_planning is enabled, otherwise zero)
min_plan_time	double precision	Minimum time spent planning the statement, in milliseconds (if pg_stat_statements.track_planning is enabled, otherwise zero)
A full list of statistics is available in the pg_stat_statements docs.

For more information on query optimization, check out the query performance guide.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "pg_stat_statements" and enable the extension.
Inspecting activity#
A common use for pg_stat_statements is to track down expensive or slow queries. The pg_stat_statements view contains a row for each executed query with statistics inlined. For example, you can leverage the statistics to identify frequently executed and slow queries against a given table.

select
	calls,
	mean_exec_time,
	max_exec_time,
	total_exec_time,
	stddev_exec_time,
	query
from
	pg_stat_statements
where
    calls > 50                   -- at least 50 calls
    and mean_exec_time > 2.0     -- averaging at least 2ms/call
    and total_exec_time > 60000  -- at least one minute total server time spent
    and query ilike '%user_in_organization%' -- filter to queries that touch the user_in_organization table
order by
	calls desc
From the results, we can make an informed decision about which queries to optimize or index.

Resources#
Official pg_stat_statements documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Inspecting activity
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pg_repack: Storage Optimization
pg_repack: Physical storage optimization and maintenance

pg_repack is a Postgres extension to remove bloat from tables and indexes, and optionally restore the physical order of clustered indexes. Unlike CLUSTER and VACUUM FULL, pg_repack runs "online" and does not hold a exclusive locks on the processed tables that could prevent ongoing database operations. pg_repack's efficiency is comparable to using CLUSTER directly.

pg_repack provides the following methods to optimize physical storage:

Online CLUSTER: ordering table data by cluster index in a non-blocking way
Ordering table data by specified columns
Online VACUUM FULL: packing rows only in a non-blocking way
Rebuild or relocate only the indexes of a table
pg_repack has 2 components, the database extension and a client-side CLI to control it.

Requirements#
A target table must have a PRIMARY KEY, or a UNIQUE total index on a NOT NULL column.
Performing a full-table repack requires free disk space about twice as large as the target table and its indexes.
pg_repack requires the Postgres superuser role by default. That role is not available to users on the Supabase platform. To avoid that requirement, use the -k or --no-superuser-check flags on every pg_repack CLI command.

The first version of pg_repack with full support for non-superuser repacking is 1.5.2. You can check the version installed on your Supabase instance using

select default_version
from pg_available_extensions
where name = 'pg_repack';
If pg_repack is not present, or the version is < 1.5.2, upgrade to the latest version of Supabase to gain access.

Usage#
Enable the extension#
Get started with pg_repack by enabling the extension in the Supabase Dashboard.


Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "pg_repack" and enable the extension.
Install the CLI#
Select an option from the pg_repack docs to install the client CLI.

Syntax#
All pg_repack commands should include the -k flag to skip the client-side superuser check.

pg_repack -k [OPTION]... [DBNAME]
Example#
Perform an online VACUUM FULL on the tables public.foo and public.bar in the database postgres:

pg_repack -k -h db.<PROJECT_REF>.supabase.co -p 5432 -U postgres -d postgres --no-order --table public.foo --table public.bar
See the official pg_repack documentation for the full list of options.

Limitations#
pg_repack cannot reorganize temporary tables.
pg_repack cannot cluster tables by GiST indexes.
You cannot perform DDL commands of the target tables except VACUUM or ANALYZE while pg_repack is working. pg_repack holds an ACCESS SHARE lock on the target table to enforce this restriction.
Resources#
Official pg_repack documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Requirements
Usage
Enable the extension
Install the CLI
Syntax
Example
Limitations
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
PostGIS: Geo queries
PostGIS: Geo queries

PostGIS is a Postgres extension that allows you to interact with Geo data within Postgres. You can sort your data by geographic location, get data within certain geographic boundaries, and do much more with it.

Overview#
While you may be able to store simple lat/long geographic coordinates as a set of decimals, it does not scale very well when you try to query through a large data set. PostGIS comes with special data types that are efficient, and indexable for high scalability.

The additional data types that PostGIS provides include Point, Polygon, LineString, and many more to represent different types of geographical data. In this guide, we will mainly focus on how to interact with Point type, which represents a single set of latitude and longitude. If you are interested in digging deeper, you can learn more about different data types on the data management section of PostGIS docs.

Enable the extension#
You can get started with PostGIS by enabling the PostGIS extension in your Supabase dashboard.


Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for postgis and enable the extension.
In the confirmation prompt select "Create a new schema" and name it gis for example.
Examples#
Now that we are ready to get started with PostGIS, let’s create a table and see how we can utilize PostGIS for some typical use cases. Let’s imagine we are creating a simple restaurant-searching app.

Let’s create our table. Each row represents a restaurant with its location stored in location column as a Point type.

create table if not exists public.restaurants (
	id int generated by default as identity primary key,
	name text not null,
	location extensions.geography(POINT) not null
);
We can then set a spatial index on the location column of this table.

create index restaurants_geo_index
  on public.restaurants
  using GIST (location);
Inserting data#
You can insert geographical data through SQL or through our API.


Data

SQL

JavaScript

Dart

Swift

Kotlin
Restaurants
id	name	location
1	Supa Burger	lat: 40.807416, long: -73.946823
2	Supa Pizza	lat: 40.807475, long: -73.94581
3	Supa Taco	lat: 40.80629, long: -73.945826
Notice the order in which you pass the latitude and longitude. Longitude comes first, and is because longitude represents the x-axis of the location. Another thing to watch for is when inserting data from the client library, there is no comma between the two values, just a single space.

At this point, if you go into your Supabase dashboard and look at the data, you will notice that the value of the location column looks something like this.

0101000020E6100000A4DFBE0E9C91614044FAEDEBC0494240
We can query the restaurants table directly, but it will return the location column in the format you see above.
We will create database functions so that we can use the st_y() and st_x() function to convert it back to lat and long floating values.

Order by distance#
Sorting datasets from closest to farthest, sometimes called nearest-neighbor sort, is a very common use case in Geo-queries. PostGIS can handle it with the use of the <-> operator. <-> operator returns the two-dimensional distance between two geometries and will utilize the spatial index when used within order by clause. You can create the following database function to sort the restaurants from closest to farthest by passing the current locations as parameters.

create or replace function nearby_restaurants(lat float, long float)
returns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float, dist_meters float)
set search_path = ''
language sql
as $$
  select id, name, extensions.st_y(location::extensions.geometry) as lat, extensions.st_x(location::extensions.geometry) as long, extensions.st_distance(location, extensions.st_point(long, lat)::extensions.geography) as dist_meters
  from public.restaurants
  order by location operator(extensions.<->) extensions.st_point(long, lat)::extensions.geography;
$$;
Now you can call this function from your client using rpc() like this:


JavaScript

Dart

Swift

Kotlin

Result
const { data, error } = await supabase.rpc('nearby_restaurants', {
  lat: 40.807313,
  long: -73.946713,
})
Finding all data points within a bounding box#
Searching within a bounding box of a map

When you are working on a map-based application where the user scrolls through your map, you might want to load the data that lies within the bounding box of the map every time your users scroll. PostGIS can return the rows that are within the bounding box just by supplying the bottom left and the top right coordinates. Let’s look at what the function would look like:

create or replace function restaurants_in_view(min_lat float, min_long float, max_lat float, max_long float)
returns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float)
set search_path to ''
language sql
as $$
	select id, name, extensions.st_y(location::extensions.geometry) as lat, extensions.st_x(location::extensions.geometry) as long
	from public.restaurants
	where location operator(extensions.&&) extensions.ST_SetSRID(extensions.ST_MakeBox2D(extensions.ST_Point(min_long, min_lat), extensions.ST_Point(max_long, max_lat)), 4326)
$$;
The && operator used in the where statement here returns a boolean of whether the bounding box of the two geometries intersect or not. We are basically creating a bounding box from the two points and finding those points that fall under the bounding box. We are also utilizing a few different PostGIS functions:

ST_MakeBox2D: Creates a 2-dimensional box from two points.
ST_SetSRID: Sets the SRID, which is an identifier of what coordinate system to use for the geometry. 4326 is the standard longitude and latitude coordinate system.
You can call this function from your client using rpc() like this:


JavaScript

Dart

Swift

Kotlin

Result
const { data, error } = await supabase.rpc('restaurants_in_view', {
  min_lat: 40.807,
  min_long: -73.946,
  max_lat: 40.808,
  max_long: -73.945,
})
Troubleshooting#
As of PostGIS 2.3 or newer, the PostGIS extension is no longer relocatable from one schema to another. If you need to move it from one schema to another for any reason (e.g. from the public schema to the extensions schema for security reasons), you would normally run a ALTER EXTENSION to relocate the schema. However, you will now to do the following steps:

Backup your Database to prevent data loss - You can do this through the CLI or Postgres backup tools such as pg_dumpall

Drop all dependencies you created and the PostGIS extension - DROP EXTENSION postgis CASCADE;

Enable PostGIS extension in the new schema - CREATE EXTENSION postgis SCHEMA extensions;

Restore dropped data via the Backup if necessary from step 1 with your tool of choice.

Alternatively, you can contact the Supabase Support Team and ask them to run the following SQL on your instance:

BEGIN;
	UPDATE pg_extension
	  SET extrelocatable = true
	WHERE extname = 'postgis';
	ALTER EXTENSION postgis
	  SET SCHEMA extensions;
	ALTER EXTENSION postgis
	  UPDATE TO "<POSTGIS_VERSION>next";
	ALTER EXTENSION postgis UPDATE;
	UPDATE pg_extension
	  SET extrelocatable = false
	WHERE extname = 'postgis';
COMMIT;
Resources#
Official PostGIS documentation
Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Overview
Enable the extension
Examples
Inserting data
Order by distance
Finding all data points within a bounding box
Troubleshooting
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pgmq: Queues
pgmq: Queues

See the Supabase Queues docs.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pgsodium (pending deprecation): Encryption Features
pgsodium (pending deprecation): Encryption Features

Supabase DOES NOT RECOMMEND any new usage of pgsodium.

The pgsodium extension is expected to go through a deprecation cycle in the near future. We will reach out to owners of impacted projects to assist with migrations away from pgsodium once the deprecation process begins.

The Vault extension won’t be impacted. Its internal implementation will shift away from pgsodium, but the interface and API will remain unchanged.

pgsodium is a Postgres extension which provides SQL access to libsodium's high-level cryptographic algorithms.

Supabase previously documented two features derived from pgsodium. Namely Server Key Management and Transparent Column Encryption. At this time, we do not recommend using either on the Supabase platform due to their high level of operational complexity and misconfiguration risk.

Note that Supabase projects are encrypted at rest by default which likely is sufficient for your compliance needs e.g. SOC2 & HIPAA.

Get the root encryption key for your Supabase project#
Encryption requires keys. Keeping the keys in the same database as the encrypted data would be unsafe. For more information about managing the pgsodium root encryption key on your Supabase project see encryption key location. This key is required to decrypt values stored in Supabase Vault and data encrypted with Transparent Column Encryption.

Resources#
Supabase Vault
Read more about Supabase Vault in the blog post
Supabase Vault on GitHub
Resources#
Official pgsodium documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Get the root encryption key for your Supabase project
Resources
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
pgTAP: Unit Testing
pgTAP: Unit Testing

pgTAP is a unit testing extension for Postgres.

Overview#
Let's cover some basic concepts:

Unit tests: allow you to test small parts of a system (like a database table!).
TAP: stands for Test Anything Protocol. It is an framework which aims to simplify the error reporting during testing.
Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for pgtap and enable the extension.
Testing tables#
begin;
select plan( 1 );
select has_table( 'profiles' );
select * from finish();
rollback;
API:

has_table(): Tests whether or not a table exists in the database
has_index(): Checks for the existence of a named index associated with the named table.
has_relation(): Tests whether or not a relation exists in the database.
Testing columns#
begin;
select plan( 2 );
select has_column( 'profiles', 'id' ); -- test that the "id" column exists in the "profiles" table
select col_is_pk( 'profiles', 'id' ); -- test that the "id" column is a primary key
select * from finish();
rollback;
API:

has_column(): Tests whether or not a column exists in a given table, view, materialized view or composite type.
col_is_pk(): Tests whether the specified column or columns in a table is/are the primary key for that table.
Testing RLS policies#
begin;
select plan( 1 );
select policies_are(
  'public',
  'profiles',
  ARRAY [
    'Profiles are public', -- Test that there is a policy called  "Profiles are public" on the "profiles" table.
    'Profiles can only be updated by the owner'  -- Test that there is a policy called  "Profiles can only be updated by the owner" on the "profiles" table.
  ]
);
select * from finish();
rollback;
API:

policies_are(): Tests that all of the policies on the named table are only the policies that should be on that table.
policy_roles_are(): Tests whether the roles to which policy applies are only the roles that should be on that policy.
policy_cmd_is(): Tests whether the command to which policy applies is same as command that is given in function arguments.
You can also use the results_eq() method to test that a Policy returns the correct data:

begin;
select plan( 1 );
select results_eq(
    'select * from profiles()',
    $$VALUES ( 1, 'Anna'), (2, 'Bruce'), (3, 'Caryn')$$,
    'profiles() should return all users'
);
select * from finish();
rollback;
API:

results_eq()
results_ne()
Testing functions#
prepare hello_expr as select 'hello'
begin;
select plan(3);
-- You'll need to create a hello_world and is_even function
select function_returns( 'hello_world', 'text' );                   -- test if the function "hello_world" returns text
select function_returns( 'is_even', ARRAY['integer'], 'boolean' );  -- test if the function "is_even" returns a boolean
select results_eq('select * from hello_world()', 'hello_expr');          -- test if the function "hello_world" returns "hello"
select * from finish();
rollback;
API:

function_returns(): Tests that a particular function returns a particular data type
is_definer(): Tests that a function is a security definer (that is, a setuid function).
Resources#
Official pgTAP documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Enable the extension
Testing tables
Testing columns
Testing RLS policies
Testing functions
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
plpgsql_check: PL/pgSQL Linter
plpgsql_check: PL/pgSQL Linter

plpgsql_check is a Postgres extension that lints plpgsql for syntax, semantic and other related issues. The tool helps developers to identify and correct errors before executing the code. plpgsql_check is most useful for developers who are working with large or complex SQL codebases, as it can help identify and resolve issues early in the development cycle.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "plpgsql_check" and enable the extension.
API#
plpgsql_check_function( ... ): Scans a function for errors.
plpgsql_check_function is highly customizable. For a complete list of available arguments see the docs

Usage#
To demonstrate plpgsql_check we can create a function with a known error. In this case we create a function some_func, that references a non-existent column place.created_at.

create table place(
  x float,
  y float
);
create or replace function public.some_func()
  returns void
  language plpgsql
as $$
declare
  rec record;
begin
  for rec in select * from place
  loop
    -- Bug: There is no column `created_at` on table `place`
    raise notice '%', rec.created_at;
  end loop;
end;
$$;
Note that executing the function would not catch the invalid reference error because the loop does not execute if no rows are present in the table.

select public.some_func();
  some_func
 ───────────
 (1 row)
Now we can use plpgsql_check's plpgsql_check_function function to identify the known error.

select plpgsql_check_function('public.some_func()');
                   plpgsql_check_function
------------------------------------------------------------
 error:42703:8:RAISE:record "rec" has no field "created_at"
 Context: SQL expression "rec.created_at"
Resources#
Official plpgsql_check documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
API
Usage
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
timescaledb (deprecated)
timescaledb: Time-Series data

The timescaledb extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See the Upgrading to Postgres 17 notes for more information.

timescaledb is a Postgres extension designed for improved handling of time-series data. It provides a scalable, high-performance solution for storing and querying time-series data on top of a standard Postgres database.

timescaledb uses a time-series-aware storage model and indexing techniques to improve performance of Postgres in working with time-series data. The extension divides data into chunks based on time intervals, allowing it to scale efficiently, especially for large data sets. The data is then compressed, optimized for write-heavy workloads, and partitioned for parallel processing. timescaledb also includes a set of functions, operators, and indexes that work with time-series data to reduce query times, and make data easier to work with.

Supabase projects come with TimescaleDB Apache 2 Edition. Functionality only available under the Community Edition is not available.

Enable the extension#

Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for timescaledb and enable the extension.
Even though the SQL code is create extension, this is the equivalent of "enabling the extension". To disable an extension you can call drop extension.

It's good practice to create the extension within a separate schema (like extensions) to keep your public schema clean.

Usage#
To demonstrate how timescaledb works, let's consider a simple example where we have a table that stores temperature data from different sensors. We will create a table named "temperatures" and store data for two sensors.

First we create a hypertable, which is a virtual table that is partitioned into chunks based on time intervals. The hypertable acts as a proxy for the actual table and makes it easy to query and manage time-series data.

create table temperatures (
  time timestamptz not null,
  sensor_id int not null,
  temperature double precision not null
);
select create_hypertable('temperatures', 'time');
Next, we can populate some values

insert into temperatures (time, sensor_id, temperature)
values
    ('2023-02-14 09:00:00', 1, 23.5),
    ('2023-02-14 09:00:00', 2, 21.2),
    ('2023-02-14 09:05:00', 1, 24.5),
    ('2023-02-14 09:05:00', 2, 22.3),
    ('2023-02-14 09:10:00', 1, 25.1),
    ('2023-02-14 09:10:00', 2, 23.9),
    ('2023-02-14 09:15:00', 1, 24.9),
    ('2023-02-14 09:15:00', 2, 22.7),
    ('2023-02-14 09:20:00', 1, 24.7),
    ('2023-02-14 09:20:00', 2, 23.5);
And finally we can query the table using timescaledb's time_bucket function to divide the time-series into intervals of the specified size (in this case, 1 hour) averaging the temperature reading within each group.

select
    time_bucket('1 hour', time) AS hour,
    avg(temperature) AS average_temperature
from
    temperatures
where
    sensor_id = 1
    and time > NOW() - interval '1 hour'
group by
    hour;
Resources#
Official timescaledb documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enable the extension
Usage
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
uuid-ossp: Unique Identifiers
uuid-ossp: Unique Identifiers

The uuid-ossp extension can be used to generate a UUID.

Overview#
A UUID is a "Universally Unique Identifier" and it is, for practical purposes, unique.
This makes them particularly well suited as Primary Keys. It is occasionally referred to as a GUID, which stands for "Globally Unique Identifier".

Enable the extension#
Note:
Currently uuid-ossp extension is enabled by default and cannot be disabled.


Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for uuid-ossp and enable the extension.
The uuid type#
Once the extension is enabled, you now have access to a uuid type.

uuid_generate_v1()#
Creates a UUID value based on the combination of computer’s MAC address, current timestamp, and a random value.

UUIDv1 leaks identifiable details, which might make it unsuitable for certain security-sensitive applications.

uuid_generate_v4()#
Creates UUID values based solely on random numbers. You can also use Postgres's built-in gen_random_uuid() function to generate a UUIDv4.

Examples#
Within a query#
select uuid_generate_v4();
As a primary key#
Automatically create a unique, random ID in a table:

create table contacts (
  id uuid default uuid_generate_v4(),
  first_name text,
  last_name text,
  primary key (id)
);
Resources#
Choosing a Postgres Primary Key
The Basics Of Postgres UUID Data Type
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Enable the extension
The uuid type
uuid_generate_v1()
uuid_generate_v4()
Examples
Within a query
As a primary key
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Extensions
RUM: inverted index for full-text search
RUM: improved inverted index for full-text search based on GIN index

RUM is an extension which adds a RUM index to Postgres.

RUM index is based on GIN that stores additional per-entry information in a posting tree. For example, positional information of lexemes or timestamps. In comparison to GIN it can use this information to make faster index-only scans for:

Phrase search
Text search with ranking by text distance operator
Text SELECTs with ordering by some non-indexed additional column e.g. by timestamp.
RUM works best in scenarios when the possible keys are highly repeatable. I.e. all texts are composed of a
limited amount of words, so per-lexeme indexing gives significant speed-up in searching texts containing word
combinations or phrases.

Main operators for ordering are:

tsvector <=> tsquery | float4 | Distance between tsvector and tsquery.
value <=> value | float8 | Distance between two values.

Where value is timestamp, timestamptz, int2, int4, int8, float4, float8, money and oid

Usage#
Enable the extension#
You can get started with rum by enabling the extension in your Supabase dashboard.


Dashboard

SQL
Go to the Database page in the Dashboard.
Click on Extensions in the sidebar.
Search for "rum" and enable the extension.
Syntax#
For type: tsvector#
To understand the following you may need first to see Official Postgres documentation on text
search

rum_tsvector_ops

CREATE TABLE test_rum(t text, a tsvector);
CREATE TRIGGER tsvectorupdate
BEFORE UPDATE OR INSERT ON test_rum
FOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger('a', 'pg_catalog.english', 't');
INSERT INTO test_rum(t) VALUES ('The situation is most beautiful');
INSERT INTO test_rum(t) VALUES ('It is a beautiful');
INSERT INTO test_rum(t) VALUES ('It looks like a beautiful place');
CREATE INDEX rumidx ON test_rum USING rum (a rum_tsvector_ops);
And we can execute tsvector selects with ordering by text distance operator:

SELECT t, a `<=>` to_tsquery('english', 'beautiful | place') AS rank
    FROM test_rum
    WHERE a @@ to_tsquery('english', 'beautiful | place')
    ORDER BY a `<=>` to_tsquery('english', 'beautiful | place');
                t                |  rank
---------------------------------+---------
 It looks like a beautiful place | 8.22467
 The situation is most beautiful | 16.4493
 It is a beautiful               | 16.4493
(3 rows)
rum_tsvector_addon_ops

CREATE TABLE tsts (id int, t tsvector, d timestamp);
CREATE INDEX tsts_idx ON tsts USING rum (t rum_tsvector_addon_ops, d)
    WITH (attach = 'd', to = 't');
Now we can execute the selects with ordering distance operator on attached column:

SELECT id, d, d `<=>` '2016-05-16 14:21:25' FROM tsts WHERE t @@ 'wr&qh' ORDER BY d `<=>` '2016-05-16 14:21:25' LIMIT 5;
 id  |                d                |   ?column?
-----+---------------------------------+---------------
 355 | Mon May 16 14:21:22.326724 2016 |      2.673276
 354 | Mon May 16 13:21:22.326724 2016 |   3602.673276
 371 | Tue May 17 06:21:22.326724 2016 |  57597.326724
 406 | Wed May 18 17:21:22.326724 2016 | 183597.326724
 415 | Thu May 19 02:21:22.326724 2016 | 215997.326724
(5 rows)
For type: anyarray#
rum_anyarray_ops

This operator class stores anyarray elements with length of the array. It supports operators &&, @>, <@, =, % operators. It also supports ordering by <=> operator.

CREATE TABLE test_array (i int2[]);
INSERT INTO test_array VALUES ('{}'), ('{0}'), ('{1,2,3,4}'), ('{1,2,3}'), ('{1,2}'), ('{1}');
CREATE INDEX idx_array ON test_array USING rum (i rum_anyarray_ops);
Now we can execute the query using index scan:

SELECT * FROM test_array WHERE i && '{1}' ORDER BY i `<=>` '{1}' ASC;
     i
-----------
 {1}
 {1,2}
 {1,2,3}
 {1,2,3,4}
(4 rows)
rum_anyarray_addon_ops

The does the same with anyarray index as rum_tsvector_addon_ops i.e. allows to order select results using distance
operator by attached column.

Limitations#
RUM has slower build and insert times than GIN due to:

It is bigger due to the additional attributes stored in the index.
It uses generic WAL records.
Resources#
Official RUM documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Usage
Enable the extension
Syntax
Limitations
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Foreign Data Wrappers
Overview
Foreign Data Wrappers

Connecting to external systems using Postgres Foreign Data Wrappers.

Foreign Data Wrappers (FDW) are a core feature of Postgres that allow you to access and query data stored in external data sources as if they were native Postgres tables.

Postgres includes several built-in foreign data wrappers, such as postgres_fdw for accessing other Postgres databases, and file_fdw for reading data from files. Supabase extends this feature to query other databases or any other external systems. We do this with our open source Wrappers framework. In these guides we'll refer to them as "Wrappers", Foreign Data Wrappers, or FDWs. They are conceptually the same thing.

Concepts#
Wrappers introduce some new terminology and different workflows.

Foreign Data Wrappers (FDW)
Remote servers#
A Remote Server is an external database, API, or any system containing data that you want to query from your Postgres database. Examples include:

An external database, like Postgres or Firebase.
A remote data warehouse, like ClickHouse, BigQuery, or Snowflake.
An API, like Stripe or GitHub.
It's possible to connect to multiple remote servers of the same type. For example, you can connect to two different Firebase projects within the same Supabase database.

Foreign tables#
A table in your database which maps to some data inside a Remote Server.

Examples:

An analytics table which maps to a table inside your data warehouse.
A subscriptions table which maps to your Stripe subscriptions.
A collections table which maps to a Firebase collection.
Although a foreign table behaves like any other table, the data is not stored inside your database. The data remains inside the Remote Server.

ETL with Wrappers#
ETL stands for Extract, Transform, Load. It's an established process for moving data from one system to another. For example, it's common to move data from a production database to a data warehouse.

There are many popular ETL tools, such as Fivetran and Airbyte.

Wrappers provide an alternative to these tools. You can use SQL to move data from one table to another:

-- Copy data from your production database to your
-- data warehouse for the last 24 hours:
insert into warehouse.analytics
select * from public.analytics
where ts > (now() - interval '1 DAY');
This approach provides several benefits:

Simplicity: the Wrappers API is just SQL, so data engineers don't need to learn new tools and languages.
Save on time: avoid setting up additional data pipelines.
Save on Data Engineering costs: less infrastructure to be managed.
One disadvantage is that Wrappers are not as feature-rich as ETL tools. They also couple the ETL process to your database.

On-demand ETL with Wrappers#
Supabase extends the ETL concept with real-time data access. Instead of moving gigabytes of data from one system to another before you can query it, you can instead query the data directly from the remote server. This additional option, "Query", extends the ETL process and is called QETL (pronounced "kettle"): Query, Extract, Transform, Load.

-- Get all purchases for a user from your data warehouse:
select
  auth.users.id as user_id,
  warehouse.orders.id as order_id
from
  warehouse.orders
join 
  auth.users on auth.users.id = warehouse.orders.user_id
where 
  auth.users.id = '<some_user_id>';
This approach has several benefits:

On-demand: analytical data is immediately available within your application with no additional infrastructure.
Always in sync: since the data is queried directly from the remote server, it's always up-to-date.
Integrated: large datasets are available within your application, and can be joined with your operational/transactional data.
Save on egress: only extract/load what you need.
Batch ETL with Wrappers#
A common use case for Wrappers is to extract data from a production database and load it into a data warehouse. This can be done within your database using pg_cron. For example, you can schedule a job to run every night to extract data from your production database and load it into your data warehouse.

-- Every day at 3am, copy data from your
-- production database to your data warehouse:
select cron.schedule(
  'nightly-etl',
  '0 3 * * *',
  $$
    insert into warehouse.analytics
    select * from public.analytics
    where ts > (now() - interval '1 DAY');
  $$
);
FDW with pg_cron
This process can be taxing on your database if you are moving large amounts of data. Often, it's better to use an external tool for batch ETL, such as Fivetran or Airbyte.

WebAssembly Wrappers#
WebAssembly (Wasm) is a binary instruction format that enables high-performance execution of code on the web. Wrappers now includes a Wasm runtime, which provides a sandboxed execution environment, to run Wasm foreign data wrappers. Combined Wrappers with Wasm, developing and distributing new FDW becomes much easier and you can even build your own Wasm FDW and use it on Supabase platform.

To learn more about Wasm FDW, visit Wrappers official documentation.

Security#
Foreign Data Wrappers do not provide Row Level Security, thus it is not advised to expose them via your API. Wrappers should always be stored in a private schema. For example, if you are connecting to your Stripe account, you should create a stripe schema to store all of your foreign tables inside. This schema should not be added to the “Additional Schemas” setting in the API section.

If you want to expose any of the foreign table columns to your public API, you can create a Database Function with security definer in the public schema, and then you can interact with your foreign table through API. For better access control, the function should have appropriate filters on the foreign table to apply security rules based on your business needs.

As an example, go to SQL Editor and then follow below steps,

Create a Stripe Products foreign table:

create foreign table stripe.stripe_products (
  id text,
  name text,
  active bool,
  default_price text,
  description text,
  created timestamp,
  updated timestamp,
  attrs jsonb
)
  server stripe_fdw_server
  options (
    object 'products',
    rowid_column 'id'
  );
Create a security definer function that queries the foreign table and filters on the name prefix parameter:

create function public.get_stripe_products(name_prefix text)
returns table (
  id text,
  name text,
  active boolean,
  default_price text,
  description text
)
language plpgsql
security definer set search_path = ''
as $$
begin
  return query
  select
    t.id,
    t.name,
    t.active,
    t.default_price,
    t.description
  from
    stripe.stripe_products t
  where
    t.name like name_prefix || '%'
  ;
end;
$$;
Restrict the function execution to a specific role only, for example, the authenticated users:

By default, the function created can be executed by any roles like anon, that means the
foreign table is public accessible. Always limit the function execution permission to
appropriate roles.

-- revoke public execute permission
revoke execute on function public.get_stripe_products from public;
revoke execute on function public.get_stripe_products from anon;
-- grant execute permission to a specific role only
grant execute on function public.get_stripe_products to authenticated;
Once the preceding steps are finished, the function can be invoked from Supabase client to query the foreign table:

const { data, error } = await supabase
  .rpc('get_stripe_products', { name_prefix: 'Test' })
  .select('*')
if (error) console.error(error)
else console.log(data)
Resources#
Official supabase/wrappers documentation
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Concepts
Remote servers
Foreign tables
ETL with Wrappers
On-demand ETL with Wrappers
Batch ETL with Wrappers
WebAssembly Wrappers
Security
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Foreign Data Wrappers
Connecting to Auth0
Auth0

You can enable the Auth0 wrapper right from the Supabase dashboard.

Open wrapper in dashboard
Auth0 is a flexible, drop-in solution to add authentication and authorization services to your applications

The Auth0 Wrapper allows you to read data from your Auth0 tenant for use within your Postgres database.

Preparation#
Before you can query Auth0, you need to enable the Wrappers extension and store your credentials in Postgres.

Enable Wrappers#
Make sure the wrappers extension is installed on your database:

create extension if not exists wrappers with schema extensions;
Enable the Auth0 Wrapper#
Enable the auth0_wrapper FDW:

create foreign data wrapper auth0_wrapper
  handler auth0_fdw_handler
  validator auth0_fdw_validator;
Store your credentials (optional)#
By default, Postgres stores FDW credentials inside pg_catalog.pg_foreign_server in plain text. Anyone with access to this table will be able to view these credentials. Wrappers is designed to work with Vault, which provides an additional level of security for storing credentials. We recommend using Vault to store your credentials.

-- Save your Auth0 API key in Vault and retrieve the created `key_id`
select vault.create_secret(
  '<Auth0 API Key or PAT>', -- Auth0 API key or Personal Access Token (PAT)
  'auth0',
  'Auth0 API key for Wrappers'
);
Connecting to Auth0#
We need to provide Postgres with the credentials to connect to Auth0, and any additional options. We can do this using the create server command:


With Vault

Without Vault
create server auth0_server
  foreign data wrapper auth0_wrapper
  options (
    url 'https://dev-<tenant-id>.us.auth0.com/api/v2/users',
    api_key_id '<key_ID>' -- The Key ID from above.
  );
Create a schema#
We recommend creating a schema to hold all the foreign tables:

create schema if not exists auth0;
Entities#
The Auth0 Wrapper supports data reads from Auth0 API.

Users#
The Auth0 Wrapper supports data reads from Auth0's Management API List users endpoint endpoint (read only).

Operations#
Object	Select	Insert	Update	Delete	Truncate
Users	✅	❌	❌	❌	❌
Usage#
create foreign table auth0.my_foreign_table (
  name text
  -- other fields
)
server auth0_server
options (
  object 'users'
);
Notes#
Currently only supports the users object
Query Pushdown Support#
This FDW doesn't support query pushdown.

Limitations#
This section describes important limitations and considerations when using this FDW:

No query pushdown support, all filtering must be done locally
Large result sets may experience slower performance due to full data transfer requirement
Only supports the users object from Auth0 Management API
Cannot modify Auth0 user properties via FDW
Materialized views using these foreign tables may fail during logical backups
Examples#
Basic Auth0 Users Query#
This example demonstrates querying Auth0 users data.

create foreign table auth0.auth0_table (
  created_at text,
  email text,
  email_verified bool,
  identities jsonb
)
  server auth0_server
  options (
    object 'users'
  );
You can now fetch your Auth0 data from within your Postgres database:

select * from auth0.auth0_table;
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Preparation
Enable Wrappers
Enable the Auth0 Wrapper
Store your credentials (optional)
Connecting to Auth0
Create a schema
Entities
Users
Query Pushdown Support
Limitations
Examples
Basic Auth0 Users Query
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Foreign Data Wrappers
Connecting to Airtable
Airtable

You can enable the Airtable wrapper right from the Supabase dashboard.

Open wrapper in dashboard
Airtable is an easy-to-use online platform for creating and sharing relational databases.

The Airtable Wrapper allows you to read data from your Airtable bases/tables within your Postgres database.

Preparation#
Before you can query Airtable, you need to enable the Wrappers extension and store your credentials in Postgres.

Enable Wrappers#
Make sure the wrappers extension is installed on your database:

create extension if not exists wrappers with schema extensions;
Enable the Airtable Wrapper#
Enable the airtable_wrapper FDW:

create foreign data wrapper airtable_wrapper
  handler airtable_fdw_handler
  validator airtable_fdw_validator;
Store your credentials (optional)#
By default, Postgres stores FDW credentials inside pg_catalog.pg_foreign_server in plain text. Anyone with access to this table will be able to view these credentials. Wrappers is designed to work with Vault, which provides an additional level of security for storing credentials. We recommend using Vault to store your credentials.

Get your token from Airtable's developer portal.

-- Save your Airtable API key in Vault and retrieve the created `key_id`
select vault.create_secret(
  '<Airtable API Key or PAT>', -- Airtable API key or Personal Access Token (PAT)
  'airtable',
  'Airtable API key for Wrappers'
);
Connecting to Airtable#
We need to provide Postgres with the credentials to connect to Airtable, and any additional options. We can do this using the create server command:


With Vault

Without Vault
create server airtable_server
  foreign data wrapper airtable_wrapper
  options (
    api_key_id '<key_ID>' -- The Key ID from above.
  );
Create a schema#
We recommend creating a schema to hold all the foreign tables:

create schema if not exists airtable;
Entities#
The Airtable Wrapper supports data reads from the Airtable API.

Records#
The Airtable Wrapper supports data reads from Airtable's Records endpoint (read only).

Operations#
Object	Select	Insert	Update	Delete	Truncate
Records	✅	❌	❌	❌	❌
Usage#
Get your base ID and table ID from your table's URL.

airtable_credentials

Foreign tables must be lowercase, regardless of capitalization in Airtable.

create foreign table airtable.my_foreign_table (
  message text
  -- other fields
)
server airtable_server
options (
  base_id 'appXXXX',
  table_id 'tblXXXX'
);
Notes#
The table requires both base_id and table_id options
Optional view_id can be specified to query a specific view
Query Pushdown Support#
This FDW doesn't support query pushdown.

Supported Data Types#
Postgres Data Type	Airtable Data Type
boolean	Checkbox
smallint	Number
integer	Number
bigint	Autonumber
bigint	Number
real	Number
double precision	Number
numeric	Number
numeric	Currency
numeric	Percent
text	Single line text
text	Long text
text	Single select
text	Phone number
text	Email
text	URL
date	Date
timestamp	Created time
timestamp	Last modified time
jsonb	Multiple select
jsonb	Created by
jsonb	Last modified by
jsonb	User
Limitations#
This section describes important limitations and considerations when using this FDW:

No query pushdown support, all filtering must be done locally
Large result sets may experience slower performance due to full data transfer requirement
No support for Airtable formulas or computed fields
Views must be pre-configured in Airtable
No support for Airtable's block features
Materialized views using these foreign tables may fail during logical backups
Examples#
Query an Airtable table#
This will create a "foreign table" inside your Postgres database called airtable_table:

create foreign table airtable.airtable_table (
  name text,
  notes text,
  content text,
  amount numeric,
  updated_at timestamp
)
server airtable_server
options (
  base_id 'appTc3yI68KN6ukZc',
  table_id 'tbltiLinE56l3YKfn'
);
You can now fetch your Airtable data from within your Postgres database:

select * from airtable.airtable_table;
Query an Airtable view#
We can also create a foreign table from an Airtable View called airtable_view:

create foreign table airtable.airtable_view (
  name text,
  notes text,
  content text,
  amount numeric,
  updated_at timestamp
)
server airtable_server
options (
  base_id 'appTc3yI68KN6ukZc',
  table_id 'tbltiLinE56l3YKfn',
  view_id 'viwY8si0zcEzw3ntZ'
);
You can now fetch your Airtable data from within your Postgres database:

select * from airtable.airtable_view;
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Preparation
Enable Wrappers
Enable the Airtable Wrapper
Store your credentials (optional)
Connecting to Airtable
Create a schema
Entities
Records
Query Pushdown Support
Supported Data Types
Limitations
Examples
Query an Airtable table
Query an Airtable view
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Database
Foreign Data Wrappers
Connecting to AWS Cognito
AWS Cognito

You can enable the AWS Cognito wrapper right from the Supabase dashboard.

Open wrapper in dashboard
AWS Cognito is an identity platform for web and mobile apps.

The Cognito wrapper allows you to read data from your Cognito Userpool within your Postgres database.

Preparation#
Before you can query AWS Cognito, you need to enable the Wrappers extension and store your credentials in Postgres.

Enable Wrappers#
Make sure the wrappers extension is installed on your database:

create extension if not exists wrappers with schema extensions;
Enable the Cognito Wrapper#
Enable the cognito_wrapper FDW:

create foreign data wrapper cognito_wrapper
  handler cognito_fdw_handler
  validator cognito_fdw_validator;
Store your credentials (optional)#
By default, Postgres stores FDW credentials inside pg_catalog.pg_foreign_server in plain text. Anyone with access to this table will be able to view these credentials. Wrappers are designed to work with Vault, which provides an additional level of security for storing credentials. We recommend using Vault to store your credentials.

-- Save your Cognito secret access key in Vault and retrieve the created `key_id`
select vault.create_secret(
  '<secret access key>',
  'cognito',
  'Cognito secret key for Wrappers'
);
Connecting to Cognito#
We need to provide Postgres with the credentials to connect to Cognito, and any additional options. We can do this using the create server command:


With Vault

Without Vault
create server cognito_server
  foreign data wrapper cognito_wrapper
  options (
    aws_access_key_id '<your_access_key>',
    api_key_id '<your_secret_key_id_in_vault>',
    region '<your_aws_region>',
    user_pool_id '<your_user_pool_id>'
  );
Create a schema#
We recommend creating a schema to hold all the foreign tables:

create schema if not exists cognito;
Entities#
We can use SQL import foreign schema to import foreign table definitions from Cognito.

For example, using below SQL can automatically create foreign table in the cognito schema.

import foreign schema cognito from server cognito_server into cognito;
The foreign table will be created as below:

Users#
This is an object representing Cognito User Records.

Ref: AWS Cognito User Records

Operations#
Object	Select	Insert	Update	Delete	Truncate
Users	✅	❌	❌	❌	❌
Usage#
create foreign table cognito.users (
  username text,
  email text,
  status text,
  enabled boolean,
  created_at timestamp,
  updated_at timestamp,
  attributes jsonb
)
server cognito_server
options (
  object 'users'
);
Notes#
Only the columns listed above are accepted in the foreign table
The attributes column contains additional user attributes in JSON format
Query Pushdown Support#
This FDW doesn't support query pushdown.

Limitations#
This section describes important limitations and considerations when using this FDW:

No query pushdown support, all filtering must be done locally
Large result sets may experience slower performance due to full data transfer requirement
Only supports User Pool objects from Cognito API
No support for Identity Pool operations
Materialized views using these foreign tables may fail during logical backups
Examples#
Basic example#
This will create a "foreign table" inside your Postgres database called cognito_table:

create foreign table cognito.users (
  username text,
  email text,
  status text,
  enabled boolean,
  created_at timestamp,
  updated_at timestamp,
  attributes jsonb
)
server cognito_server
options (
   object 'users'
);
You can now fetch your Cognito data from within your Postgres database:

select * from cognito.users;
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Preparation
Enable Wrappers
Enable the Cognito Wrapper
Store your credentials (optional)
Connecting to Cognito
Create a schema
Entities
Users
Query Pushdown Support
Limitations
Examples
Basic example
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth
Database
Foreign Data Wrappers
Connecting to Stripe
Stripe

You can enable the Stripe wrapper right from the Supabase dashboard.

Open wrapper in dashboard
Stripe is an API driven online payment processing utility.

The Stripe Wrapper allows you to read data from Stripe within your Postgres database.

Preparation#
Before you can query Stripe, you need to enable the Wrappers extension and store your credentials in Postgres.

Enable Wrappers#
Make sure the wrappers extension is installed on your database:

create extension if not exists wrappers with schema extensions;
Enable the Stripe Wrapper#
Enable the stripe_wrapper FDW:

create foreign data wrapper stripe_wrapper
  handler stripe_fdw_handler
  validator stripe_fdw_validator;
Store your credentials (optional)#
By default, Postgres stores FDW credentials inside pg_catalog.pg_foreign_server in plain text. Anyone with access to this table will be able to view these credentials. Wrappers is designed to work with Vault, which provides an additional level of security for storing credentials. We recommend using Vault to store your credentials.

-- Save your Stripe API key in Vault and retrieve the created `key_id`
select vault.create_secret(
  '<Stripe API key>',
  'stripe',
  'Stripe API key for Wrappers'
);
Connecting to Stripe#
We need to provide Postgres with the credentials to connect to Stripe, and any additional options. We can do this using the create server command:


With Vault

Without Vault
create server stripe_server
  foreign data wrapper stripe_wrapper
  options (
    api_key_id '<key_ID>', -- The Key ID from above, required if api_key_name is not specified.
    api_key_name '<key_Name>', -- The Key Name from above, required if api_key_id is not specified.
    api_url 'https://api.stripe.com/v1/',  -- Stripe API base URL, optional. Default is 'https://api.stripe.com/v1/'
    api_version '2024-06-20'  -- Stripe API version, optional. Default is your Stripe account’s default API version.
  );
Create a schema#
We recommend creating a schema to hold all the foreign tables:

create schema if not exists stripe;
Entities#
The Stripe Wrapper supports data read and modify from Stripe API.

Object	Select	Insert	Update	Delete	Truncate
Accounts	✅	❌	❌	❌	❌
Balance	✅	❌	❌	❌	❌
Balance Transactions	✅	❌	❌	❌	❌
Charges	✅	❌	❌	❌	❌
Checkout Sessions	✅	❌	❌	❌	❌
Customers	✅	✅	✅	✅	❌
Disputes	✅	❌	❌	❌	❌
Events	✅	❌	❌	❌	❌
Files	✅	❌	❌	❌	❌
File Links	✅	❌	❌	❌	❌
Invoices	✅	❌	❌	❌	❌
Mandates	✅	❌	❌	❌	❌
Meters	✅	❌	❌	❌	❌
PaymentIntents	✅	❌	❌	❌	❌
Payouts	✅	❌	❌	❌	❌
Prices	✅	❌	❌	❌	❌
Products	✅	✅	✅	✅	❌
Refunds	✅	❌	❌	❌	❌
SetupAttempts	✅	❌	❌	❌	❌
SetupIntents	✅	❌	❌	❌	❌
Subscriptions	✅	✅	✅	✅	❌
Tokens	✅	❌	❌	❌	❌
Topups	✅	❌	❌	❌	❌
Transfers	✅	❌	❌	❌	❌
We can use SQL import foreign schema to import foreign table definitions from Stripe.

For example, using below SQL can automatically create foreign tables in the stripe schema.

-- create all the foreign tables
import foreign schema stripe from server stripe_server into stripe;
-- or, create "checkout_sessions", "customers" and "balance" tables only
import foreign schema stripe
   limit to ("checkout_sessions", "customers", "balance")
   from server stripe_server into stripe;
-- or, create all foreign tables except "checkout_sessions" and "billing_meters"
import foreign schema stripe
   except ("checkout_sessions", "billing_meters")
   from server stripe_server into stripe;
The full list of the foreign tables is below:

Accounts#
This is an object representing a Stripe account.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Accounts	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.accounts (
  id text,
  business_type text,
  country text,
  email text,
  type text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'accounts'
  );
Notes#
While any column is allowed in a where clause, it is most efficient to filter by id
Use the attrs jsonb column to access additional account details
Balance#
This is an object representing your Stripe account's current balance.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Balance	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.balance (
  balance_type text,
  amount bigint,
  currency text,
  attrs jsonb
)
  server stripe_server
  options (
    object 'balance'
  );
Notes#
Balance is a read-only object that shows the current funds in your Stripe account
The balance is broken down by source types (e.g., card, bank account) and currencies
Use the attrs jsonb column to access additional balance details like pending amounts
While any column is allowed in a where clause, filtering options are limited as this is a singleton object
Balance Transactions#
This is an object representing funds moving through your Stripe account. Balance transactions are created for every type of transaction that comes into or flows out of your Stripe account balance.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Balance Transactions	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.balance_transactions (
  id text,
  amount bigint,
  currency text,
  description text,
  fee bigint,
  net bigint,
  status text,
  type text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'balance_transactions'
  );
Notes#
Balance transactions are read-only records of all funds movement in your Stripe account
Each transaction includes amount, currency, fees, and net amount information
Use the attrs jsonb column to access additional transaction details
While any column is allowed in a where clause, it is most efficient to filter by:
id
type
Charges#
This is an object representing a charge on a credit or debit card. You can retrieve and refund individual charges as well as list all charges. Charges are identified by a unique, random ID.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Charges	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.charges (
  id text,
  amount bigint,
  currency text,
  customer text,
  description text,
  invoice text,
  payment_intent text,
  status text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'charges'
  );
Notes#
Charges are read-only records of payment transactions in your Stripe account
Each charge includes amount, currency, customer, and payment status information
Use the attrs jsonb column to access additional charge details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
Checkout Sessions#
This is an object representing your customer's session as they pay for one-time purchases or subscriptions through Checkout or Payment Links. We recommend creating a new Session each time your customer attempts to pay.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Checkout Sessions	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.checkout_sessions (
  id text,
  customer text,
  payment_intent text,
  subscription text,
  attrs jsonb
)
  server stripe_server
  options (
    object 'checkout/sessions',
    rowid_column 'id'
  );
Notes#
Checkout Sessions are read-only records of customer payment sessions in your Stripe account
Each session includes customer, payment intent, and subscription information
Use the attrs jsonb column to access additional session details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
payment_intent
subscription
Customers#
This is an object representing your Stripe customers. You can create, retrieve, update, and delete customers.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Customers	✅	✅	✅	✅	❌
Usage#
create foreign table stripe.customers (
  id text,
  email text,
  name text,
  description text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'customers',
    rowid_column 'id'
  );
Example operations:

-- create a new customer
insert into stripe.customers(email, name, description)
values ('jane@example.com', 'Jane Smith', 'Premium customer');
-- update a customer
update stripe.customers
set name = 'Jane Doe'
where email = 'jane@example.com';
-- delete a customer
delete from stripe.customers
where id = 'cus_xxx';
Notes#
Customers can be created, retrieved, updated, and deleted through SQL operations
Each customer can have an email, name, and description
Use the attrs jsonb column to access additional customer details
While any column is allowed in a where clause, it is most efficient to filter by:
id
email
Disputes#
This is an object representing a dispute that occurs when a customer questions your charge with their card issuer.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Disputes	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.disputes (
  id text,
  amount bigint,
  currency text,
  charge text,
  payment_intent text,
  reason text,
  status text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'disputes'
  );
Notes#
Disputes are read-only records of customer payment disputes in your Stripe account
Each dispute includes amount, currency, charge, and payment intent information
Use the attrs jsonb column to access additional dispute details
While any column is allowed in a where clause, it is most efficient to filter by:
id
charge
payment_intent
Events#
This is an object representing events that occur in your Stripe account, letting you know when something interesting happens.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Events	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.events (
  id text,
  type text,
  api_version text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'events'
  );
Notes#
Events are read-only records of activities in your Stripe account
Each event includes type, API version, and timestamp information
Use the attrs jsonb column to access additional event details
While any column is allowed in a where clause, it is most efficient to filter by:
id
type
Files#
This is an object representing a file hosted on Stripe's servers.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Files	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.files (
  id text,
  filename text,
  purpose text,
  title text,
  size bigint,
  type text,
  url text,
  created timestamp,
  expires_at timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'files'
  );
Notes#
Files are read-only records of files hosted on Stripe's servers
Each file includes filename, purpose, size, type, and URL information
Files may have an expiration date specified in expires_at
Use the attrs jsonb column to access additional file details
While any column is allowed in a where clause, it is most efficient to filter by:
id
purpose
File Links#
This is an object representing a link that can be used to share the contents of a File object with non-Stripe users.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
File Links	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.file_links (
  id text,
  file text,
  url text,
  created timestamp,
  expired bool,
  expires_at timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'file_links'
  );
Notes#
File Links are read-only records that provide shareable access to Stripe files
Each link includes a reference to the file and a public URL
Links can be configured to expire at a specific time
Use the expired boolean to check if a link has expired
Use the attrs jsonb column to access additional link details
While any column is allowed in a where clause, it is most efficient to filter by:
id
file
Invoices#
This is an object representing statements of amounts owed by a customer, which are either generated one-off or periodically from a subscription.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Invoices	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.invoices (
  id text,
  customer text,
  subscription text,
  status text,
  total bigint,
  currency text,
  period_start timestamp,
  period_end timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'invoices'
  );
Notes#
Invoices are read-only records of amounts owed by customers
Each invoice includes customer, subscription, status, and amount information
Invoices track billing periods with period_start and period_end timestamps
Use the attrs jsonb column to access additional invoice details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
status
subscription
Mandates#
This is an object representing a record of the permission a customer has given you to debit their payment method.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Mandates	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.mandates (
  id text,
  payment_method text,
  status text,
  type text,
  attrs jsonb
)
  server stripe_server
  options (
    object 'mandates'
  );
Notes#
Mandates are read-only records of customer payment permissions
Each mandate includes payment method, status, and type information
Use the attrs jsonb column to access additional mandate details
While any column is allowed in a where clause, it is most efficient to filter by:
id
Meters#
This is an object representing a billing meter that allows you to track usage of a particular event.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Meters	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.billing_meter (
  id text,
  display_name text,
  event_name text,
  event_time_window text,
  status text,
  attrs jsonb
)
  server stripe_server
  options (
    object 'billing/meters'
  );
Notes#
Meters are read-only records for tracking event usage in billing
Each meter includes display name, event name, and time window information
The status field indicates whether the meter is active
Use the attrs jsonb column to access additional meter details
While any column is allowed in a where clause, it is most efficient to filter by:
id
Payment Intents#
This is an object representing a guide through the process of collecting a payment from your customer.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Payment Intents	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.payment_intents (
  id text,
  customer text,
  amount bigint,
  currency text,
  payment_method text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'payment_intents'
  );
Notes#
Payment Intents are read-only records that guide the payment collection process
Each intent includes customer, amount, currency, and payment method information
The created timestamp tracks when the payment intent was initiated
Use the attrs jsonb column to access additional payment intent details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
Payouts#
This is an object representing funds received from Stripe or initiated payouts to a bank account or debit card of a connected Stripe account.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Payouts	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.payouts (
  id text,
  amount bigint,
  currency text,
  arrival_date timestamp,
  description text,
  statement_descriptor text,
  status text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'payouts'
  );
Notes#
Payouts are read-only records of fund transfers
Each payout includes amount, currency, and status information
The arrival_date indicates when funds will be available
The statement_descriptor appears on your bank statement
Use the attrs jsonb column to access additional payout details
While any column is allowed in a where clause, it is most efficient to filter by:
id
status
Prices#
This is an object representing pricing configurations for products to facilitate multiple currencies and pricing options.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Prices	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.prices (
  id text,
  active bool,
  currency text,
  product text,
  unit_amount bigint,
  type text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'prices'
  );
Notes#
Prices are read-only records that define product pricing configurations
Each price includes currency, unit amount, and product reference
The active boolean indicates if the price can be used
The type field specifies the pricing model (e.g., one-time, recurring)
Use the attrs jsonb column to access additional price details
While any column is allowed in a where clause, it is most efficient to filter by:
id
active
Products#
This is an object representing all products available in Stripe.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Products	✅	✅	✅	✅	❌
Usage#
create foreign table stripe.products (
  id text,
  name text,
  active bool,
  default_price text,
  description text,
  created timestamp,
  updated timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'products',
    rowid_column 'id'
  );
Notes#
Products can be created, read, updated, and deleted
Each product includes name, description, and active status
The default_price links to the product's default Price object
The updated timestamp tracks the last modification time
Use the attrs jsonb column to access additional product details
While any column is allowed in a where clause, it is most efficient to filter by:
id
active
Refunds#
This is an object representing refunds for charges that have previously been created but not yet refunded.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Refunds	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.refunds (
  id text,
  amount bigint,
  currency text,
  charge text,
  payment_intent text,
  reason text,
  status text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'refunds'
  );
Notes#
Refunds are read-only records of charge reversals
Each refund includes amount, currency, and status information
The charge and payment_intent fields link to the original transaction
The reason field provides context for the refund
Use the attrs jsonb column to access additional refund details
While any column is allowed in a where clause, it is most efficient to filter by:
id
charge
payment_intent
SetupAttempts#
This is an object representing attempted confirmations of SetupIntents, tracking both successful and unsuccessful attempts.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
SetupAttempts	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.setup_attempts (
  id text,
  application text,
  customer text,
  on_behalf_of text,
  payment_method text,
  setup_intent text,
  status text,
  usage text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'setup_attempts'
  );
Notes#
SetupAttempts are read-only records of payment setup confirmation attempts
Each attempt includes customer, payment method, and status information
The setup_intent field links to the associated SetupIntent
The usage field indicates the intended payment method usage
Use the attrs jsonb column to access additional attempt details
While any column is allowed in a where clause, it is most efficient to filter by:
id
setup_intent
SetupIntents#
This is an object representing a guide through the process of setting up and saving customer payment credentials for future payments.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
SetupIntents	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.setup_intents (
  id text,
  client_secret text,
  customer text,
  description text,
  payment_method text,
  status text,
  usage text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'setup_intents'
  );
Notes#
SetupIntents are read-only records for saving customer payment credentials
Each intent includes customer, payment method, and status information
The client_secret is used for client-side confirmation
The usage field indicates how the payment method will be used
Use the attrs jsonb column to access additional intent details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
payment_method
Subscriptions#
This is an object representing customer recurring payment schedules.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Subscriptions	✅	✅	✅	✅	❌
Usage#
create foreign table stripe.subscriptions (
  id text,
  customer text,
  currency text,
  current_period_start timestamp,
  current_period_end timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'subscriptions',
    rowid_column 'id'
  );
Notes#
Subscriptions can be created, read, updated, and deleted
Each subscription includes customer and currency information
The current_period_start and current_period_end track billing cycles
The rowid_column option enables modification operations
Use the attrs jsonb column to access additional subscription details
While any column is allowed in a where clause, it is most efficient to filter by:
id
customer
price
status
Tokens#
This is an object representing a secure way to collect sensitive card, bank account, or personally identifiable information (PII) from customers.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Tokens	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.tokens (
  id text,
  type text,
  client_ip text,
  created timestamp,
  livemode boolean,
  used boolean,
  attrs jsonb
)
  server stripe_server
  options (
    object 'tokens'
  );
Notes#
Tokens are read-only, single-use objects for secure data collection
Each token includes type information (card, bank_account, pii, etc.)
The client_ip field records where the token was created
The used field indicates if the token has been used
Use the attrs jsonb column to access token details like card or bank information
While any column is allowed in a where clause, it is most efficient to filter by:
id
type
used
Top-ups#
This is an object representing a way to add funds to your Stripe balance.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Top-ups	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.topups (
  id text,
  amount bigint,
  currency text,
  description text,
  status text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'topups'
  );
Notes#
Top-ups are read-only records of balance additions
Each top-up includes amount and currency information
The status field tracks the top-up state (e.g., succeeded, failed)
Use the attrs jsonb column to access additional top-up details
While any column is allowed in a where clause, it is most efficient to filter by:
id
status
Transfers#
This is an object representing fund movements between Stripe accounts as part of Connect.

Ref: Stripe docs

Operations#
Object	Select	Insert	Update	Delete	Truncate
Transfers	✅	❌	❌	❌	❌
Usage#
create foreign table stripe.transfers (
  id text,
  amount bigint,
  currency text,
  description text,
  destination text,
  created timestamp,
  attrs jsonb
)
  server stripe_server
  options (
    object 'transfers'
  );
Notes#
Transfers are read-only records of fund movements between accounts
Each transfer includes amount, currency, and destination information
The destination field identifies the receiving Stripe account
Use the attrs jsonb column to access additional transfer details
While any column is allowed in a where clause, it is most efficient to filter by:
id
destination
Query Pushdown Support#
This FDW supports where clause pushdown. You can specify a filter in where clause and it will be passed to Stripe API call.

For example, this query

select * from stripe.customers where id = 'cus_xxx';
will be translated to a Stripe API call: https://api.stripe.com/v1/customers/cus_xxx.

For supported filter columns for each object, please check out foreign table documents above.

Limitations#
This section describes important limitations and considerations when using this FDW:

Large result sets may experience slower performance due to full data transfer requirement
Webhook events and real-time updates are not supported
API version mismatches can cause unexpected data format issues
Materialized views using these foreign tables may fail during logical backups
Examples#
Some examples on how to use Stripe foreign tables.

Basic example#
-- always limit records to reduce API calls to Stripe
select * from stripe.customers limit 10;
select * from stripe.invoices limit 10;
select * from stripe.subscriptions limit 10;
Query JSON attributes#
-- extract account name for an invoice
select id, attrs->>'account_name' as account_name
from stripe.invoices where id = 'in_xxx';
-- extract invoice line items for an invoice
select id, attrs#>'{lines,data}' as line_items
from stripe.invoices where id = 'in_xxx';
-- extract subscription items for a subscription
select id, attrs#>'{items,data}' as items
from stripe.subscriptions where id = 'sub_xxx';
Data modify#
-- insert
insert into stripe.customers(email,name,description)
values ('test@test.com', 'test name', null);
-- update
update stripe.customers
set description='hello fdw'
where id = 'cus_xxx';
update stripe.customers
set attrs='{"metadata[foo]": "bar"}'
where id = 'cus_xxx';
-- delete
delete from stripe.customers
where id = 'cus_xxx';
To insert into an object with sub-fields, we need to create the foreign table with column name exactly same as the API required. For example, to insert a subscription object we can define the foreign table following the Stripe API docs:

-- create the subscription table for data insertion, the 'customer'
-- and 'items[0][price]' fields are required.
create foreign table stripe.subscriptions (
  id text,
  customer text,
  "items[0][price]" text  -- column name will be used in API Post request
)
  server stripe_server
  options (
    object 'subscriptions',
    rowid_column 'id'
  );
And then we can insert a subscription like below:

insert into stripe.subscriptions(customer, "items[0][price]")
values ('cus_Na6dX7aXxi11N4', 'price_1MowQULkdIwHu7ixraBm864M');
Note this foreign table is only for data insertion, it cannot be used in select statement.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Preparation
Enable Wrappers
Enable the Stripe Wrapper
Store your credentials (optional)
Connecting to Stripe
Create a schema
Entities
Accounts
Balance
Balance Transactions
Charges
Checkout Sessions
Customers
Disputes
Events
Files
File Links
Invoices
Mandates
Meters
Payment Intents
Payouts
Prices
Products
Refunds
SetupAttempts
SetupIntents
Subscriptions
Tokens
Top-ups
Transfers
Query Pushdown Support
Limitations
Examples
Basic example
Query JSON attributes
Data modify
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth
Database
Examples
Drop All Tables in Schema
Drop all tables in a PostgreSQL schema

Execute the following query to drop all tables in a given schema.
Replace my-schema-name with the name of your schema. In Supabase, the default schema is public.

This deletes all tables and their associated data. Ensure you have a recent backup before proceeding.

do $$ declare
    r record;
begin
    for r in (select tablename from pg_tables where schemaname = 'my-schema-name') loop
        execute 'drop table if exists ' || quote_ident(r.tablename) || ' cascade';
    end loop;
end $$;

This query works by listing out all the tables in the given schema and then executing a drop table for each (hence the for... loop).

You can run this query using the SQL Editor in the Supabase Dashboard, or via psql if you're connecting directly to the database.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Auth

Select first row for each group in PostgreSQL

Given a table seasons:

id	team	points
1	Liverpool	82
2	Liverpool	84
3	Brighton	34
4	Brighton	28
5	Liverpool	79
We want to find the rows containing the maximum number of points per team.

The expected output we want is:

id	team	points
3	Brighton	34
2	Liverpool	84
From the SQL Editor, you can run a query like:

select distinct
  on (team) id,
  team,
  points
from
  seasons
order BY
  id,
  points desc,
  team;
The important bits here are:

The desc keyword to order the points from highest to lowest.
The distinct keyword that tells Postgres to only return a single row per team.
This query can also be executed via psql or any other query editor if you prefer to connect directly to the database.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSq

Database
Examples
Print PostgreSQL Version
Print PostgreSQL version

It's important to know which version of Postgres you are running as each major version has different features and may cause breaking changes. You may also need to update your schema when upgrading or downgrading to a major Postgres version.

Run the following query using the SQL Editor in the Supabase Dashboard:

select
  version();
Which should return something like:

PostgreSQL 15.1 on aarch64-unknown-linux-gnu, compiled by gcc (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0, 64-bit
This query can also be executed via psql or any other query editor if you prefer to connect directly to the database.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSq

Database
Examples
Replicating from Supabase to External Postgres
Replicate to another Postgres database using Logical Replication

For this example, you will need:

A Supabase project
A Postgres database (running v10 or newer)
You will be running commands on both of these databases to publish changes from the Supabase database to the external database.

Create a publication on the Supabase database:
CREATE PUBLICATION example_pub;
Also on the Supabase database, create a replication slot:
select pg_create_logical_replication_slot('example_slot', 'pgoutput');
Now we will connect to our external database and subscribe to our publication Note: ):
This will need a direct connection (not a Connection Pooler) to your database and you can find the connection info in the Connect panel in the Direct connection section.

You will also need to ensure that IPv6 is supported by your replication destination (or you can enable the IPv4 add-on)

If you would prefer not to use the postgres user, then you can run CREATE ROLE <user> WITH REPLICATION; using the postgres user.

CREATE SUBSCRIPTION example_sub
CONNECTION 'host=db.oaguxblfdassqxvvwtfe.supabase.co user=postgres password=YOUR_PASS dbname=postgres'
PUBLICATION example_pub
WITH (copy_data = true, create_slot=false, slot_name=example_slot);
create_slot is set to false because slot_name is provided and the slot was already created in Step 2.
To copy data from before the slot was created, set copy_data to true.

Now we'll go back to the Supabase DB and add all the tables that you want replicated to the publication.
ALTER PUBLICATION example_pub ADD TABLE example_table;
Check the replication status using pg_stat_replication
select * from pg_stat_replication;
You can add more tables to the initial publication, but you're going to need to do a REFRESH on the subscribing database.
See https://www.postgresql.org/docs/current/sql-alterpublication.html

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSq

Features

This is a non-exhaustive list of features that Supabase provides for every project.

Database#
Postgres database#
Every project is a full Postgres database. Docs.

Vector database#
Store vector embeddings right next to the rest of your data. Docs.

Auto-generated REST API via PostgREST#
RESTful APIs are auto-generated from your database, without a single line of code. Docs.

Auto-generated GraphQL API via pg_graphql#
Fast GraphQL APIs using our custom Postgres GraphQL extension. Docs.

Database webhooks#
Send database changes to any external service using Webhooks. Docs.

Secrets and encryption#
Encrypt sensitive data and store secrets using our Postgres extension, Supabase Vault. Docs.

Platform#
Database backups#
Projects are backed up daily with the option to upgrade to Point in Time recovery. Docs.

Custom domains#
White-label the Supabase APIs to create a branded experience for your users. Docs.

Network restrictions#
Restrict IP ranges that can connect to your database. Docs.

SSL enforcement#
Enforce Postgres clients to connect via SSL. Docs.

Branching#
Use Supabase Branches to test and preview changes. Docs.

Terraform provider#
Manage Supabase infrastructure via Terraform, an Infrastructure as Code tool. Docs.

Read replicas#
Deploy read-only databases across multiple regions, for lower latency and better resource management. Docs.

Log drains#
Export Supabase logs to 3rd party providers and external tooling. Docs.

Studio#
Studio Single Sign-On#
Login to the Supabase dashboard via SSO. Docs.


Realtime#
Postgres changes#
Receive your database changes through WebSockets. Docs.

Broadcast#
Send messages between connected users through WebSockets. Docs.

Presence#
Synchronize shared state across your users, including online status and typing indicators. Docs.

Auth#
Email login#
Build email logins for your application or website. Docs.

Social login#
Provide social logins - everything from Apple, to GitHub, to Slack. Docs.

Phone logins#
Provide phone logins using a third-party SMS provider. Docs.

Passwordless login#
Build passwordless logins via magic links for your application or website. Docs.

Authorization via Row Level Security#
Control the data each user can access with Postgres Policies. Docs.

CAPTCHA protection#
Add CAPTCHA to your sign-in, sign-up, and password reset forms. Docs.

Server-Side Auth#
Helpers for implementing user authentication in popular server-side languages and frameworks like Next.js, SvelteKit and Remix. Docs.


Storage#
File storage#
Supabase Storage makes it simple to store and serve files. Docs.

Content Delivery Network#
Cache large files using the Supabase CDN. Docs.

Smart Content Delivery Network#
Automatically revalidate assets at the edge via the Smart CDN. Docs.

Image transformations#
Transform images on the fly. Docs.

Resumable uploads#
Upload large files using resumable uploads. Docs.

S3 compatibility#
Interact with Storage from tool which supports the S3 protocol. Docs.

Edge Functions#
Deno Edge Functions#
Globally distributed TypeScript functions to execute custom business logic. Docs.

Regional invocations#
Execute an Edge Function in a region close to your database. Docs.

NPM compatibility#
Edge functions natively support NPM modules and Node built-in APIs. Link.

Project management#
CLI#
Use our CLI to develop your project locally and deploy to the Supabase Platform. Docs.

Management API#
Manage your projects programmatically. Docs.

Client libraries#
Official client libraries for JavaScript, Flutter and Swift.
Unofficial libraries are supported by the community.

Feature status#
Supabase Features are in 4 different states - Private Alpha, Public Alpha, Beta and Generally Available.

Private alpha#
Features are initially launched as a private alpha to gather feedback from the community. To join our early access program, send an email to product-ops@supabase.io.

Public alpha#
The alpha stage indicates that the API might change in the future, not that the service isn’t stable. Even though the uptime Service Level Agreement does not cover products in Alpha, we do our best to have the service as stable as possible.

Beta#
Features in Beta are tested by an external penetration tester for security issues. The API is guaranteed to be stable and there is a strict communication process for breaking changes.

Generally available#
In addition to the Beta requirements, features in GA are covered by the uptime SLA.

Product	Feature	Stage	Available on self-hosted
Database	Postgres	GA	✅
Database	Vector Database	GA	✅
Database	Auto-generated Rest API	GA	✅
Database	Auto-generated GraphQL API	GA	✅
Database	Webhooks	beta	✅
Database	Vault	public alpha	✅
Platform		GA	✅
Platform	Point-in-Time Recovery	GA	🚧 wal-g
Platform	Custom Domains	GA	N/A
Platform	Network Restrictions	GA	N/A
Platform	SSL enforcement	GA	N/A
Platform	Branching	beta	N/A
Platform	Terraform Provider	public alpha	N/A
Platform	Read Replicas	GA	N/A
Platform	Log Drains	public alpha	✅
Platform	MCP	public alpha	✅
Studio		GA	✅
Studio	SSO	GA	✅
Studio	Column Privileges	public alpha	✅
Realtime	Postgres Changes	GA	✅
Realtime	Broadcast	GA	✅
Realtime	Presence	GA	✅
Realtime	Broadcast Authorization	public beta	✅
Realtime	Presence Authorization	public beta	✅
Realtime	Broadcast from Database	public beta	✅
Storage		GA	✅
Storage	CDN	GA	🚧 Cloudflare
Storage	Smart CDN	GA	🚧 Cloudflare
Storage	Image Transformations	GA	✅
Storage	Resumable Uploads	GA	✅
Storage	S3 compatibility	GA	✅
Edge Functions		GA	✅
Edge Functions	Regional Invocations	GA	✅
Edge Functions	NPM compatibility	GA	✅
Auth		GA	✅
Auth	Email login	GA	✅
Auth	Social login	GA	✅
Auth	Phone login	GA	✅
Auth	Passwordless login	GA	✅
Auth	SSO with SAML	GA	✅
Auth	Authorization via RLS	GA	✅
Auth	CAPTCHA protection	GA	✅
Auth	Server-side Auth	beta	✅
Auth	Third-Party Auth	GA	✅
Auth	Hooks	beta	✅
CLI		GA	✅ Works with self-hosted
Management API		GA	N/A
Client Library	JavaScript	GA	N/A
Client Library	Flutter	GA	N/A
Client Library	Swift	GA	N/A
Client Library	Python	beta	N/A
✅ = Fully Available
🚧 = Available, but requires external tools or configuration
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Database
Postgres database
Vector database
Auto-generated REST API via PostgREST
Auto-generated GraphQL API via pg_graphql
Database webhooks
Secrets and encryption
Platform
Database backups
Custom domains
Network restrictions
SSL enforcement
Branching
Terraform provider
Read replicas
Log drains
Studio
Studio Single Sign-On
Realtime
Postgres changes
Broadcast
Presence
Auth
Email login
Social login
Phone logins
Passwordless login
Authorization via Row Level Security
CAPTCHA protection
Server-Side Auth
Storage
File storage
Content Delivery Network
Smart Content Delivery Network
Image transformations
Resumable uploads
S3 compatibility
Edge Functions
Deno Edge Functions
Regional invocations
NPM compatibility
Project management
CLI
Management API
Client libraries
Feature status
Private alpha
Public alpha
Beta
Generally available
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Stylegui

Start with Supabase
Architecture
Architecture

Supabase is open source. We choose open source tools which are scalable and make them simple to use.

Supabase is not a 1-to-1 mapping of Firebase. While we are building many of the features that Firebase offers, we are not going about it the same way:
our technological choices are quite different; everything we use is open source; and wherever possible, we use and support existing tools rather than developing from scratch.

Most notably, we use Postgres rather than a NoSQL store. This choice was deliberate. We believe that no other database offers the functionality required to compete with Firebase, while maintaining the scalability required to go beyond it.

Choose your comfort level#
Our goal at Supabase is to make all of Postgres easy to use. That doesn’t mean you have to use all of it. If you’re a Postgres veteran, you’ll probably love the tools that we offer. If you’ve never used Postgres before, then start smaller and grow into it. If you just want to treat Postgres like a simple table-store, that’s perfectly fine.

Architecture#
Each Supabase project consists of several tools:

Diagram showing the architecture of Supabase. The Kong API gateway sits in front of 7 services: GoTrue, PostgREST, Realtime, Storage, pg_meta, Functions, and pg_graphql. All the services talk to a single Postgres instance.
Postgres (database)#
Postgres is the core of Supabase. We do not abstract the Postgres database—you can access it and use it with full privileges. We provide tools which make Postgres as easy to use as Firebase.

Official Docs: postgresql.org/docs
Source code: github.com/postgres/postgres (mirror)
License: PostgreSQL License- Language: C
Studio (dashboard)#
An open source Dashboard for managing your database and services.

Official Docs: Supabase docs
Source code: github.com/supabase/supabase
License: Apache 2
Language: TypeScript
GoTrue (Auth)#
A JWT-based API for managing users and issuing access tokens. This integrates with PostgreSQL's Row Level Security and the API servers.

Official Docs: Supabase Auth reference docs
Source code: github.com/supabase/gotrue
License: MIT
Language: Go
PostgREST (API)#
A standalone web server that turns your Postgres database directly into a RESTful API.
We use this with our pg_graphql extension to provide a GraphQL API.

Official Docs: postgrest.org
Source code: github.com/PostgREST/postgrest
License: MIT
Language: Haskell
Realtime (API & multiplayer)#
A scalable WebSocket engine for managing user Presence, broadcasting messages, and streaming database changes.

Official Docs: Supabase Realtime docs
Source code: github.com/supabase/realtime
License: Apache 2
Language: Elixir
Storage API (large file storage)#
An S3-compatible object storage service that stores metadata in Postgres.

Official Docs: Supabase Storage reference docs
Source code: github.com/supabase/storage-api
License: Apache 2.0
Language: Node.js / TypeScript
Deno (Edge Functions)#
A modern runtime for JavaScript and TypeScript.

Official Docs: Deno documentation
Source code: Deno source code
License: MIT
Language: TypeScript / Rust
postgres-meta (database management)#
A RESTful API for managing your Postgres. Fetch tables, add roles, and run queries.

Official Docs: supabase.github.io/postgres-meta
Source code: github.com/supabase/postgres-meta
License: Apache 2.0
Language: Node.js / TypeScript
Supavisor#
A cloud-native, multi-tenant Postgres connection pooler.

Official Docs: Supavisor GitHub Pages
Source code: supabase/supavisor
License: Apache 2.0
Language: Elixir
Kong (API gateway)#
A cloud-native API gateway, built on top of NGINX.

Official Docs: docs.konghq.com
Source code: github.com/kong/kong
License: Apache 2.0
Language: Lua
Product principles#
It is our goal to provide an architecture that any large-scale company would design for themselves,
and then provide tooling around that architecture that is easy-to-use for indie-developers and small teams.

We use a series of principles to ensure that scalability and usability are never mutually exclusive:

Everything works in isolation#
Each system must work as a standalone tool with as few moving parts as possible.
The litmus test for this is: "Can a user run this product with nothing but a Postgres database?"

Everything is integrated#
Supabase is composable. Even though every product works in isolation, each product on the platform needs to 10x the other products.
For integration, each tool should expose an API and Webhooks.

Everything is extensible#
We're deliberate about adding a new tool, and prefer instead to extend an existing one.
This is the opposite of many cloud providers whose product offering expands into niche use-cases. We provide primitives for developers, which allow them to achieve any goal.
Less, but better.

Everything is portable#
To avoid lock-in, we make it easy to migrate in and out. Our cloud offering is compatible with our self-hosted product.
We use existing standards to increase portability (like pg_dump and CSV files). If a new standard emerges which competes with a "Supabase" approach, we will deprecate the approach in favor of the standard.
This forces us to compete on user experience. We aim to be the best Postgres hosting service.

Play the long game#
We sacrifice short-term wins for long-term gains. For example, it is tempting to run a fork of Postgres with additional functionality which only our customers need.
Instead, we prefer to support efforts to upstream missing functionality so that the entire community benefits. This has the additional benefit of ensuring portability and longevity.

Build for developers#
"Developers" are a specific profile of user: they are builders.
When assessing impact as a function of effort, developers have a large efficiency due to the type of products and systems they can build.
As the profile of a developer changes over time, Supabase will continue to evolve the product to fit this evolving profile.

Support existing tools#
Supabase supports existing tools and communities wherever possible. Supabase is more like a "community of communities" - each tool typically has its own community which we work with.
Open source is something we approach collaboratively: we employ maintainers, sponsor projects, invest in businesses, and develop our own open source tools.

Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Choose your comfort level
Architecture
Postgres (database)
Studio (dashboard)
GoTrue (Auth)
PostgREST (API)
Realtime (API & multiplayer)
Storage API (large file storage)
Deno (Edge Functions)
postgres-meta (database management)
Supavisor
Kong (API gateway)
Product principles
Everything works in isolation
Everything is integrated
Everything is extensible
Everything is portable
Play the long game
Build for developers
Support existing tools
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Stylegui
Start with Supabase
Framework Quickstarts
SvelteKit
Use Supabase with SvelteKit

Learn how to create a Supabase project, add some sample data to your database, and query the data from a SvelteKit app.

1
Create a Supabase project
Go to database.new and create a new Supabase project.

Alternatively, you can create a project using the Management API:

# First, get your access token from https://supabase.com/dashboard/account/tokens
export SUPABASE_ACCESS_TOKEN="your-access-token"
# List your organizations to get the organization ID
curl -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  https://api.supabase.com/v1/organizations
# Create a new project (replace <org-id> with your organization ID)
curl -X POST https://api.supabase.com/v1/projects \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "organization_id": "<org-id>",
    "name": "My Project",
    "region": "us-east-1",
    "db_pass": "<your-secure-password>"
  }'

When your project is up and running, go to the Table Editor, create a new table and insert some data.

Alternatively, you can run the following snippet in your project's SQL Editor. This will create a instruments table with some sample data.

-- Create the table
create table instruments (
  id bigint primary key generated always as identity,
  name text not null
);
-- Insert some sample data into the table
insert into instruments (name)
values
  ('violin'),
  ('viola'),
  ('cello');
alter table instruments enable row level security;

Make the data in your table publicly readable by adding an RLS policy:

create policy "public can read instruments"
on public.instruments
for select to anon
using (true);

2
Create a SvelteKit app
Create a SvelteKit app using the npm create command.

Terminal
npx sv create my-app
3
Install the Supabase client library
The fastest way to get started is to use the supabase-js client library which provides a convenient interface for working with Supabase from a SvelteKit app.

Navigate to the SvelteKit app and install supabase-js.

Terminal
cd my-app && npm install @supabase/supabase-js
4
Declare Supabase Environment Variables
Create a .env file at the root of your project and populate with your Supabase connection variables:

Project URL
No project found
YOUR PROJECT URL

To get your Project URL, log in.

Publishable key
No project found
YOUR PUBLISHABLE KEY

To get your Publishable key, log in.

Anon key
No project found
YOUR ANON KEY

To get your Anon key, log in.


.env
PUBLIC_SUPABASE_URL=<SUBSTITUTE_SUPABASE_URL>
PUBLIC_SUPABASE_PUBLISHABLE_KEY=<SUBSTITUTE_SUPABASE_PUBLISHABLE_KEY>
5
Create the Supabase client
Create a src/lib directory in your SvelteKit app, create a file called supabaseClient.js and add the following code to initialize the Supabase client:


src/lib/supabaseClient.js

src/lib/supabaseClient.ts
import { createClient } from '@supabase/supabase-js';
import { PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY } from '$env/static/public';
export const supabase = createClient(PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY)
6
Query data from the app
Use load method to fetch the data server-side and display the query results as a simple list.

Create +page.server.js file in the src/routes directory with the following code.


src/routes/+page.server.js

src/routes/+page.server.ts
import { supabase } from "$lib/supabaseClient";
  export async function load() {
    const { data } = await supabase.from("instruments").select();
    return {
      instruments: data ?? [],
    };
  }
Replace the existing content in your +page.svelte file in the src/routes directory with the following code.

src/routes/+page.svelte
<script>
    let { data } = $props();
  </script>
  <ul>
    {#each data.instruments as instrument}
      <li>{instrument.name}</li>
    {/each}
  </ul>
7
Start the app
Start the app and go to http://localhost:5173 in a browser and you should see the list of instruments.

Terminal
npm run dev
Next steps#
Set up Auth for your app
Insert more data into your database
Upload and serve static files using Storage
Edit this page on GitHub
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Stylegui
Start with Supabase
Web app demos
Svelte
Build a User Management App with Svelte

This tutorial demonstrates how to build a basic user management app. The app authenticates and identifies the user, stores their profile information in the database, and allows the user to log in, update their profile details, and upload a profile photo. The app uses:

Supabase Database - a Postgres database for storing your user data and Row Level Security so data is protected and users can only access their own information.
Supabase Auth - allow users to sign up and log in.
Supabase Storage - allow users to upload a profile photo.
Supabase User Management example

If you get stuck while working through this guide, refer to the full example on GitHub.

Project setup#
Before you start building you need to set up the Database and API. You can do this by starting a new Project in Supabase and then creating a "schema" inside the database.

Create a project#
Create a new project in the Supabase Dashboard.
Enter your project details.
Wait for the new database to launch.
Set up the database schema#
Now set up the database schema. You can use the "User Management Starter" quickstart in the SQL Editor, or you can copy/paste the SQL from below and run it.


Dashboard

SQL
Go to the SQL Editor page in the Dashboard.
Click User Management Starter under the Community > Quickstarts tab.
Click Run.
You can pull the database schema down to your local project by running the db pull command. Read the local development docs for detailed instructions.

supabase link --project-ref <project-id>
# You can get <project-id> from your project's dashboard URL: https://supabase.com/dashboard/project/<project-id>
supabase db pull
Get API details#
Now that you've created some database tables, you are ready to insert data using the auto-generated API.

To do this, you need to get the Project URL and key. Get the URL from the API settings section of a project and the key from the the API Keys section of a project's Settings page.

Changes to API keys
Supabase is changing the way keys work to improve project security and developer experience. You can read the full announcement, but in the transition period, you can use both the current anon and service_role keys and the new publishable key with the form sb_publishable_xxx which will replace the older keys.

To get the key values, open the API Keys section of a project's Settings page and do the following:

For legacy keys, copy the anon key for client-side operations and the service_role key for server-side operations from the Legacy API Keys tab.
For new keys, open the API Keys tab, if you don't have a publishable key already, click Create new API Keys, and copy the value from the Publishable key section.
Building the app#
Start building the Svelte app from scratch.

Initialize a Svelte app#
You can use the Vite Svelte TypeScript Template to initialize an app called supabase-svelte:

npm create vite@latest supabase-svelte -- --template svelte-ts
cd supabase-svelte
npm install
Install the only additional dependency: supabase-js

npm install @supabase/supabase-js
Finally, save the environment variables in a .env.
All you need are the API URL and the key that you copied earlier.


.env
VITE_SUPABASE_URL=YOUR_SUPABASE_URL
VITE_SUPABASE_PUBLISHABLE_KEY=YOUR_SUPABASE_PUBLISHABLE_KEY
Now you have the API credentials in place, create a helper file to initialize the Supabase client. These variables will be exposed on the browser, and that's fine since you have Row Level Security enabled on the Database.

src/supabaseClient.ts
import { createClient } from '@supabase/supabase-js'
const supabaseUrl = import.meta.env.VITE_SUPABASE_URL
const supabasePublishableKey = import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY
export const supabase = createClient(supabaseUrl, supabasePublishableKey)
View source
App styling (optional)#
Optionally, update the CSS file src/app.css to make the app look nice.
You can find the full contents of this file on GitHub.

Set up a login component#
Set up a Svelte component to manage logins and sign ups. It uses Magic Links, so users can sign in with their email without using passwords.

src/lib/Auth.svelte
<script lang="ts">
  import { supabase } from "../supabaseClient";
  let loading = $state(false);
  let email = $state("");
  const handleLogin = async () => {
    try {
      loading = true;
      const { error } = await supabase.auth.signInWithOtp({ email });
      if (error) throw error;
      alert("Check your email for login link!");
    } catch (error) {
      if (error instanceof Error) {
        alert(error.message);
      }
    } finally {
      loading = false;
    }
  };
</script>
<div class="row flex-center flex">
  <div class="col-6 form-widget" aria-live="polite">
    <h1 class="header">Supabase + Svelte</h1>
    <p class="description">Sign in via magic link with your email below</p>
    <form class="form-widget" onsubmit={(e) => { e.preventDefault(); handleLogin(); }}>
      <div>
        <label for="email">Email</label>
        <input
          id="email"
          class="inputField"
          type="email"
          placeholder="Your email"
          bind:value={email}
        />
      </div>
      <div>
        <button
          type="submit"
          class="button block"
          aria-live="polite"
          disabled={loading}
        >
          <span>{loading ? "Loading" : "Send magic link"}</span>
        </button>
      </div>
    </form>
  </div>
</div>
View source
Account page#
After a user is signed in, allow them to edit their profile details and manage their account.
Create a new component for that called Account.svelte.

<script lang="ts">
  import { onMount } from "svelte";
  import type { AuthSession } from "@supabase/supabase-js";
  import { supabase } from "../supabaseClient";
  // ...
  interface Props {
    session: AuthSession;
  }
  let { session }: Props = $props();
  // ...
  let username = $state<string | null>(null);
  let website = $state<string | null>(null);
  let avatarUrl = $state<string | null>(null);
  onMount(() => {
    getProfile();
  });
  const getProfile = async () => {
    try {
      loading = true;
      const { user } = session;
      const { data, error, status } = await supabase
        .from("profiles")
        .select("username, website, avatar_url")
        .eq("id", user.id)
        .single();
      if (error && status !== 406) throw error;
// ...
      if (data) {
        username = data.username;
        website = data.website;
        avatarUrl = data.avatar_url;
      }
    } catch (error) {
      if (error instanceof Error) {
        alert(error.message);
      }
    } finally {
      loading = false;
    }
  };
  const updateProfile = async () => {
    try {
      loading = true;
      const { user } = session;
        // ...
        id: user.id,
        username,
        website,
        avatar_url: avatarUrl,
        updated_at: new Date().toISOString(),
      };
      const { error } = await supabase.from("profiles").upsert(updates);
      if (error) {
        throw error;
      }
    } catch (error) {
      if (error instanceof Error) {
        alert(error.message);
      }
    } finally {
      loading = false;
    }
// ...
</script>
<form onsubmit={(e) => { e.preventDefault(); updateProfile(); }} class="form-widget">
  <div>Email: {session.user.email}</div>
  <div>
    <Avatar bind:url={avatarUrl} size={150} onupload={updateProfile} />
    <label for="username">Name</label>
    <input id="username" type="text" bind:value={username} />
  </div>
  <div>
    <label for="website">Website</label>
    <input id="website" type="text" bind:value={website} />
  </div>
  <div>
    <button type="submit" class="button primary block" disabled={loading}>
      {loading ? "Saving ..." : "Update profile"}
    </button>
  </div>
  <button
    type="button"
    class="button block"
    onclick={() => supabase.auth.signOut()}
  >
    Sign Out
  </button>
</form>
View source
Launch!#
Now that you have all the components in place, update App.svelte:

src/App.svelte
<script lang="ts">
  import { onMount } from 'svelte'
  import { supabase } from './supabaseClient'
  import type { AuthSession } from '@supabase/supabase-js'
  import Account from './lib/Account.svelte'
  import Auth from './lib/Auth.svelte'
  let session = $state<AuthSession | null>(null)
  onMount(() => {
    supabase.auth.getSession().then(({ data }) => {
      session = data.session
    })
    supabase.auth.onAuthStateChange((_event, _session) => {
      session = _session
    })
  })
</script>
<div class="container" style="padding: 50px 0 100px 0">
  {#if !session}
  <Auth />
  {:else}
  <Account {session} />
  {/if}
</div>
View source
Once that's done, run this in a terminal window:

npm run dev
And then open the browser to localhost:5173 and you should see the completed app.

Svelte uses Vite and the default port is 5173, Supabase uses port 3000. To change the redirection port for Supabase go to: Authentication > URL Configuration and change the Site URL to http://localhost:5173/

Supabase Svelte

Bonus: Profile photos#
Every Supabase project is configured with Storage for managing large files like photos and videos.

Create an upload widget#
Create an avatar for the user so that they can upload a profile photo. Start by creating a new component:

src/lib/Avatar.svelte
<script lang="ts">
  import { supabase } from "../supabaseClient";
  interface Props {
    size: number;
    url?: string | null;
    onupload?: () => void;
  }
  let { size, url = $bindable(null), onupload }: Props = $props();
  let avatarUrl = $state<string | null>(null);
  let uploading = $state(false);
  let files = $state<FileList>();
  const downloadImage = async (path: string) => {
    try {
      const { data, error } = await supabase.storage
        .from("avatars")
        .download(path);
      if (error) {
        throw error;
      }
      const url = URL.createObjectURL(data);
      avatarUrl = url;
    } catch (error) {
      if (error instanceof Error) {
        console.log("Error downloading image: ", error.message);
      }
    }
  };
  const uploadAvatar = async () => {
    try {
      uploading = true;
      if (!files || files.length === 0) {
        throw new Error("You must select an image to upload.");
      }
      const file = files[0];
      const fileExt = file.name.split(".").pop();
      const filePath = `${Math.random()}.${fileExt}`;
      const { error } = await supabase.storage
        .from("avatars")
        .upload(filePath, file);
      if (error) {
        throw error;
      }
      url = filePath;
      onupload?.();
    } catch (error) {
      if (error instanceof Error) {
        alert(error.message);
      }
    } finally {
      uploading = false;
    }
  };
  $effect(() => {
    if (url) downloadImage(url);
  });
</script>
<div style="width: {size}px" aria-live="polite">
  {#if avatarUrl}
    <img
      src={avatarUrl}
      alt={avatarUrl ? "Avatar" : "No image"}
      class="avatar image"
      style="height: {size}px, width: {size}px"
    />
  {:else}
    <div class="avatar no-image" style="height: {size}px, width: {size}px"></div>
  {/if}
  <div style="width: {size}px">
    <label class="button primary block" for="single">
      {uploading ? "Uploading ..." : "Upload avatar"}
    </label>
    <span style="display:none">
      <input
        type="file"
        id="single"
        accept="image/*"
        bind:files
        onchange={uploadAvatar}
        disabled={uploading}
      />
    </span>
  </div>
</div>
View source
Add the new widget#
And then you can add the widget to the Account page:

<script lang="ts">
  // ...
  import Avatar from "./Avatar.svelte";
    // ...
    } finally {
      loading = false;
    }
  // ...
  };
  // ...
  </div>
  <button
    type="button"
    class="button block"
    onclick={() => supabase.auth.signOut()}
  >
    Sign Out
  </button>
</form>
View source
At this stage you have a fully functional application!

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Project setup
Create a project
Set up the database schema
Get API details
Building the app
Initialize a Svelte app
App styling (optional)
Set up a login component
Account page
Launch!
Bonus: Profile photos
Create an upload widget
Add the new widget
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Stylegui

Start with Supabase
Web app demos
SvelteKit
Build a User Management App with SvelteKit

This tutorial demonstrates how to build a basic user management app. The app authenticates and identifies the user, stores their profile information in the database, and allows the user to log in, update their profile details, and upload a profile photo. The app uses:

Supabase Database - a Postgres database for storing your user data and Row Level Security so data is protected and users can only access their own information.
Supabase Auth - allow users to sign up and log in.
Supabase Storage - allow users to upload a profile photo.
Supabase User Management example

If you get stuck while working through this guide, refer to the full example on GitHub.

Project setup#
Before you start building you need to set up the Database and API. You can do this by starting a new Project in Supabase and then creating a "schema" inside the database.

Create a project#
Create a new project in the Supabase Dashboard.
Enter your project details.
Wait for the new database to launch.
Set up the database schema#
Now set up the database schema. You can use the "User Management Starter" quickstart in the SQL Editor, or you can copy/paste the SQL from below and run it.


Dashboard

SQL
Go to the SQL Editor page in the Dashboard.
Click User Management Starter under the Community > Quickstarts tab.
Click Run.
You can pull the database schema down to your local project by running the db pull command. Read the local development docs for detailed instructions.

supabase link --project-ref <project-id>
# You can get <project-id> from your project's dashboard URL: https://supabase.com/dashboard/project/<project-id>
supabase db pull
Get API details#
Now that you've created some database tables, you are ready to insert data using the auto-generated API.

To do this, you need to get the Project URL and key. Get the URL from the API settings section of a project and the key from the the API Keys section of a project's Settings page.

Changes to API keys
Supabase is changing the way keys work to improve project security and developer experience. You can read the full announcement, but in the transition period, you can use both the current anon and service_role keys and the new publishable key with the form sb_publishable_xxx which will replace the older keys.

To get the key values, open the API Keys section of a project's Settings page and do the following:

For legacy keys, copy the anon key for client-side operations and the service_role key for server-side operations from the Legacy API Keys tab.
For new keys, open the API Keys tab, if you don't have a publishable key already, click Create new API Keys, and copy the value from the Publishable key section.
Building the app#
Start building the Svelte app from scratch.

Initialize a Svelte app#
Use the SvelteKit Skeleton Project to initialize an app called supabase-sveltekit (for this tutorial, select "SvelteKit minimal" and use TypeScript):

npx sv create supabase-sveltekit
cd supabase-sveltekit
npm install
Then install the Supabase client library: supabase-js

npm install @supabase/supabase-js
And finally, save the environment variables in a .env file.
All you need are the PUBLIC_SUPABASE_URL and the key that you copied earlier.


.env
PUBLIC_SUPABASE_URL="YOUR_SUPABASE_URL"
PUBLIC_SUPABASE_PUBLISHABLE_KEY="YOUR_SUPABASE_PUBLISHABLE_KEY"
App styling (optional)#
An optional step is to update the CSS file src/styles.css to make the app look nice.
You can find the full contents of this file in the example repository.

Creating a Supabase client for SSR#
The ssr package configures Supabase to use Cookies, which are required for server-side languages and frameworks.

Install the SSR package:

npm install @supabase/ssr
Creating a Supabase client with the ssr package automatically configures it to use Cookies. This means the user's session is available throughout the entire SvelteKit stack - page, layout, server, and hooks.

Add the code below to a src/hooks.server.ts file to initialize the client on the server:


src/hooks.server.ts
// src/hooks.server.ts
import { PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY } from '$env/static/public'
import { createServerClient } from '@supabase/ssr'
import type { Handle } from '@sveltejs/kit'
export const handle: Handle = async ({ event, resolve }) => {
  event.locals.supabase = createServerClient(PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY, {
    cookies: {
      getAll: () => event.cookies.getAll(),
      /**
       * Note: You have to add the `path` variable to the
       * set and remove method due to sveltekit's cookie API
       * requiring this to be set, setting the path to `/`
       * will replicate previous/standard behaviour (https://kit.svelte.dev/docs/types#public-types-cookies)
       */
      setAll: (cookiesToSet) => {
        cookiesToSet.forEach(({ name, value, options }) => {
          event.cookies.set(name, value, { ...options, path: '/' })
        })
      },
    },
  })
  /**
   * Unlike `supabase.auth.getSession`, which is unsafe on the server because it
   * doesn't validate the JWT, this function validates the JWT by first calling
   * `getUser` and aborts early if the JWT signature is invalid.
   */
  event.locals.safeGetSession = async () => {
    const {
      data: { user },
      error,
    } = await event.locals.supabase.auth.getUser()
    if (error) {
      return { session: null, user: null }
    }
    const {
      data: { session },
    } = await event.locals.supabase.auth.getSession()
    return { session, user }
  }
  return resolve(event, {
    filterSerializedResponseHeaders(name: string) {
      return name === 'content-range' || name === 'x-supabase-api-version'
    },
  })
}
View source
Note that auth.getSession reads the auth token and the unencoded session data from the local storage medium. It doesn't send a request back to the Supabase Auth server unless the local session is expired.

You should never trust the unencoded session data if you're writing server code, since it could be tampered with by the sender. If you need verified, trustworthy user data, call auth.getUser instead, which always makes a request to the Auth server to fetch trusted data.

As this tutorial uses TypeScript the compiler complains about event.locals.supabase and event.locals.safeGetSession, you can fix this by updating the src/app.d.ts with the content below:


src/app.d.ts
import { SupabaseClient, Session } from '@supabase/supabase-js'
// See https://kit.svelte.dev/docs/types#app
// for information about these interfaces
declare global {
	namespace App {
		// interface Error {}
		interface Locals {
			supabase: SupabaseClient
			safeGetSession(): Promise<{ session: Session | null; user?: Session["user"] | null }>
		}
		interface PageData {
			session: Session | null
			user?: Session["user"] | null
		}
		// interface PageState {}
		// interface Platform {}
	}
}
export {};
View source
Create a new src/routes/+layout.server.ts file to handle the session on the server-side.


src/routes/+layout.server.ts
// src/routes/+layout.server.ts
import type { LayoutServerLoad } from './$types'
export const load: LayoutServerLoad = async ({ locals: { safeGetSession }, cookies }) => {
  const { session, user } = await safeGetSession()
  return {
    session,
    user,
    cookies: cookies.getAll(),
  }
}
View source
Start the dev server (npm run dev) to generate the ./$types files we are referencing in our project.

Create a new src/routes/+layout.ts file to handle the session and the supabase object on the client-side.


src/routes/+layout.ts
// src/routes/+layout.ts
import { PUBLIC_SUPABASE_PUBLISHABLE_KEY, PUBLIC_SUPABASE_URL } from '$env/static/public'
import type { LayoutLoad } from './$types'
import { createBrowserClient, createServerClient, isBrowser } from '@supabase/ssr'
export const load: LayoutLoad = async ({ fetch, data, depends }) => {
  depends('supabase:auth')
  const supabase = isBrowser()
    ? createBrowserClient(PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY, {
        global: {
          fetch,
        },
      })
    : createServerClient(PUBLIC_SUPABASE_URL, PUBLIC_SUPABASE_PUBLISHABLE_KEY, {
        global: {
          fetch,
        },
        cookies: {
          getAll() {
            return data.cookies
          },
        },
      })
  /**
   * It's fine to use `getSession` here, because on the client, `getSession` is
   * safe, and on the server, it reads `session` from the `LayoutData`, which
   * safely checked the session using `safeGetSession`.
   */
  const {
    data: { session },
  } = await supabase.auth.getSession()
  return { supabase, session }
}
View source
Create src/routes/+layout.svelte:


src/routes/+layout.svelte
<!-- src/routes/+layout.svelte -->
<script lang="ts">
	import '../styles.css'
	import { invalidate } from '$app/navigation'
	import { onMount } from 'svelte'
	let { data, children } = $props()
	let { supabase, session } = $derived(data)
	onMount(() => {
		const { data } = supabase.auth.onAuthStateChange((event, _session) => {
			if (_session?.expires_at !== session?.expires_at) {
				invalidate('supabase:auth')
			}
		})
		return () => data.subscription.unsubscribe()
	})
</script>
<svelte:head>
	<title>User Management</title>
</svelte:head>
<div class="container" style="padding: 50px 0 100px 0">
	{@render children()}
</div>
View source
Set up a login page#
Create a magic link login/signup page for your application by updating the routes/+page.svelte file:


src/routes/+page.svelte
<!-- src/routes/+page.svelte -->
<script lang="ts">
	import { enhance } from '$app/forms'
	import type { ActionData, SubmitFunction } from './$types.js'
	interface Props {
		form: ActionData
	}
	let { form }: Props = $props()
	let loading = $state(false)
	const handleSubmit: SubmitFunction = () => {
		loading = true
		return async ({ update }) => {
			update()
			loading = false
		}
	}
</script>
<svelte:head>
	<title>User Management</title>
</svelte:head>
<form class="row flex flex-center" method="POST" use:enhance={handleSubmit}>
	<div class="col-6 form-widget">
		<h1 class="header">Supabase + SvelteKit</h1>
		<p class="description">Sign in via magic link with your email below</p>
		{#if form?.message !== undefined}
		<div class="success {form?.success ? '' : 'fail'}">
			{form?.message}
		</div>
		{/if}
		<div>
			<label for="email">Email address</label>
			<input 
				id="email" 
				name="email" 
				class="inputField" 
				type="email" 
				placeholder="Your email" 
				value={form?.email ?? ''} 
			/>
		</div>
		{#if form?.errors?.email}
		<span class="flex items-center text-sm error">
			{form?.errors?.email}
		</span>
		{/if}
		<div>
			<button class="button primary block">
				{ loading ? 'Loading' : 'Send magic link' }
			</button>
		</div>
	</div>
</form>
View source
Create a src/routes/+page.server.ts file that handles the magic link form when submitted.


src/routes/+page.server.ts
// src/routes/+page.server.ts
import { fail, redirect } from '@sveltejs/kit'
import type { Actions, PageServerLoad } from './$types'
export const load: PageServerLoad = async ({ url, locals: { safeGetSession } }) => {
  const { session } = await safeGetSession()
  // if the user is already logged in return them to the account page
  if (session) {
    redirect(303, '/account')
  }
  return { url: url.origin }
}
export const actions: Actions = {
	default: async (event) => {
		const {
			url,
			request,
			locals: { supabase }
		} = event
		const formData = await request.formData()
		const email = formData.get('email') as string
    const validEmail = /^[\w-\.+]+@([\w-]+\.)+[\w-]{2,8}$/.test(email)
    
		if (!validEmail) {
			return fail(400, { errors: { email: "Please enter a valid email address" }, email })
		}
		const { error } = await supabase.auth.signInWithOtp({ email })
		if (error) {
			return fail(400, {
				success: false,
				email,
				message: `There was an issue, Please contact support.`
			})
		}
		return {
			success: true,
			message: 'Please check your email for a magic link to log into the website.'
		}
	}
}
View source
Email template#
Change the email template to support a server-side authentication flow.

Before we proceed, let's change the email template to support sending a token hash:

Go to the Auth > Emails page in the project dashboard.
Select the Confirm signup template.
Change {{ .ConfirmationURL }} to {{ .SiteURL }}/auth/confirm?token_hash={{ .TokenHash }}&type=email.
Repeat the previous step for Magic link template.
Did you know? You can also customize emails sent out to new users, including the email's looks, content, and query parameters. Check out the settings of your project.

Confirmation endpoint#
As this is a server-side rendering (SSR) environment, you need to create a server endpoint responsible for exchanging the token_hash for a session.

The following code snippet performs the following steps:

Retrieves the token_hash sent back from the Supabase Auth server using the token_hash query parameter.
Exchanges this token_hash for a session, which you store in storage (in this case, cookies).
Finally, redirect the user to the account page or the error page.

src/routes/auth/confirm/+server.ts
// src/routes/auth/confirm/+server.js
import type { EmailOtpType } from '@supabase/supabase-js'
import { redirect } from '@sveltejs/kit'
import type { RequestHandler } from './$types'
export const GET: RequestHandler = async ({ url, locals: { supabase } }) => {
  const token_hash = url.searchParams.get('token_hash')
  const type = url.searchParams.get('type') as EmailOtpType | null
  const next = url.searchParams.get('next') ?? '/account'
  /**
   * Clean up the redirect URL by deleting the Auth flow parameters.
   *
   * `next` is preserved for now, because it's needed in the error case.
   */
  const redirectTo = new URL(url)
  redirectTo.pathname = next
  redirectTo.searchParams.delete('token_hash')
  redirectTo.searchParams.delete('type')
  if (token_hash && type) {
    const { error } = await supabase.auth.verifyOtp({ type, token_hash })
    if (!error) {
      redirectTo.searchParams.delete('next')
      redirect(303, redirectTo)
    }
  }
  redirectTo.pathname = '/auth/error'
  redirect(303, redirectTo)
}
View source
Authentication error page#
If there is an error with confirming the token, redirect the user to an error page.


src/routes/auth/error/+page.svelte
<p>Login error</p>
View source
Account page#
After a user signs in, they need to be able to edit their profile details page.
Create a new src/routes/account/+page.svelte file with the content below.


src/routes/account/+page.svelte
<script lang="ts">
	import { enhance } from '$app/forms';
	import type { SubmitFunction } from '@sveltejs/kit';
	// ...
	let { data, form } = $props()
	let { session, supabase, profile } = $derived(data)
	let profileForm: HTMLFormElement
	let loading = $state(false)
	let fullName: string = profile?.full_name ?? ''
	let username: string = profile?.username ?? ''
	let website: string = profile?.website ?? ''
	// ...
	const handleSubmit: SubmitFunction = () => {
		loading = true
		return async () => {
			loading = false
		}
	}
	const handleSignOut: SubmitFunction = () => {
		loading = true
		return async ({ update }) => {
			loading = false
			update()
		}
	}
</script>
<div class="form-widget">
	<form
		class="form-widget"
		method="post"
		action="?/update"
		use:enhance={handleSubmit}
		bind:this={profileForm}
	>
		// ...
		<div>
			<label for="email">Email</label>
			<input id="email" type="text" value={session.user.email} disabled />
		</div>
		<div>
			<label for="fullName">Full Name</label>
			<input id="fullName" name="fullName" type="text" value={form?.fullName ?? fullName} />
		</div>
		<div>
			<label for="username">Username</label>
			<input id="username" name="username" type="text" value={form?.username ?? username} />
		</div>
		<div>
			<label for="website">Website</label>
			<input id="website" name="website" type="url" value={form?.website ?? website} />
		</div>
		<div>
			<input
				type="submit"
				class="button block primary"
				value={loading ? 'Loading...' : 'Update'}
				disabled={loading}
			/>
		</div>
	</form>
	<form method="post" action="?/signout" use:enhance={handleSignOut}>
		<div>
			<button class="button block" disabled={loading}>Sign Out</button>
		</div>
	</form>
</div>
View source
Now, create the associated src/routes/account/+page.server.ts file that handles loading data from the server through the load function
and handle all form actions through the actions object.

src/routes/account/+page.server.ts
import { fail, redirect } from '@sveltejs/kit'
import type { Actions, PageServerLoad } from './$types'
export const load: PageServerLoad = async ({ locals: { supabase, safeGetSession } }) => {
  const { session } = await safeGetSession()
  if (!session) {
    redirect(303, '/')
  }
  const { data: profile } = await supabase
    .from('profiles')
    .select(`username, full_name, website, avatar_url`)
    .eq('id', session.user.id)
    .single()
  return { session, profile }
}
export const actions: Actions = {
  update: async ({ request, locals: { supabase, safeGetSession } }) => {
    const formData = await request.formData()
    const fullName = formData.get('fullName') as string
    const username = formData.get('username') as string
    const website = formData.get('website') as string
    const avatarUrl = formData.get('avatarUrl') as string
    const { session } = await safeGetSession()
    const { error } = await supabase.from('profiles').upsert({
      id: session?.user.id,
      full_name: fullName,
      username,
      website,
      avatar_url: avatarUrl,
      updated_at: new Date(),
    })
    if (error) {
      return fail(500, {
        fullName,
        username,
        website,
        avatarUrl,
      })
    }
    return {
      fullName,
      username,
      website,
      avatarUrl,
    }
  },
  signout: async ({ locals: { supabase, safeGetSession } }) => {
    const { session } = await safeGetSession()
    if (session) {
      await supabase.auth.signOut()
      redirect(303, '/')
    }
  },
}
View source
Launch!#
With all the pages in place, run this command in a terminal:

npm run dev
And then open the browser to localhost:5173 and you should see the completed app.

Supabase Svelte

Bonus: Profile photos#
Every Supabase project is configured with Storage for managing large files like photos and videos.

Create an upload widget#
Create an avatar for the user so that they can upload a profile photo. Start by creating a new component called Avatar.svelte in the src/routes/account directory:


src/routes/account/Avatar.svelte
<!-- src/routes/account/Avatar.svelte -->
<script lang="ts">
	import type { SupabaseClient } from '@supabase/supabase-js'
	interface Props {
		size?: number
		url?: string
		supabase: SupabaseClient
		onupload?: () => void
	}
	let { size = 10, url = $bindable(), supabase, onupload }: Props = $props()
	let avatarUrl: string | null = $state(null)
	let uploading = $state(false)
	let files: FileList = $state()
	const downloadImage = async (path: string) => {
		try {
			const { data, error } = await supabase.storage.from('avatars').download(path)
			if (error) {
				throw error
			}
			const url = URL.createObjectURL(data)
			avatarUrl = url
		} catch (error) {
			if (error instanceof Error) {
				console.log('Error downloading image: ', error.message)
			}
		}
	}
	const uploadAvatar = async () => {
		try {
			uploading = true
			if (!files || files.length === 0) {
				throw new Error('You must select an image to upload.')
			}
			const file = files[0]
			const fileExt = file.name.split('.').pop()
			const filePath = `${Math.random()}.${fileExt}`
			const { error } = await supabase.storage.from('avatars').upload(filePath, file)
			if (error) {
				throw error
			}
			url = filePath
			setTimeout(() => {
				onupload?.()
			}, 100)
		} catch (error) {
			if (error instanceof Error) {
				alert(error.message)
			}
		} finally {
			uploading = false
		}
	}
	$effect(() => {
		if (url) downloadImage(url)
	})
</script>
<div>
	{#if avatarUrl}
		<img
			src={avatarUrl}
			alt={avatarUrl ? 'Avatar' : 'No image'}
			class="avatar image"
			style="height: {size}em; width: {size}em;"
		/>
	{:else}
		<div class="avatar no-image" style="height: {size}em; width: {size}em;"></div>
	{/if}
	<input type="hidden" name="avatarUrl" value={url} />
	<div style="width: {size}em;">
		<label class="button primary block" for="single">
			{uploading ? 'Uploading ...' : 'Upload'}
		</label>
		<input
			style="visibility: hidden; position:absolute;"
			type="file"
			id="single"
			accept="image/*"
			bind:files
			onchange={uploadAvatar}
			disabled={uploading}
		/>
	</div>
</div>
View source
Add the new widget#
Add the widget to the Account page:


src/routes/account/+page.svelte
<script lang="ts">
    // ...
    import Avatar from './Avatar.svelte'
// ...
<div class="form-widget">
        // ...
        <Avatar
            {supabase}
            bind:url={avatarUrl}
            size={10}
            onupload={() => {
                profileForm.requestSubmit();
            }}
        />
// ...
</div>
View source
At this stage you have a fully functional application!

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Project setup
Create a project
Set up the database schema
Get API details
Building the app
Initialize a Svelte app
App styling (optional)
Creating a Supabase client for SSR
Set up a login page
Email template
Confirmation endpoint
Authentication error page
Account page
Launch!
Bonus: Profile photos
Create an upload widget
Add the new widget
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Stylegui
Auth

Use Supabase to authenticate and authorize your users.

Supabase Auth makes it easy to implement authentication and authorization in your app. We provide client SDKs and API endpoints to help you create and manage users.

Your users can use many popular Auth methods, including password, magic link, one-time password (OTP), social login, and single sign-on (SSO).

About authentication and authorization#
Authentication and authorization are the core responsibilities of any Auth system.

Authentication means checking that a user is who they say they are.
Authorization means checking what resources a user is allowed to access.
Supabase Auth uses JSON Web Tokens (JWTs) for authentication. For a complete reference of all JWT fields, see the JWT Fields Reference. Auth integrates with Supabase's database features, making it easy to use Row Level Security (RLS) for authorization.

The Supabase ecosystem#
You can use Supabase Auth as a standalone product, but it's also built to integrate with the Supabase ecosystem.

Auth uses your project's Postgres database under the hood, storing user data and other Auth information in a special schema. You can connect this data to your own tables using triggers and foreign key references.

Auth also enables access control to your database's automatically generated REST API. When using Supabase SDKs, your data requests are automatically sent with the user's Auth Token. The Auth Token scopes database access on a row-by-row level when used along with RLS policies.

Providers#
Supabase Auth works with many popular Auth methods, including Social and Phone Auth using third-party providers. See the following sections for a list of supported third-party providers.

Social Auth#
Apple Icon
Apple
Azure (Microsoft) Icon
Azure (Microsoft)
Bitbucket Icon
Bitbucket
Discord Icon
Discord
Facebook Icon
Facebook
Figma Icon
Figma
GitHub Icon
GitHub
GitLab Icon
GitLab
Google Icon
Google
Kakao Icon
Kakao
Keycloak Icon
Keycloak
LinkedIn Icon
LinkedIn
Notion Icon
Notion
Slack Icon
Slack
Spotify Icon
Spotify
Twitter Icon
Twitter
Twitch Icon
Twitch
WorkOS Icon
WorkOS
Zoom Icon
Zoom
Phone Auth#
MessageBird Icon
MessageBird
Twilio Icon
Twilio
Vonage Icon
Vonage
Pricing#
Charges apply to Monthly Active Users (MAU), Monthly Active Third-Party Users (Third-Party MAU), and Monthly Active SSO Users (SSO MAU) and Advanced MFA Add-ons. For a detailed breakdown of how these charges are calculated, refer to the following pages:

Pricing MAU
Pricing Third-Party MAU
Pricing SSO MAU
Advanced MFA - Phone
Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
About authentication and authorization
The Supabase ecosystem
Providers
Social Auth
Phone Auth
Pricing
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co

Auth
Concepts
Users
Users

A user in Supabase Auth is someone with a user ID, stored in the Auth schema. Once someone is a user, they can be issued an Access Token, which can be used to access Supabase endpoints. The token is tied to the user, so you can restrict access to resources via RLS policies.

Permanent and anonymous users#
Supabase distinguishes between permanent and anonymous users.

Permanent users are tied to a piece of Personally Identifiable Information (PII), such as an email address, a phone number, or a third-party identity. They can use these identities to sign back into their account after signing out.
Anonymous users aren't tied to any identities. They have a user ID and a personalized Access Token, but they have no way of signing back in as the same user if they are signed out.
Anonymous users are useful for:

E-commerce applications, to create shopping carts before checkout
Full-feature demos without collecting personal information
Temporary or throw-away accounts
See the Anonymous Signins guide to learn more about anonymous users.

Anonymous users do not use the anon role
Just like permanent users, anonymous users use the authenticated role for database access.

The anon role is for those who aren't signed in at all and are not tied to any user ID. We refer to these as unauthenticated or public users.

The user object#
The user object stores all the information related to a user in your application. The user object can be retrieved using one of these methods:

supabase.auth.getUser()
Retrieve a user object as an admin using supabase.auth.admin.getUserById()
A user can sign in with one of the following methods:

Password-based method (with email or phone)
Passwordless method (with email or phone)
OAuth
SAML SSO
An identity describes the authentication method that a user can use to sign in. A user can have multiple identities. These are the types of identities supported:

Email
Phone
OAuth
SAML
A user with an email or phone identity will be able to sign in with either a password or passwordless method (e.g. use a one-time password (OTP) or magic link). By default, a user with an unverified email or phone number will not be able to sign in.

The user object contains the following attributes:

Attributes	Type	Description
id	string	The unique id of the identity of the user.
aud	string	The audience claim.
role	string	The role claim used by Postgres to perform Row Level Security (RLS) checks.
email	string	The user's email address.
email_confirmed_at	string	The timestamp that the user's email was confirmed. If null, it means that the user's email is not confirmed.
phone	string	The user's phone number.
phone_confirmed_at	string	The timestamp that the user's phone was confirmed. If null, it means that the user's phone is not confirmed.
confirmed_at	string	The timestamp that either the user's email or phone was confirmed. If null, it means that the user does not have a confirmed email address and phone number.
last_sign_in_at	string	The timestamp that the user last signed in.
app_metadata	object	The provider attribute indicates the first provider that the user used to sign up with. The providers attribute indicates the list of providers that the user can use to login with.
user_metadata	object	Defaults to the first provider's identity data but can contain additional custom user metadata if specified. Refer to User Identity for more information about the identity object. Don't rely on the order of information in this field. Do not use it in security sensitive context (such as in RLS policies or authorization logic), as this value is editable by the user without any checks.
identities	UserIdentity[]	Contains an object array of identities linked to the user.
created_at	string	The timestamp that the user was created.
updated_at	string	The timestamp that the user was last updated.
is_anonymous	boolean	Is true if the user is an anonymous user.
Resources#
User Management guide
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Permanent and anonymous users
The user object
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co

Auth
Concepts
Identities
Identities

An identity is an authentication method associated with a user. Supabase Auth supports the following types of identity:

Email
Phone
OAuth
SAML
A user can have more than one identity. Anonymous users have no identity until they link an identity to their user.

The user identity object#
The user identity object contains the following attributes:

Attributes	Type	Description
provider_id	string	The provider id returned by the provider. If the provider is an OAuth provider, the id refers to the user's account with the OAuth provider. If the provider is email or phone, the id is the user's id from the auth.users table.
user_id	string	The user's id that the identity is linked to.
identity_data	object	The identity metadata. For OAuth and SAML identities, this contains information about the user from the provider.
id	string	The unique id of the identity.
provider	string	The provider name.
email	string	The email is a generated column that references the optional email property in the identity_data
created_at	string	The timestamp that the identity was created.
last_sign_in_at	string	The timestamp that the identity was last used to sign in.
updated_at	string	The timestamp that the identity was last updated.
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
The user identity object
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co

Auth

More
Sessions
User sessions
User sessions

Supabase Auth provides fine-grained control over your user's sessions.

Some security sensitive applications, or those that need to be SOC 2, HIPAA, PCI-DSS or ISO27000 compliant will require some sort of additional session controls to enforce timeouts or provide additional security guarantees. Supabase Auth makes it easy to build compliant applications.

What is a session?#
A session is created when a user signs in. By default, it lasts indefinitely and a user can have an unlimited number of active sessions on as many devices.

A session is represented by the Supabase Auth access token in the form of a JWT, and a refresh token which is a unique string.

Access tokens are designed to be short lived, usually between 5 minutes and 1 hour while refresh tokens never expire but can only be used once. You can exchange a refresh token only once to get a new access and refresh token pair.

This process is called refreshing the session.

A session terminates, depending on configuration, when:

The user clicks sign out.
The user changes their password or performs a security sensitive action.
It times out due to inactivity.
It reaches its maximum lifetime.
A user signs in on another device.
Access token (JWT) claims#
Every access token contains a session_id claim, a UUID, uniquely identifying the session of the user. You can correlate this ID with the primary key of the auth.sessions table.

Initiating a session#
A session is initiated when a user signs in. The session is stored in the auth.sessions table, and your app should receive the access and refresh tokens.

There are two flows for initiating a session and receiving the tokens:

Implicit flow
PKCE flow
Limiting session lifetime and number of allowed sessions per user#
This feature is only available on Pro Plans and up.

Supabase Auth can be configured to limit the lifetime of a user's session. By default, all sessions are active until the user signs out or performs some other action that terminates a session.

In some applications, it's useful or required for security to ensure that users authenticate often, or that sessions are not left active on devices for too long.

There are three ways to limit the lifetime of a session:

Time-boxed sessions, which terminate after a fixed amount of time.
Set an inactivity timeout, which terminates sessions that haven't been refreshed within the timeout duration.
Enforce a single-session per user, which only keeps the most recently active session.
To make sure that users are required to re-authenticate periodically, you can set a positive value for the Time-box user sessions option in the Auth settings for your project.

To make sure that sessions expire after a period of inactivity, you can set a positive duration for the Inactivity timeout option in the Auth settings.

You can also enforce only one active session per user per device or browser. When this is enabled, the session from the most recent sign in will remain active, while the rest are terminated. Enable this via the Single session per user option in the Auth settings.

Sessions are not proactively destroyed when you change these settings, but rather the check is enforced whenever a session is refreshed next. This can confuse developers because the actual duration of a session is the configured timeout plus the JWT expiration time. For single session per user, the effect will only be noticed at intervals of the JWT expiration time. Make sure you adjust this setting depending on your needs. We do not recommend going below 5 minutes for the JWT expiration time.

Otherwise sessions are progressively deleted from the database 24 hours after they expire, which prevents you from causing a high load on your project by accident and allows you some freedom to undo changes without adversely affecting all users.

Frequently asked questions#
What are recommended values for access token (JWT) expiration?#
Most applications should use the default expiration time of 1 hour. This can be customized in your project's Auth settings in the Advanced Settings section.

Setting a value over 1 hour is generally discouraged for security reasons, but it may make sense in certain situations.

Values below 5 minutes, and especially below 2 minutes, should not be used in most situations because:

The shorter the expiration time, the more frequently refresh tokens are used, which increases the load on the Auth server.
Time is not absolute. Servers can often be off sync for tens of seconds, but user devices like laptops, desktops or mobile devices can sometimes be off by minutes or even hours. Having too short expiration time can cause difficult-to-debug errors due to clock skew.
Supabase's client libraries always try to refresh the session ahead of time, which won't be possible if the expiration time is too short.
Access tokens should generally be valid for at least as long as the longest running request in your application. This helps you avoid issues where the access token becomes invalid midway through processing.
What is refresh token reuse detection and what does it protect from?#
As your users continue using your app, refresh tokens are being constantly exchanged for new access tokens.

The general rule is that a refresh token can only be used once. However, strictly enforcing this can cause certain issues to arise. There are two exceptions to this design to prevent the early and unexpected termination of user's sessions:

A refresh token can be used more than once within a defined reuse interval. By default this is 10 seconds and we do not recommend changing this value. This exception is granted for legitimate situations such as:
Using server-side rendering where the same refresh token needs to be reused on the server and soon after on the client
To allow some leeway for bugs or issues with serializing access to the refresh token request
If the parent of the currently active refresh token for the user's session is being used, the active token will be returned. This exception solves an important and often common situation:
All clients such as browsers, mobile or desktop apps, and even some servers are inherently unreliable due to network issues. A request does not indicate that they received a response or even processed the response they received.
If a refresh token is revoked after being used only once, and the response wasn't received and processed by the client, when the client comes back online, it will attempt to use the refresh token that was already used. Since this might happen outside of the reuse interval, it can cause sudden and unexpected session termination.
Should the reuse attempt not fall under these two exceptions, the whole session is regarded as terminated and all refresh tokens belonging to it are marked as revoked. You can disable this behavior in the Advanced Settings of the Auth settings page, though it is generally not recommended.

The purpose of this mechanism is to guard against potential security issues where a refresh token could have been stolen from the user, for example by exposing it accidentally in logs that leak (like logging cookies, request bodies or URL params) or via vulnerable third-party servers. It does not guard against the case where a user's session is stolen from their device.

What are the benefits of using access and refresh tokens instead of traditional sessions?#
Traditionally user sessions were implemented by using a unique string stored in cookies that identified the authorization that the user had on a specific browser. Applications would use this unique string to constantly fetch the attached user information on every API call.

This approach has some tradeoffs compared to using a JWT-based approach:

If the authentication server or its database crashes or is unavailable for even a few seconds, the whole application goes down. Scheduling maintenance or dealing with transient errors becomes very challenging.
A failing authentication server can cause a chain of failures across other systems and APIs, paralyzing the whole application system.
All requests that require authentication has to be routed through the authentication, which adds an additional latency overhead to all requests.
Supabase Auth prefers a JWT-based approach using access and refresh tokens because session information is encoded within the short-lived access token, enabling transfer across APIs and systems without dependence on a central server's availability or performance. This approach enhances an application's tolerance to transient failures or performance issues. Furthermore, proactively refreshing the access token allows the application to function reliably even during significant outages.

It's better for cost optimization and scaling as well, as the authentication system's servers and database only handle traffic for this use case.

How to ensure an access token (JWT) cannot be used after a user signs out#
Most applications rarely need such strong guarantees. Consider adjusting the JWT expiry time to an acceptable value. If this is still necessary, you should try to use this validation logic only for the most sensitive actions within your application.

When a user signs out, the sessions affected by the logout are removed from the database entirely. You can check that the session_id claim in the JWT corresponds to a row in the auth.sessions table. If such a row does not exist, it means that the user has logged out.

Note that sessions are not proactively terminated when their maximum lifetime (time-box) or inactivity timeout are reached. These sessions are cleaned up progressively 24 hours after reaching that status. This allows you to tweak the values or roll back changes without causing unintended user friction.

Using HTTP-only cookies to store access and refresh tokens#
This is possible, but only for apps that use the traditional server-only web app approach where all of the application logic is implemented on the server and it returns rendered HTML only.

If your app uses any client side JavaScript to build a rich user experience, using HTTP-Only cookies is not feasible since only your server will be able to read and refresh the session of the user. The browser will not have access to the access and refresh tokens.

Because of this, the Supabase JavaScript libraries provide only limited support. You can override the storage option when creating the Supabase client on the server to store the values in cookies or your preferred storage choice, for example:

import { createClient } from '@supabase/supabase-js'
const supabase = createClient('SUPABASE_URL', 'SUPABASE_PUBLISHABLE_KEY', {
  auth: {
    storage: {
      getItem: () => {
        return Promise.resolve('FETCHED_COOKIE')
      },
      setItem: () => {},
      removeItem: () => {},
    },
  },
})
The customStorageObject should implement the getItem, setItem, and removeItem methods from the Storage interface. Async versions of these methods are also supported.

When using cookies to store access and refresh tokens, make sure that the Expires or Max-Age attributes of the cookies is set to a timestamp very far into the future. Browsers will clear the cookies, but the session will remain active in Supabase Auth. Therefore it's best to let Supabase Auth control the validity of these tokens and instruct the browser to always store the cookies indefinitely.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
What is a session?
Access token (JWT) claims
Initiating a session
Limiting session lifetime and number of allowed sessions per user
Frequently asked questions
What are recommended values for access token (JWT) expiration?
What is refresh token reuse detection and what does it protect from?
What are the benefits of using access and refresh tokens instead of traditional sessions?
How to ensure an access token (JWT) cannot be used after a user signs out
Using HTTP-only cookies to store access and refresh tokens
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co
Auth

More
Sessions
Implicit flow
Implicit flow

About authenticating with implicit flow.

The implicit flow is one of two ways that a user can authenticate and your app can receive the necessary access and refresh tokens.

The flow is an implementation detail handled for you by Supabase Auth, but understanding the difference between implicit and PKCE flow is important for understanding the difference between client-only and server-side auth.

How it works#
After a successful signin, the user is redirected to your app with a URL that looks like this:

https://yourapp.com/...#access_token=<...>&refresh_token=<...>&...
The access and refresh tokens are contained in the URL fragment.

The client libraries:

Detect this type of URL
Extract the access token, refresh token, and some extra information
Persist this information to local storage for further use by the library and your app
Limitations#
The implicit flow only works on the client. Web browsers do not send the URL fragment to the server by design. This is a security feature:

You may be hosting your single-page app on a third-party server. The third-party service shouldn't get access to your user's credentials.
Even if the server is under your direct control, GET requests and their full URLs are often logged. This approach avoids leaking credentials in request or access logs.
If you wish to obtain the access token and refresh token on a server, use the PKCE flow.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How it works
Limitations
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co
/
Auth

More
Sessions
PKCE flow
PKCE flow

About authenticating with PKCE flow.

The Proof Key for Code Exchange (PKCE) flow is one of two ways that a user can authenticate and your app can receive the necessary access and refresh tokens.

The flow is an implementation detail handled for you by Supabase Auth, but understanding the difference between PKCE and implicit flow is important for understanding the difference between client-only and server-side auth.

How it works#
After a successful verification, the user is redirected to your app with a URL that looks like this:

https://yourapp.com/...?code=<...>
The code parameter is commonly known as the Auth Code and can be exchanged for an access token by calling exchangeCodeForSession(code).

For security purposes, the code has a validity of 5 minutes and can only be exchanged for an access token once. You will need to restart the authentication flow from scratch if you wish to obtain a new access token.

As the flow is run server side, localStorage may not be available. You may configure the client library to use a custom storage adapter and an alternate backing storage such as cookies by setting the storage option to an object with the following methods:

const customStorageAdapter: SupportedStorage = {
    getItem: (key) => {
    if (!supportsLocalStorage()) {
        // Configure alternate storage
        return null
    }
    return globalThis.localStorage.getItem(key)
    },
    setItem: (key, value) => {
    if (!supportsLocalStorage()) {
        // Configure alternate storage here
        return
    }
    globalThis.localStorage.setItem(key, value)
    },
    removeItem: (key) => {
    if (!supportsLocalStorage()) {
        // Configure alternate storage here
        return
    }
    globalThis.localStorage.removeItem(key)
    },
}
You may also configure the client library to automatically exchange it for a session after a successful redirect. This can be done by setting the detectSessionInUrl option to true.

Putting it all together, your client library initialization may look like this:

const supabase = createClient('https://xyzcompany.supabase.co', 'publishable-or-anon-key', {
  // ...
  auth: {
    // ...
    detectSessionInUrl: true,
    flowType: 'pkce',
    storage: {
      getItem: () => Promise.resolve('FETCHED_TOKEN'),
      setItem: () => {},
      removeItem: () => {},
    },
  },
  // ...
})
Limitations#
Behind the scenes, the code exchange requires a code verifier. Both the code in the URL and the code verifier are sent back to the Auth server for a successful exchange.

The code verifier is created and stored locally when the Auth flow is first initiated. That means the code exchange must be initiated on the same browser and device where the flow was started.

Resources#
OAuth 2.0 guide to PKCE flow
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How it works
Limitations
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co

Auth

More
Server-Side Rendering
Creating a client
Creating a Supabase client for SSR

Configure your Supabase client to use cookies

To use Server-Side Rendering (SSR) with Supabase, you need to configure your Supabase client to use cookies. The @supabase/ssr package helps you do this for JavaScript/TypeScript applications.

Install#
Install the @supabase/supabase-js and @supabase/ssr helper packages:


npm

yarn

pnpm
npm install @supabase/supabase-js @supabase/ssr
Set environment variables#
Create a .env.local file in the project root directory. In the file, set the project's Supabase URL and Key:

Project URL
No project found
YOUR PROJECT URL

To get your Project URL, log in.

Publishable key
No project found
YOUR PUBLISHABLE KEY

To get your Publishable key, log in.

Anon key
No project found
YOUR ANON KEY

To get your Anon key, log in.


Next.js

SvelteKit

Astro

Remix

React Router

Express

Hono
NEXT_PUBLIC_SUPABASE_URL=supabase_project_url
NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY=supabase_publishable_key
Create a client#
You need setup code to configure a Supabase client to use cookies. Once you have the utility code, you can use the createClient utility functions to get a properly configured Supabase client.

Use the browser client in code that runs on the browser, and the server client in code that runs on the server.


Next.js

SvelteKit

Astro

Remix

React Router

Express

Hono
Write utility functions to create Supabase clients#
To access Supabase from a Next.js app, you need 2 types of Supabase clients:

Client Component client - To access Supabase from Client Components, which run in the browser.
Server Component client - To access Supabase from Server Components, Server Actions, and Route Handlers, which run only on the server.
Since Next.js Server Components can't write cookies, you need middleware to refresh expired Auth tokens and store them.

The middleware is responsible for:

Refreshing the Auth token by calling supabase.auth.getClaims().
Passing the refreshed Auth token to Server Components, so they don't attempt to refresh the same token themselves. This is accomplished with request.cookies.set.
Passing the refreshed Auth token to the browser, so it replaces the old token. This is accomplished with response.cookies.set.

What does the `cookies` object do?

Do I need to create a new client for every route?
Create a utils/supabase folder at the root of your project, or inside the ./src folder if you are using one, with a file for each type of client. Then copy the utility functions for each client type.


utils/supabase/client.ts

utils/supabase/server.ts
import { createBrowserClient } from '@supabase/ssr'
export function createClient() {
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY!
  )
}
View source
Hook up middleware#
The code adds a matcher so the middleware doesn't run on routes that don't access Supabase.

Be careful when protecting pages. The server gets the user session from the cookies, which can be spoofed by anyone.

Always use supabase.auth.getClaims() to protect pages and user data.

Never trust supabase.auth.getSession() inside server code such as middleware. It isn't guaranteed to revalidate the Auth token.

It's safe to trust getClaims() because it validates the JWT signature against the project's published public keys every time.


middleware.ts

utils/supabase/middleware.ts
import { type NextRequest } from "next/server"
import { updateSession } from "@/utils/supabase/middleware"
export async function middleware(request: NextRequest) {
  return await updateSession(request)
}
export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * Feel free to modify this pattern to include more paths.
     */
    "/((?!_next/static|_next/image|favicon.ico|.*\\.(?:svg|png|jpg|jpeg|gif|webp)$).*)",
  ],
}
View source
Congratulations#
You're done! To recap, you've successfully:

Called Supabase from a Server Action.
Called Supabase from a Server Component.
Set up a Supabase client utility to call Supabase from a Client Component. You can use this if you need to call Supabase from a Client Component, for example to set up a realtime subscription.
Set up middleware to automatically refresh the Supabase Auth session.
You can now use any Supabase features from your client or server code!

Next steps#
Implement Authentication using Email and Password
Implement Authentication using OAuth
Learn more about SSR
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Install
Set environment variables
Create a client
Write utility functions to create Supabase clients
Hook up middleware
Congratulations
Next steps
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Co

Supabase wordmark
DOCS
Start
Products
Build
Manage
Reference
Resources

Search
docs...

K
Sign up

Main menu
Auth
Overview
Architecture
Getting Started
Next.js
React
React Native
React Native with Expo & Social Auth
Concepts
Users
Identities

Sessions
User sessions
Implicit flow
PKCE flow
Flows (How-tos)

Server-Side Rendering
Creating a client
Migrating from Auth Helpers
Advanced guide
Password-based
Email (Magic Link or OTP)
Phone Login

Social Login (OAuth)

Enterprise SSO
Anonymous Sign-Ins
Web3 (Ethereum or Solana)
Mobile Deep Linking
Identity Linking

Multi-Factor Authentication
Signout
Debugging
Error Codes
Troubleshooting
Third-party auth
Overview
Clerk
Firebase Auth
Auth0
AWS Cognito (Amplify)
WorkOS
Configuration
General Configuration
Email Templates
Redirect URLs

Auth Hooks
Custom SMTP
User Management
Security
Password Security
Rate Limits
Bot Detection (CAPTCHA)
Audit Logs

JSON Web Tokens (JWT)
JWT Signing Keys
Row Level Security
Column Level Security
Custom Claims & RBAC
Auth UI
Auth UI (Deprecated)
Flutter Auth UI
Auth

More
Server-Side Rendering
Migrating from Auth Helpers
Migrating to the SSR package from Auth Helpers

The new ssr package takes the core concepts of the Auth Helpers and makes them available to any server language or framework. This page will guide you through migrating from the Auth Helpers package to ssr.

Replacing Supabase packages#

Next.js

SvelteKit

Remix
npm uninstall @supabase/auth-helpers-sveltekit
npm install @supabase/ssr
Creating a client#
The new ssr package exports two functions for creating a Supabase client. The createBrowserClient function is used in the client, and the createServerClient function is used in the server.

Check out the Creating a client page for examples of creating a client in your framework.

Next steps#
Implement Authentication using Email and Password
Implement Authentication using OAuth
Learn more about SSR
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Replacing Supabase packages
Creating a client
Next steps
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author Styleguide
Open Source
SupaSquad
Privacy Settings
GitHub
Twitter
Discord
Migrating to the SSR package from Auth Helpers | Supabase Docs

Advanced guide

Details about SSR Auth flows and implementation for advanced users.

When a user authenticates with Supabase Auth, two pieces of information are issued by the server:

Access token in the form of a JWT.
Refresh token which is a randomly generated string.
The default behavior if you're not using SSR is to store this information in local storage. Local storage isn't accessible by the server, so for SSR, the tokens instead need to be stored in a secure cookie. The cookie can then be passed back and forth between your app code in the client and your app code in the server.

If you're not using SSR, you might also be using the implicit flow to get the access and refresh tokens. The server can't access the tokens in this flow, so for SSR, you should change to the PKCE flow. You can change the flow type when initiating your Supabase client if your client library provides this option.

In the @supabase/ssr package, Supabase clients are initiated to use the PKCE flow by default. They are also automatically configured to handle the saving and retrieval of session information in cookies.

How it works#
In the PKCE flow, a redirect is made to your app, with an Auth Code contained in the URL. When you exchange this code using exchangeCodeForSession, you receive the session information, which contains the access and refresh tokens.

To maintain the session, these tokens must be stored in a storage medium securely shared between client and server, which is traditionally cookies. Whenever the session is refreshed, the auth and refresh tokens in the shared storage medium must be updated. Supabase client libraries provide a customizable storage option when a client is initiated, allowing you to change where tokens are stored.

Frequently asked questions#
No session on the server side with Next.js route prefetching?#
When you use route prefetching in Next.js using <Link href="/..."> components or the Router.push() APIs can send server-side requests before the browser processes the access and refresh tokens. This means that those requests may not have any cookies set and your server code will render unauthenticated content.

To improve experience for your users, we recommend redirecting users to one specific page after sign-in that does not include any route prefetching from Next.js. Once the Supabase client library running in the browser has obtained the access and refresh tokens from the URL fragment, you can send users to any pages that use prefetching.

How do I make the cookies HttpOnly?#
This is not necessary. Both the access token and refresh token are designed to be passed around to different components in your application. The browser-based side of your application needs access to the refresh token to properly maintain a browser session anyway.

My server is getting invalid refresh token errors. What's going on?#
It is likely that the refresh token sent from the browser to your server is stale. Make sure the onAuthStateChange listener callback is free of bugs and is registered relatively early in your application's lifetime

When you receive this error on the server-side, try to defer rendering to the browser where the client library can access an up-to-date refresh token and present the user with a better experience.

Should I set a shorter Max-Age parameter on the cookies?#
The Max-Age or Expires cookie parameters only control whether the browser sends the value to the server. Since a refresh token represents the long-lived authentication session of the user on that browser, setting a short Max-Age or Expires parameter on the cookies only results in a degraded user experience.

The only way to ensure that a user has logged out or their session has ended is to get the user's details with getUser(). The getClaims() method only checks local JWT validation (signature and expiration), but it doesn't verify with the auth server whether the session is still valid or if the user has logged out server-side.

What should I use for the SameSite property?#
Make sure you understand the behavior of the property in different situations as some properties can degrade the user experience.

A good default is to use Lax which sends cookies when users are navigating to your site. Cookies typically require the Secure attribute, which only sends them over HTTPS. However, this can be a problem when developing on localhost.

Can I use server-side rendering with a CDN or cache?#
Yes, but you need to be careful to include at least the refresh token cookie value in the cache key. Otherwise you may be accidentally serving pages with data belonging to different users!

Also be sure you set proper cache control headers. We recommend invalidating cache keys every hour or less.

Which authentication flows have PKCE support?#
At present, PKCE is supported on the Magic Link, OAuth, Sign Up, and Password Recovery routes. These correspond to the signInWithOtp, signInWithOAuth, signUp, and resetPasswordForEmail methods on the Supabase client library. When using PKCE with Phone and Email OTPs, there is no behavior change with respect to the implicit flow - an access token will be returned in the body when a request is successful.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How it works
Frequently asked questions
No session on the server side with Next.js route prefetching?
How do I make the cookies HttpOnly?
My server is getting invalid refresh token errors. What's going on?
Should I set a shorter Max-Age parameter on the cookies?
What should I use for the SameSite property?
Can I use server-side rendering with a CDN or cache?
Which authentication flows have PKCE support?
Need some help?

Contact support
Latest product updates?

See Changelog
Auth
Flows (How-tos)
Password-based
Password-based Auth

Allow users to sign in with a password connected to their email or phone number.

Users often expect to sign in to your site with a password. Supabase Auth helps you implement password-based auth safely, using secure configuration options and best practices for storing and verifying passwords.

Users can associate a password with their identity using their email address or a phone number.

With email#
Enabling email and password-based authentication#
Email authentication is enabled by default.

You can configure whether users need to verify their email to sign in. On hosted Supabase projects, this is true by default. On self-hosted projects or in local development, this is false by default.

Change this setting on the Auth Providers page for hosted projects, or in the configuration file for self-hosted projects.

Signing up with an email and password#
There are two possible flows for email signup: implicit flow and PKCE flow. If you're using SSR, you're using the PKCE flow. If you're using client-only code, the default flow depends upon the client library. The implicit flow is the default in JavaScript and Dart, and the PKCE flow is the default in Swift.

The instructions in this section assume that email confirmations are enabled.


Implicit flow

PKCE flow
The implicit flow only works for client-only apps. Your site directly receives the access token after the user confirms their email.


JavaScript

Dart

Swift

Kotlin

Python
To sign up the user, call signUp() with their email address and password.

You can optionally specify a URL to redirect to after the user clicks the confirmation link. This URL must be configured as a Redirect URL, which you can do in the dashboard for hosted projects, or in the configuration file for self-hosted projects.

If you don't specify a redirect URL, the user is automatically redirected to your site URL. This defaults to localhost:3000, but you can also configure this.

async function signUpNewUser() {
  const { data, error } = await supabase.auth.signUp({
    email: 'valid.email@supabase.io',
    password: 'example-password',
    options: {
      emailRedirectTo: 'https://example.com/welcome',
    },
  })
}
Signing in with an email and password#

JavaScript

Dart

Swift

Kotlin

Python
When your user signs in, call signInWithPassword() with their email address and password:

async function signInWithEmail() {
  const { data, error } = await supabase.auth.signInWithPassword({
    email: 'valid.email@supabase.io',
    password: 'example-password',
  })
}
Resetting a password#

Implicit flow

PKCE flow
Step 1: Create a reset password page#
Create a reset password page. This page should be publicly accessible.

Collect the user's email address and request a password reset email. Specify the redirect URL, which should point to the URL of a change password page. This URL needs to be configured in your redirect URLs.


JavaScript

Swift

Kotlin

Python

Dart
await supabase.auth.resetPasswordForEmail('valid.email@supabase.io', {
  redirectTo: 'http://example.com/account/update-password',
})
Step 2: Create a change password page#
Create a change password page at the URL you specified in the previous step. This page should be accessible only to authenticated users.

Collect the user's new password and call updateUser to update their password.


JavaScript

Swift

Kotlin

Python

Dart
await supabase.auth.updateUser({ password: 'new_password' })
Email sending#
The signup confirmation and password reset flows require an SMTP server to send emails.

The Supabase platform comes with a default email-sending service for you to try out. The service has a rate limit of 2 emails per hour, and availability is on a best-effort basis. For production use, you should consider configuring a custom SMTP server.

Consider configuring a custom SMTP server for production.

See the Custom SMTP guide for instructions.

Local development with Mailpit#
You can test email flows on your local machine. The Supabase CLI automatically captures emails sent locally by using Mailpit.

In your terminal, run supabase status to get the Mailpit URL. Go to this URL in your browser, and follow the instructions to find your emails.

With phone#
You can use a user's mobile phone number as an identifier, instead of an email address, when they sign up with a password.

This practice is usually discouraged because phone networks recycle mobile phone numbers. Anyone receiving a recycled phone number gets access to the original user's account. To mitigate this risk, implement MFA.

Protect users who use a phone number as a password-based auth identifier by enabling MFA.

Enabling phone and password-based authentication#
Enable phone authentication on the Auth Providers page for hosted Supabase projects.

For self-hosted projects or local development, use the configuration file. See the configuration variables namespaced under auth.sms.

If you want users to confirm their phone number on signup, you need to set up an SMS provider. Each provider has its own configuration. Supported providers include MessageBird, Twilio, Vonage, and TextLocal (community-supported).

Configuring SMS Providers

MessageBird Icon
MessageBird

Twilio Icon
Twilio

Vonage Icon
Vonage

Textlocal (Community Supported) Icon
Textlocal (Community Supported)
Signing up with a phone number and password#
To sign up the user, call signUp() with their phone number and password:


JavaScript

Swift

Kotlin

Python

Dart

HTTP
const { data, error } = await supabase.auth.signUp({
  phone: '+13334445555',
  password: 'some-password',
})
If you have phone verification turned on, the user receives an SMS with a 6-digit pin that you must verify within 60 seconds:


JavaScript

Swift

Kotlin

Python

Dart

HTTP
You should present a form to the user so they can input the 6 digit pin, then send it along with the phone number to verifyOtp:

const {
  data: { session },
  error,
} = await supabase.auth.verifyOtp({
  phone: '+13334445555',
  token: '123456',
  type: 'sms',
})
Signing in a with a phone number and password#
Call the function to sign in with the user's phone number and password:


JavaScript

Swift

Kotlin

Python

Dart

HTTP
const { data, error } = await supabase.auth.signInWithPassword({
  phone: '+13334445555',
  password: 'some-password',
})
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
With email
Enabling email and password-based authentication
Signing up with an email and password
Signing in with an email and password
Resetting a password
Email sending
With phone
Enabling phone and password-based authentication
Signing up with a phone number and password
Signing in a with a phone number and password
Need some help?

Contact support
Latest product updates?

See Changelog

Auth
Flows (How-tos)
Email (Magic Link or OTP)
Passwordless email logins

Email logins using Magic Links or One-Time Passwords (OTPs)

Supabase Auth provides several passwordless login methods. Passwordless logins allow users to sign in without a password, by clicking a confirmation link or entering a verification code.

Passwordless login can:

Improve the user experience by not requiring users to create and remember a password
Increase security by reducing the risk of password-related security breaches
Reduce support burden of dealing with password resets and other password-related flows
Supabase Auth offers two passwordless login methods that use the user's email address:

Magic Link
OTP
With Magic Link#
Magic Links are a form of passwordless login where users click on a link sent to their email address to log in to their accounts. Magic Links only work with email addresses and are one-time use only.

Enabling Magic Link#
Email authentication methods, including Magic Links, are enabled by default.

Configure the Site URL and any additional redirect URLs. These are the only URLs that are allowed as redirect destinations after the user clicks a Magic Link. You can change the URLs on the URL Configuration page for hosted projects, or in the configuration file for self-hosted projects.

By default, a user can only request a magic link once every 60 seconds and they expire after 1 hour.

Signing in with Magic Link#
Call the "sign in with OTP" method from the client library.

Though the method is labelled "OTP", it sends a Magic Link by default. The two methods differ only in the content of the confirmation email sent to the user.

If the user hasn't signed up yet, they are automatically signed up by default. To prevent this, set the shouldCreateUser option to false.


JavaScript

Expo React Native

Dart

Swift

Kotlin

Python
async function signInWithEmail() {
  const { data, error } = await supabase.auth.signInWithOtp({
    email: 'valid.email@supabase.io',
    options: {
      // set this to false if you do not want the user to be automatically signed up
      shouldCreateUser: false,
      emailRedirectTo: 'https://example.com/welcome',
    },
  })
}
That's it for the implicit flow.

If you're using PKCE flow, edit the Magic Link email template to send a token hash:

<h2>Magic Link</h2>
<p>Follow this link to login:</p>
<p><a href="{{ .SiteURL }}/auth/confirm?token_hash={{ .TokenHash }}&type=email">Log In</a></p>
At the /auth/confirm endpoint, exchange the hash for the session:

const { error } = await supabase.auth.verifyOtp({
  token_hash: 'hash',
  type: 'email',
})
With OTP#
Email one-time passwords (OTP) are a form of passwordless login where users key in a six digit code sent to their email address to log in to their accounts.

Enabling email OTP#
Email authentication methods, including Email OTPs, are enabled by default.

Email OTPs share an implementation with Magic Links. To send an OTP instead of a Magic Link, alter the Magic Link email template. For a hosted Supabase project, go to Email Templates in the Dashboard. For a self-hosted project or local development, see the Email Templates guide.

Modify the template to include the {{ .Token }} variable, for example:

<h2>One time login code</h2>
<p>Please enter this code: {{ .Token }}</p>
By default, a user can only request an OTP once every 60 seconds and they expire after 1 hour. This is configurable via Auth > Providers > Email > Email OTP Expiration. An expiry duration of more than 86400 seconds (one day) is disallowed to guard against brute force attacks. The longer an OTP remains valid, the more time an attacker has to attempt brute force attacks. If the OTP is valid for several days, an attacker might have more opportunities to guess the correct OTP through repeated attempts.

Signing in with email OTP#
Step 1: Send the user an OTP code#
Get the user's email and call the "sign in with OTP" method from your client library.

If the user hasn't signed up yet, they are automatically signed up by default. To prevent this, set the shouldCreateUser option to false.


JavaScript

Dart

Swift

Kotlin

Python
const { data, error } = await supabase.auth.signInWithOtp({
  email: 'valid.email@supabase.io',
  options: {
    // set this to false if you do not want the user to be automatically signed up
    shouldCreateUser: false,
  },
})
If the request is successful, you receive a response with error: null and a data object where both user and session are null. Let the user know to check their email inbox.

{
  "data": {
    "user": null,
    "session": null
  },
  "error": null
}
Step 2: Verify the OTP to create a session#
Provide an input field for the user to enter their one-time code.

Call the "verify OTP" method from your client library with the user's email address, the code, and a type of email:


JavaScript

Swift

Kotlin

Python
const {
  data: { session },
  error,
} = await supabase.auth.verifyOtp({
  email: 'email@example.com',
  token: '123456',
  type: 'email',
})
If successful, the user is now logged in, and you receive a valid session that looks like:

{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhdXRoZW50aWNhdGVkIiwiZXhwIjoxNjI3MjkxNTc3LCJzdWIiOiJmYTA2NTQ1Zi1kYmI1LTQxY2EtYjk1NC1kOGUyOTg4YzcxOTEiLCJlbWFpbCI6IiIsInBob25lIjoiNjU4NzUyMjAyOSIsImFwcF9tZXRhZGF0YSI6eyJwcm92aWRlciI6InBob25lIn0sInVzZXJfbWV0YWRhdGEiOnt9LCJyb2xlIjoiYXV0aGVudGljYXRlZCJ9.1BqRi0NbS_yr1f6hnr4q3s1ylMR3c1vkiJ4e_N55dhM",
  "token_type": "bearer",
  "expires_in": 3600,
  "refresh_token": "LSp8LglPPvf0DxGMSj-vaQ",
  "user": {...}
}
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
With Magic Link
Enabling Magic Link
Signing in with Magic Link
With OTP
Enabling email OTP
Signing in with email OTP
Need some help?

Contact support
Latest product updates?

See Changelog

Auth
Flows (How-tos)
Phone Login
Phone Login

Phone Login is a method of authentication that allows users to log in to a website or application without using a password. The user authenticates through a one-time password (OTP) sent via a channel (SMS or WhatsApp).

At this time, WhatsApp is only supported as a channel for the Twilio and Twilio Verify Providers.

Users can also log in with their phones using Native Mobile Login with the built-in identity provider. For Native Mobile Login with Android and iOS, see the Social Login guides.

Phone OTP login can:

Improve the user experience by not requiring users to create and remember a password
Increase security by reducing the risk of password-related security breaches
Reduce support burden of dealing with password resets and other password-related flows
To keep SMS sending costs under control, make sure you adjust your project's rate limits and configure CAPTCHA. See the Production Checklist to learn more.


Some countries have special regulations for services that send SMS messages to users, (e.g India's TRAI DLT regulations). Remember to look up and follow the regulations of countries where you operate.

Enabling phone login#
Enable phone authentication on the Auth Providers page for hosted Supabase projects.

For self-hosted projects or local development, use the configuration file. See the configuration variables namespaced under auth.sms.

You also need to set up an SMS provider. Each provider has its own configuration. Supported providers include MessageBird, Twilio, Vonage, and TextLocal (community-supported).

Configuring SMS Providers

MessageBird Icon
MessageBird

Twilio Icon
Twilio

Vonage Icon
Vonage

Textlocal (Community Supported) Icon
Textlocal (Community Supported)
By default, a user can only request an OTP once every 60 seconds and they expire after 1 hour.

Signing in with phone OTP#
With OTP, a user can sign in without setting a password on their account. They need to verify their phone number each time they sign in.


JavaScript

Swift

Kotlin

Python

HTTP
const { data, error } = await supabase.auth.signInWithOtp({
  phone: '+13334445555',
})
The user receives an SMS with a 6-digit pin that you must verify within 60 seconds.

Verifying a phone OTP#
To verify the one-time password (OTP) sent to the user's phone number, call verifyOtp() with the phone number and OTP:


JavaScript

Swift

Kotlin

Python

HTTP
You should present a form to the user so they can input the 6 digit pin, then send it along with the phone number to verifyOtp:

const {
  data: { session },
  error,
} = await supabase.auth.verifyOtp({
  phone: '13334445555',
  token: '123456',
  type: 'sms',
})
If successful the user will now be logged in and you should receive a valid session like:

{
  "access_token": "<ACCESS_TOKEN>",
  "token_type": "bearer",
  "expires_in": 3600,
  "refresh_token": "<REFRESH_TOKEN>"
}
The access token can be sent in the Authorization header as a Bearer token for any CRUD operations on supabase-js. See our guide on Row Level Security for more info on restricting access on a user basis.

Updating a phone number#
To update a user's phone number, the user must be logged in. Call updateUser() with their phone number:


JavaScript

Swift

Kotlin

Python
const { data, error } = await supabase.auth.updateUser({
  phone: '123456789',
})
The user receives an SMS with a 6-digit pin that you must verify within 60 seconds.
Use the phone_change type when calling verifyOTP to update a user’s phone number.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Enabling phone login
Signing in with phone OTP
Verifying a phone OTP
Updating a phone number
Need some help?

Contact support
Latest product updates?

See Changelog
Auth

More
Social Login (OAuth)
Overview
Social Login

Social Login (OAuth) is an open standard for authentication that allows users to log in to one website or application using their credentials from another website or application. OAuth allows users to grant third-party applications access to their online accounts without sharing their passwords.
OAuth is commonly used for things like logging in to a social media account from a third-party app. It is a secure and convenient way to authenticate users and share information between applications.

Benefits#
There are several reasons why you might want to add social login to your applications:

Improved user experience: Users can register and log in to your application using their existing social media accounts, which can be faster and more convenient than creating a new account from scratch. This makes it easier for users to access your application, improving their overall experience.

Better user engagement: You can access additional data and insights about your users, such as their interests, demographics, and social connections. This can help you tailor your content and marketing efforts to better engage with your users and provide a more personalized experience.

Increased security: Social login can improve the security of your application by leveraging the security measures and authentication protocols of the social media platforms that your users are logging in with. This can help protect against unauthorized access and account takeovers.

Set up a social provider with Supabase Auth#
Supabase supports a suite of social providers. Follow these guides to configure a social provider for your platform.

Google Icon
Google
Facebook Icon
Facebook
Apple Icon
Apple
Azure (Microsoft) Icon
Azure (Microsoft)
Twitter Icon
Twitter
GitHub Icon
GitHub
Gitlab Icon
Gitlab
Bitbucket Icon
Bitbucket
Discord Icon
Discord
Figma Icon
Figma
Kakao Icon
Kakao
Keycloak Icon
Keycloak
LinkedIn Icon
LinkedIn
Notion Icon
Notion
Slack Icon
Slack
Spotify Icon
Spotify
Twitch Icon
Twitch
WorkOS Icon
WorkOS
Zoom Icon
Zoom
Provider tokens#
You can use the provider token and provider refresh token returned to make API calls to the OAuth provider. For example, you can use the Google provider token to access Google APIs on behalf of your user.

Supabase Auth does not manage refreshing the provider token for the user. Your application will need to use the provider refresh token to obtain a new provider token. If no provider refresh token is returned, then it could mean one of the following:

The OAuth provider does not return a refresh token
Additional scopes need to be specified in order for the OAuth provider to return a refresh token.
Provider tokens are intentionally not stored in your project's database. This is because provider tokens give access to potentially sensitive user data in third-party systems. Different applications have different needs, and one application's OAuth scopes may be significantly more permissive than another. If you want to use the provider token outside of the browser that completed the OAuth flow, it is recommended to send it to a trusted and secure server you control.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Benefits
Set up a social provider with Supabase Auth
Provider tokens
Need some help?

Contact support
Latest product updates?

See Changelog

Auth

More
Social Login (OAuth)
Google
Login with Google

Supabase Auth supports Sign in with Google for the web, native applications (Android, macOS and iOS), and Chrome extensions.

You can use Sign in with Google in two ways:

By writing application code for the web, native applications or Chrome extensions
By using Google's pre-built solutions such as personalized sign-in buttons, One Tap or automatic sign-in
Prerequisites#
You need to do some setup to get started with Sign in with Google:

Prepare a Google Cloud project. Go to the Google Cloud Platform and create a new project if necessary.
Use the Google Auth Platform console to register and set up your application's:
Audience by configuring which Google users are allowed to sign in to your application.
Data Access (Scopes) define what your application can do with your user's Google data and APIs, such as access profile information or more.
Branding and Verification show a logo and name instead of the Supabase project ID in the consent screen, improving user retention. Brand verification may take a few business days.
Setup required scopes#
Supabase Auth needs a few scopes granting access to profile data of your end users, which you have to configure in the Data Access (Scopes) screen:

openid (add manually)
.../auth/userinfo.email (added by default)
...auth/userinfo.profile (added by default)
If you add more scopes, especially those on the sensitive or restricted list your application might be subject to verification which may take a long time.

Setup consent screen branding#
It's strongly recommended you set up a custom domain and optionally verify your brand information with Google, as this makes phishing attempts easier to spot by your users.

Google's consent screen is shown to users when they sign in. Optionally configure one of the following to improve the appearance of the screen, increasing the perception of trust by your users:

Verify your application's brand (logo and name) by configuring it in the Branding section of the Google Auth Platform console. Brand verification is not automatic and may take a few business days.
Set up a custom domain for your project to present the user with a clear relationship to the website they clicked Sign in with Google on.
A good approach is to use auth.example.com or api.example.com, if your application is hosted on example.com.
If you don't set this up, users will see <project-id>.supabase.co which does not inspire trust and can make your application more susceptible to successful phishing attempts.
Project setup#
To support Sign In with Google, you need to configure the Google provider for your Supabase project.


Web

Expo React Native

Flutter (iOS and Android)

Flutter (web, macOS, Windows, Linux)

Swift

Android (Kotlin)

Chrome Extensions
Regardless of whether you use application code or Google's pre-built solutions to implement the sign in flow, you need to configure your project by obtaining a Client ID and Client Secret in the Clients section of the Google Auth Platform console:

Create a new OAuth client ID and choose Web application for the application type.
Under Authorized JavaScript origins add your application's URL. These should also be configured as the Site URL or redirect configuration in your project.
If your app is hosted on https://example.com/app add https://example.com.
Add http://localhost:<port> while developing locally. Remember to remove this when your application goes into production.
Under Authorized redirect URIs add your Supabase project's callback URL.
Access it from the Google provider page on the Dashboard.
For local development, use http://localhost:3000/auth/v1/callback.
Click Create and make sure you save the Client ID and Client Secret.
Add these values to the Google provider page on the Dashboard.
Local development#
To use the Google provider in local development:

Add a new environment variable:
SUPABASE_AUTH_EXTERNAL_GOOGLE_CLIENT_SECRET="<client-secret>"
Configure the provider:
[auth.external.google]
enabled = true
client_id = "<client-id>"
secret = "env(SUPABASE_AUTH_EXTERNAL_GOOGLE_CLIENT_SECRET)"
skip_nonce_check = false
If you have multiple client IDs, such as one for Web, iOS and Android, concatenate all of the client IDs with a comma but make sure the web's client ID is first in the list.

Using the management API#
Use the PATCH /v1/projects/{ref}/config/auth Management API endpoint to configure the project's Auth settings programmatically. For configuring the Google provider send these options:

{
  "external_google_enabled": true,
  "external_google_client_id": "your-google-client-id",
  "external_google_secret": "your-google-client-secret"
}
Signing users in#

Web

Expo React Native

Flutter (iOS and Android)

Flutter (web, macOS, Windows, Linux)

Android (Kotlin)

Chrome Extensions
Application code#
To use your own application code for the signin button, call the signInWithOAuth method (or the equivalent for your language).

Make sure you're using the right supabase client in the following code.

If you're not using Server-Side Rendering or cookie-based Auth, you can directly use the createClient from @supabase/supabase-js. If you're using Server-Side Rendering, see the Server-Side Auth guide for instructions on creating your Supabase client.

supabase.auth.signInWithOAuth({
  provider: 'google',
})
For an implicit flow, that's all you need to do. The user will be taken to Google's consent screen, and finally redirected to your app with an access and refresh token pair representing their session.

For a PKCE flow, for example in Server-Side Auth, you need an extra step to handle the code exchange. When calling signInWithOAuth, provide a redirectTo URL which points to a callback route. This redirect URL should be added to your redirect allow list.


Client

Server
In the browser, signInWithOAuth automatically redirects to the OAuth provider's authentication endpoint, which then redirects to your endpoint.

await supabase.auth.signInWithOAuth({
  provider,
  options: {
    redirectTo: `http://example.com/auth/callback`,
  },
})
At the callback endpoint, handle the code exchange to save the user session.


Next.js

SvelteKit

Astro

Remix

Express
Create a new file at src/routes/auth/callback/+server.js and populate with the following:

src/routes/auth/callback/+server.js
import { redirect } from '@sveltejs/kit';
export const GET = async (event) => {
	const {
		url,
		locals: { supabase }
	} = event;
	const code = url.searchParams.get('code') as string;
	const next = url.searchParams.get('next') ?? '/';
  if (code) {
    const { error } = await supabase.auth.exchangeCodeForSession(code)
    if (!error) {
      redirect(303, `/${next.slice(1)}`);
    }
  }
  // return the user to an error page with instructions
  redirect(303, '/auth/auth-code-error');
};
After a successful code exchange, the user's session will be saved to cookies.

Saving Google tokens#
The tokens saved by your application are the Supabase Auth tokens. Your app might additionally need the Google OAuth 2.0 tokens to access Google services on the user's behalf.

On initial login, you can extract the provider_token from the session and store it in a secure storage medium. The session is available in the returned data from signInWithOAuth (implicit flow) and exchangeCodeForSession (PKCE flow).

Google does not send out a refresh token by default, so you will need to pass parameters like these to signInWithOAuth() in order to extract the provider_refresh_token:

const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'google',
  options: {
    queryParams: {
      access_type: 'offline',
      prompt: 'consent',
    },
  },
})
Google pre-built #
Most web apps and websites can utilize Google's personalized sign-in buttons, One Tap or automatic sign-in for the best user experience.

Load the Google client library in your app by including the third-party script:

<script src="https://accounts.google.com/gsi/client" async></script>
Use the HTML Code Generator to customize the look, feel, features and behavior of the Sign in with Google button.

Pick the Swap to JavaScript callback option, and input the name of your callback function. This function will receive a CredentialResponse when sign in completes.

To make your app compatible with Chrome's third-party-cookie phase-out, make sure to set data-use_fedcm_for_prompt to true.

Your final HTML code might look something like this:

<div
  id="g_id_onload"
  data-client_id="<client ID>"
  data-context="signin"
  data-ux_mode="popup"
  data-callback="handleSignInWithGoogle"
  data-nonce=""
  data-auto_select="true"
  data-itp_support="true"
  data-use_fedcm_for_prompt="true"
></div>
<div
  class="g_id_signin"
  data-type="standard"
  data-shape="pill"
  data-theme="outline"
  data-text="signin_with"
  data-size="large"
  data-logo_alignment="left"
></div>
Create a handleSignInWithGoogle function that takes the CredentialResponse and passes the included token to Supabase. The function needs to be available in the global scope for Google's code to find it.

async function handleSignInWithGoogle(response) {
  const { data, error } = await supabase.auth.signInWithIdToken({
    provider: 'google',
    token: response.credential,
  })
}
(Optional) Configure a nonce. The use of a nonce is recommended for extra security, but optional. The nonce should be generated randomly each time, and it must be provided in both the data-nonce attribute of the HTML code and the options of the callback function.

async function handleSignInWithGoogle(response) {
  const { data, error } = await supabase.auth.signInWithIdToken({
    provider: 'google',
    token: response.credential,
    nonce: '<NONCE>',
  })
}
Note that the nonce should be the same in both places, but because Supabase Auth expects the provider to hash it (SHA-256, hexadecimal representation), you need to provide a hashed version to Google and a non-hashed version to signInWithIdToken.

You can get both versions by using the in-built crypto library:

// Adapted from https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto/digest#converting_a_digest_to_a_hex_string
const nonce = btoa(String.fromCharCode(...crypto.getRandomValues(new Uint8Array(32))))
const encoder = new TextEncoder()
const encodedNonce = encoder.encode(nonce)
crypto.subtle.digest('SHA-256', encodedNonce).then((hashBuffer) => {
  const hashArray = Array.from(new Uint8Array(hashBuffer))
  const hashedNonce = hashArray.map((b) => b.toString(16).padStart(2, '0')).join('')
})
// Use 'hashedNonce' when making the authentication request to Google
// Use 'nonce' when invoking the supabase.auth.signInWithIdToken() method
One-tap with Next.js#
If you're integrating Google One-Tap with your Next.js application, you can refer to the example below to get started:

'use client'
import Script from 'next/script'
import { createClient } from '@/utils/supabase/client'
import type { accounts, CredentialResponse } from 'google-one-tap'
import { useRouter } from 'next/navigation'
declare const google: { accounts: accounts }
// generate nonce to use for google id token sign-in
const generateNonce = async (): Promise<string[]> => {
  const nonce = btoa(String.fromCharCode(...crypto.getRandomValues(new Uint8Array(32))))
  const encoder = new TextEncoder()
  const encodedNonce = encoder.encode(nonce)
  const hashBuffer = await crypto.subtle.digest('SHA-256', encodedNonce)
  const hashArray = Array.from(new Uint8Array(hashBuffer))
  const hashedNonce = hashArray.map((b) => b.toString(16).padStart(2, '0')).join('')
  return [nonce, hashedNonce]
}
const OneTapComponent = () => {
  const supabase = createClient()
  const router = useRouter()
  const initializeGoogleOneTap = async () => {
    console.log('Initializing Google One Tap')
    const [nonce, hashedNonce] = await generateNonce()
    console.log('Nonce: ', nonce, hashedNonce)
    // check if there's already an existing session before initializing the one-tap UI
    const { data, error } = await supabase.auth.getSession()
    if (error) {
      console.error('Error getting session', error)
    }
    if (data.session) {
      router.push('/')
      return
    }
    /* global google */
    google.accounts.id.initialize({
      client_id: process.env.NEXT_PUBLIC_GOOGLE_CLIENT_ID,
      callback: async (response: CredentialResponse) => {
        try {
          // send id token returned in response.credential to supabase
          const { data, error } = await supabase.auth.signInWithIdToken({
            provider: 'google',
            token: response.credential,
            nonce,
          })
          if (error) throw error
          console.log('Session data: ', data)
          console.log('Successfully logged in with Google One Tap')
          // redirect to protected page
          router.push('/')
        } catch (error) {
          console.error('Error logging in with Google One Tap', error)
        }
      },
      nonce: hashedNonce,
      // with chrome's removal of third-party cookies, we need to use FedCM instead (https://developers.google.com/identity/gsi/web/guides/fedcm-migration)
      use_fedcm_for_prompt: true,
    })
    google.accounts.id.prompt() // Display the One Tap UI
  }
  return <Script onReady={initializeGoogleOneTap} src="https://accounts.google.com/gsi/client" />
}
export default OneTapComponent
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Prerequisites
Setup required scopes
Setup consent screen branding
Project setup
Local development
Using the management API
Signing users in
Application code
Saving Google tokens
Google pre-built
One-tap with Next.js
Need some help?

Contact support
Latest product updates?

See Changelog
Auth

More
Enterprise SSO
Overview
Enterprise Single Sign-On

Supabase Auth supports building enterprise applications that require Single Sign-On (SSO) authentication with SAML 2.0.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Auth

More
Enterprise SSO
SAML 2.0
Single Sign-On with SAML 2.0 for Projects

Looking for guides on how to use Single Sign-On with the Supabase dashboard? Head on over to Enable SSO for Your Organization.

Supabase Auth supports enterprise-level Single Sign-On (SSO) for any identity providers compatible with the SAML 2.0 protocol. This is a non-exclusive list of supported identity providers:

Google Workspaces (formerly known as G Suite)
Okta, Auth0
Microsoft Active Directory, Azure Active Directory, Microsoft Entra
PingIdentity
OneLogin
If you're having issues with identity provider software not on this list, open a support ticket.

Prerequisites#
This guide requires the use of the Supabase CLI. Make sure you're using version v1.46.4 or higher. You can use supabase -v to see the currently installed version.
You can use the supabase sso subcommands to manage your project's configuration.

SAML 2.0 support is disabled by default on Supabase projects. You can configure this on the Auth Providers page on your project.

Note that SAML 2.0 support is offered on plans Pro and above. Check the Pricing page for more information.

Terminology#
The number of SAML and SSO acronyms can often be overwhelming. Here's a glossary which you can refer back to at any time:

Identity Provider, IdP, or IDP An identity provider is a service that manages user accounts at a company or organization. It can verify the identity of a user and exchange that information with your Supabase project and other applications. It acts as a single source of truth for user identities and access rights. Commonly used identity providers are: Microsoft Active Directory (Azure AD, Microsoft Entra), Okta, Google Workspaces (G Suite), PingIdentity, OneLogin, and many others. There are also self-hosted and on-prem versions of identity providers, and sometimes they are accessible only by having access to a company VPN or being in a specific building.
Service Provider, SP This is the software that is asking for user information from an identity provider. In Supabase, this is your project's Auth server.
Assertion An assertion is a statement issued by an identity provider that contains information about a user.
EntityID A globally unique ID (usually a URL) that identifies an Identity Provider or Service Provider across the world.
NameID A unique ID (usually an email address) that identifies a user at an Identity Provider.
Metadata An XML document that describes the features and configuration of an Identity Provider or Service Provider. It can be as a standalone document or as a URL. Usually (but not always) the EntityID is the URL at which you can access the Metadata.
Certificate Supabase Auth (the Service Provider) trusts assertions from an Identity Provider based on the signature attached to the assertion. The signature is verified according to the certificate present in the Metadata.
Assertion Consumer Service (ACS) URL This is one of the most important SAML URLs. It is the URL where Supabase Auth will accept assertions from an identity provider. Basically, once the identity provider verifies the user's identity it will redirect to this URL and the redirect request will contain the assertion.
Binding (Redirect, POST, or Artifact) This is a description of the way an identity provider communicates with Supabase Auth. When using the Redirect binding, the communication occurs using HTTP 301 redirects. When it's POST, it's using POST requests sent with <form> elements on a page. When using Artifact, it's using a more secure exchange over a Redirect or POST.
RelayState State used by Supabase Auth to hold information about a request to verify the identity of a user.
Important SAML 2.0 information#
Below is information about your project's SAML 2.0 configuration which you can share with the company or organization that you're trying to on-board.

Name	Value
EntityID	https://<project>.supabase.co/auth/v1/sso/saml/metadata
Metadata URL	https://<project>.supabase.co/auth/v1/sso/saml/metadata
Metadata URL
(download)	https://<project>.supabase.co/auth/v1/sso/saml/metadata?download=true
ACS URL	https://<project>.supabase.co/auth/v1/sso/saml/acs
SLO URL	https://<project>.supabase.co/auth/v1/sso/slo
NameID	Required emailAddress or persistent
Note that SLO (Single Logout) is not supported at this time with Supabase Auth as it is a rarely supported feature by identity providers. However, the URL is registered and advertised for when this does become available. SLO is a best-effort service, so we recommend considering Session Timebox or Session Inactivity Timeout instead to force your end-users to authenticate regularly.

Append ?download=true to the Metadata URL to download the Metadata XML file. This is useful in cases where the identity provider requires a file.

Alternatively, you can use the supabase sso info --project-ref <your-project> command to get setup information for your project.

User accounts and identities#
User accounts and identities created via SSO differ from regular (email, phone, password, social login...) accounts in these ways:

No automatic linking. Each user account verified using a SSO identity provider will not be automatically linked to existing user accounts in the system. That is, if a user valid.email@supabase.io had signed up with a password, and then uses their company SSO login with your project, there will be two valid.email@supabase.io user accounts in the system.
Emails are not necessarily unique. Given the behavior with no automatic linking, email addresses are no longer a unique identifier for a user account. Always use the user's UUID to correctly reference user accounts.
Sessions may have a maximum duration. Depending on the configuration of the identity provider, a login session established with SSO may forcibly log out a user after a certain period of time.
Row Level Security#
You can use information about the SSO identity provider in Row Level Security policies.

Here are some commonly used statements to extract SSO related information from the user's JWT:

auth.jwt()#>>'{amr,0,method}' Returns the name of the last method used to verify the identity of this user. With SAML SSO this is sso/saml.
auth.jwt()#>>'{amr,0,provider}' Returns the UUID of the SSO identity provider used by the user to sign-in.
auth.jwt()#>>'{user_metadata,iss}' Returns the identity provider's SAML 2.0 EntityID
If you use Multi-Factor Authentication with SSO, the amr array may have a different method at index 0!

A common use case with SSO is to use the UUID of the identity provider as the identifier for the organization the user belongs to -- frequently known as a tenant. By associating the identity provider's UUID with your tenants, you can use restrictive RLS policies to scope down actions and data that a user is able to access.

For example, let's say you have a table like:

create table organization_settings (
  -- the organization's unique ID
  id uuid not null primary key,
  -- the organization's SSO identity provider
  sso_provider_id uuid unique,
  -- name of the organization
  name text,
  -- billing plan (paid, Free, Enterprise)
  billing_plan text
);
You can use the information present in the user's JWT to scope down which rows from this table the user can see, without doing any additional user management:

CREATE POLICY "View organization settings."
  ON organization_settings
  AS RESTRICTIVE
  USING (
    sso_provider_id = (select auth.jwt()#>>'{amr,0,provider}')
  );
Managing SAML 2.0 connections#
Once you've enabled SAML 2.0 support on your project via the Auth Providers page in the dashboard, you can use the Supabase CLI to add, update, remove and view information about identity providers.

Add a connection#
To establish a connection to a SAML 2.0 Identity Provider (IdP) you will need:

A SAML 2.0 Metadata XML file, or a SAML 2.0 Metadata URL pointing to an XML file
(Optional) Email domains that the organization's IdP uses
(Optional) Attribute mappings between the user properties of the IdP and the claims stored by Supabase Auth
You should obtain the SAML 2.0 Metadata XML file or URL from the organization whose IdP you wish to connect. Most SAML 2.0 Identity Providers support the Metadata URL standard, and we recommend using a URL if this is available.

Commonly used SAML 2.0 Identity Providers that support Metadata URLs:

Okta
Azure AD (Microsoft Entra)
PingIdentity
Commonly used SAML 2.0 Identity Providers that only support Metadata XML files:

Google Workspaces (G Suite)
Any self-hosted or on-prem identity provider behind a VPN
Once you've obtained the SAML 2.0 Metadata XML file or URL you can establish a connection with your project's Supabase Auth server by running:

supabase sso add --type saml --project-ref <your-project> \
  --metadata-url 'https://company.com/idp/saml/metadata' \
  --domains company.com
If you wish to use a Metadata XML file instead, you can use:

supabase sso add --type saml --project-ref <your-project> \
  --metadata-file /path/to/saml/metadata.xml \
  --domains company.com
This command will register a new identity provider with your project's Auth server. When successful, you will see the details of the provider such as it's SAML information and registered domains.

Note that only persons with write access to the project can register, update or remove identity providers.

Once you've added an identity provider, users who have access to it can sign in to your application. With SAML 2.0 there are two ways that users can sign in to your project:

By signing-in from your application's user interface, commonly known as SP (Service Provider) Initiated Flow
By clicking on an icon in the application menu on the company intranet or identity provider page, commonly known as Identity Provider Initiated (IdP) Flow
To initiate a sign-in request from your application's user interface (i.e. the SP Initiated Flow), you can use:


JavaScript

Dart

Swift

Kotlin
supabase.auth.signInWithSSO({
  domain: 'company.com',
})
Calling signInWithSSO starts the sign-in process using the identity provider registered for the company.com domain name. It is not required that identity providers be assigned one or multiple domain names, in which case you can use the provider's unique ID instead.

Understanding attribute mappings#
When a user signs in using the SAML 2.0 Single Sign-On protocol, an XML document called the SAML Assertion is exchanged between the identity provider and Supabase Auth.

This assertion contains information about the user's identity and other authentication information, such as:

Unique ID of the user (called NameID in SAML)
Email address
Name of the user
Department or organization
Other attributes present in the users directory managed by the identity provider
With exception of the unique user ID, SAML does not require any other attributes in the assertion. Identity providers can be configured so that only select user information is shared with your project.

Your project can be configured to recognize these attributes and map them into your project's database using a JSON structure. This process is called attribute mapping, and varies according to the configuration of the identity provider.

For example, the following JSON structure configures attribute mapping for the email and first_name user identity properties.

{
  "keys": {
    "email": {
      "name": "mail"
    },
    "first_name": {
      "name": "givenName"
    }
  }
}
When creating or updating an identity provider with the Supabase CLI you can include this JSON as a file with the --attribute-mapping-file /path/to/attribute/mapping.json flag.

For example, to change the attribute mappings to an existing provider you can use:

supabase sso update <provider-uuid> --project-ref <your-project> \
  --attribute-mapping-file /path/to/attribute/mapping.json
Given a SAML 2.0 assertion that includes these attributes:

<saml:AttributeStatement>
  <!-- will be mapped to the email key -->
  <saml:Attribute
    Name="mail"
    NameFormat="urn:oasis:names:tc:SAML:2.0:attrname-format:basic"
    >
    <saml:AttributeValue xsi:type="xs:string">
      valid.email@supabase.io
    </saml:AttributeValue>
  </saml:Attribute>
  <!-- will be mapped to the first_name key -->
  <saml:Attribute
    Name="givenName"
    NameFormat="urn:oasis:names:tc:SAML:2.0:attrname-format:basic"
    >
    <saml:AttributeValue xsi:type="xs:string">
      Jane Doe
    </saml:AttributeValue>
  </saml:Attribute>
</saml:AttributeStatement>
Will result in the following claims in the user's identity in the database and JWT:

{
  "email": "valid.email@supabase.io",
  "custom_claims": {
    "first_name": "Jane Doe"
  }
}
Supabase Auth does not require specifying attribute mappings if you only need access to the user's email. It will attempt to find an email attribute specified in the assertion. All other properties will not be automatically included, and it is those you need to map.

At this time it is not possible to have users without an email address, so SAML assertions without one will be rejected.

Most SAML 2.0 identity providers use Lightweight Directory Access Protocol (LDAP) attribute names. However, due to their variability and complexity operators of identity providers are able to customize both the Name and attribute value that is sent to Supabase Auth in an assertion. Refer to the identity provider's documentation and contact the operator for details on what attributes are mapped for your project.

Accessing the stored attributes

The stored attributes, once mapped, show up in the access token (a JWT) of the user. If you need to look these values up in the database, you can find them in the auth.identities table under the identity_data JSON column. Identities created for SSO providers have sso:<uuid-of-provider> in the provider column, while id contains the unique NameID of the user account.

Furthermore, you can find the same identity data under raw_app_meta_data inside auth.users.

Remove a connection#
Once a connection to an identity provider is established, you can remove it by running:

supabase sso remove <provider-id> --project-ref <your-project>
If successful, the details of the removed identity provider will be shown. All user accounts from that identity provider will be immediately logged out. User information will remain in the system, but it will no longer be possible for any of those accounts to be accessed in the future, even if you add the connection again.

If you need to reassign those user accounts to another identity provider, open a support ticket.
A list of all registered identity providers can be displayed by running:

supabase sso list --project-ref <your-project>
Update a connection#
You may wish to update settings about a connection to a SAML 2.0 identity provider.

Commonly this is necessary when:

Cryptographic keys are rotated or have expired
Metadata URL has changed, but is the same identity provider
Other SAML 2.0 Metadata attributes have changed, but it is still the same identity provider
You are updating the domains or attribute mapping
You can use this command to update the configuration of an identity provider:

supabase sso update <provider-id> --project-ref <your-project>
Use --help to see all available flags.

It is not possible to change the unique SAML identifier of the identity provider, known as EntityID. Everything else can be updated. If the SAML EntityID of your identity provider has changed, it is regarded as a new identity provider and you will have to register it like a new connection.

Retrieving information about a connection#
You can always obtain a list of all registered providers using:

supabase sso list --project-ref <your-project>
This list will only include basic information about each provider. To see all of the information about a provider you can use:

supabase sso show <provider-id> --project-ref <your-project>
You can use the -o json flag to output the information as JSON, should you need to. Other formats may be supported, use --help to see all available options.

Pricing#
$0.015 per SSO MAU. You are only charged for usage exceeding your subscription plan's
quota.

For a detailed breakdown of how charges are calculated, refer to Manage Monthly Active SSO Users usage.

Frequently asked questions#
Publishing your application to an identity provider's marketplace#
Many cloud-based identity providers offer a marketplace where you can register your application for easy on-boarding with customers. When you use Supabase Auth's SAML 2.0 support you can register your project in any one of these marketplaces.

Refer to the relevant documentation for each cloud-based identity provider on how you can do this. Some common marketplaces are:

Okta Integration Network
Azure Active Directory App Gallery
Google Workspaces Pre-integrated SAML apps catalog
Why do some users get: SAML assertion does not contain email address?#
Identity providers do not have to send back and email address for the user, though they often do. Supabase Auth requires that an email address is present.

The following list of commonly used SAML attribute names is inspected, in order of appearance, to discover the email address in the assertion:

urn:oid:0.9.2342.19200300.100.1.3
http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress
http://schemas.xmlsoap.org/claims/EmailAddress
mail
email
Finally if there is no such attribute, it will use the SAML NameID value but only if the format is advertised as urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress.

Should you run into this problem, it is most likely a misconfiguration issue on the identity provider side. Instruct your contact at the company to map the user's email address to one of the above listed attribute names, typically email.

Accessing the private key used for SAML in your project#
At this time it is not possible to extract the RSA private key used by your project's Supabase Auth server. This is done to keep the private key as secure as possible, given that SAML does not offer an easy way to rotate keys without disrupting service. (Use a SAML 2.0 Metadata URL whenever possible for this reason!)

If you really need access to the key, open a support ticket and we'll try to support you as best as possible.

Is multi-tenant SSO with SAML supported?#
Yes, Supabase supports multi-tenant Single Sign-On (SSO) using SAML 2.0. While the dashboard displays only one SAML field, you can set up multiple SAML connections using the Supabase CLI.
Each connection is assigned a unique sso_provider_id, which is included in the user's JWT and can be used in Row Level Security (RLS) policies. You can configure custom attribute mappings for each connection to include tenant-specific information, such as roles.
This setup allows you to implement multi-tenant SSO for multiple clients or organizations within a single application. For example, if you have an app with multiple clients using different Azure Active Directories, you can create separate SAML connections for each and use the sso_provider_id to manage access and apply appropriate security policies.

Is multi-subdomain SSO with SAML supported?#
Yes, also referred to as cross-origin authentication within the same site. To redirect to a URL other than the Site URL, following the SAML response from the IdP, the redirectTo option can be added to signInWithSSO.

const { data, error } = await supabase.auth.signInWithSSO({
  domain: 'company.com',
  options: {
    redirectTo: `https://app.company.com/callback`,
  },
})
When redirecting to a URL other than the Site URL, a /callback endpoint is necessary to process the auth code from the IdP and exchange it for a session. This assumes the Supabase SSR client has already been configured.


SvelteKit
import { error, redirect } from '@sveltejs/kit'
import type { RequestHandler } from './$types'
export const GET: RequestHandler = async ({ url, locals }) => {
  const code = url.searchParams.get('code')
  if (!code) {
    error(400, 'No authorization code provided')
  }
  const { error: tokenExchangeError } = await locals.supabase.auth.exchangeCodeForSession(code)
  if (tokenExchangeError) {
    error(400, 'Failed to exchange authorization code for session')
  }
  redirect(303, '/')
}
Why doesn't IdP-initiated SAML flow work with PKCE, and what's the alternative?#
Traditional IdP-initiated SAML flows aren't compatible with PKCE (Proof Key for Code Exchange) because PKCE requires a code_challenge and code_verifier that are generated when your application initiates the authentication flow. In IdP-initiated flows, Supabase receives an unsolicited response without this information, causing the code exchange step to fail.

To achieve the same user experience while maintaining PKCE security, you can implement a "bookmark app" approach:

Create an endpoint in your application (for example, https://your-app.com/auth/saml-init) that initiates the SAML flow using signInWithSSO. Then create a bookmark or linked application in your IdP that points to this endpoint. When users access the bookmark app, it triggers a secure SP-initiated flow.

This approach supports custom SAML assertions and lets you embed the link anywhere in your application.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Prerequisites
Terminology
Important SAML 2.0 information
User accounts and identities
Row Level Security
Managing SAML 2.0 connections
Add a connection
Understanding attribute mappings
Remove a connection
Update a connection
Retrieving information about a connection
Pricing
Frequently asked questions
Publishing your application to an identity provider's marketplace
Why do some users get: SAML assertion does not contain email address?
Accessing the private key used for SAML in your project
Is multi-tenant SSO with SAML supported?
Is multi-subdomain SSO with SAML supported?
Why doesn't IdP-initiated SAML flow work with PKCE, and what's the alternative?
Need some help?

Contact support
Latest product updates?

See Changelog

Auth
Flows (How-tos)
Anonymous Sign-Ins
Anonymous Sign-Ins

Create and use anonymous users to authenticate with Supabase

Enable Anonymous Sign-Ins to build apps which provide users an authenticated experience without requiring users to enter an email address, password, use an OAuth provider or provide any other PII (Personally Identifiable Information). Later, when ready, the user can link an authentication method to their account.

Anonymous user vs the anon key
Calling signInAnonymously() creates an anonymous user. It's just like a permanent user, except the user can't access their account if they sign out, clear browsing data, or use another device.

Like permanent users, the authenticated Postgres role will be used when using the Data APIs to access your project. JWTs for these users will have an is_anonymous claim which you can use to distinguish in RLS policies.

This is different from the anon API key which does not create a user and can be used to implement public access to your database as it uses the anonymous Postgres role.

Anonymous sign-ins can be used to build:

E-commerce applications, such as shopping carts before check-out
Full-feature demos without collecting personal information
Temporary or throw-away accounts
Review your existing RLS policies before enabling anonymous sign-ins. Anonymous users use the authenticated role. To distinguish between anonymous users and permanent users, your policies need to check the is_anonymous field of the user's JWT.

See the Access control section for more details.

Use Dynamic Rendering with Next.js
The Supabase team has received reports of user metadata being cached across unique anonymous users as a result of Next.js static page rendering. For the best user experience, utilize dynamic page rendering.

Self hosting and local development
For self-hosting, you can update your project configuration using the files and environment variables provided. See the local development docs for more details.

Sign in anonymously#

JavaScript

Flutter

Swift

Kotlin

Python
Call the signInAnonymously() method:

const { data, error } = await supabase.auth.signInAnonymously()
Convert an anonymous user to a permanent user#
Converting an anonymous user to a permanent user requires linking an identity to the user. This requires you to enable manual linking in your Supabase project.

Link an email / phone identity#

JavaScript

Flutter

Swift

Kotlin

Python
You can use the updateUser() method to link an email or phone identity to the anonymous user. To add a password for the anonymous user, the user's email or phone number needs to be verified first.

const { data: updateEmailData, error: updateEmailError } = await supabase.auth.updateUser({
  email: 'valid.email@supabase.io',
})
// verify the user's email by clicking on the email change link
// or entering the 6-digit OTP sent to the email address
// once the user has been verified, update the password
const { data: updatePasswordData, error: updatePasswordError } = await supabase.auth.updateUser({
  password: 'password',
})
Link an OAuth identity#

JavaScript

Flutter

Swift

Kotlin

Python
You can use the linkIdentity() method to link an OAuth identity to the anonymous user.

const { data, error } = await supabase.auth.linkIdentity({ provider: 'google' })
Access control#
An anonymous user assumes the authenticated role just like a permanent user. You can use row-level security (RLS) policies to differentiate between an anonymous user and a permanent user by checking for the is_anonymous claim in the JWT returned by auth.jwt():

create policy "Only permanent users can post to the news feed"
on news_feed as restrictive for insert
to authenticated
with check ((select (auth.jwt()->>'is_anonymous')::boolean) is false );
create policy "Anonymous and permanent users can view the news feed"
on news_feed for select
to authenticated
using ( true );
Use restrictive policies
RLS policies are permissive by default, which means that they are combined using an "OR" operator when multiple policies are applied. It is important to construct restrictive policies to ensure that the checks for an anonymous user are always enforced when combined with other policies.
Be aware that a single 'restrictive' RLS policy alone will fail unless combined with another policy that returns true, ensuring the combined condition is met.

Resolving identity conflicts#
Depending on your application requirements, data conflicts can arise when an anonymous user is converted to a permanent user. For example, in the context of an e-commerce application, an anonymous user would be allowed to add items to the shopping cart without signing up / signing in. When they decide to sign-in to an existing account, you will need to decide how you want to resolve data conflicts in the shopping cart:

Overwrite the items in the cart with those in the existing account
Overwrite the items in the cart with those from the anonymous user
Merge the items in the cart together
Linking an anonymous user to an existing account#
In some cases, you may need to link an anonymous user to an existing account rather than creating a new permanent account. This process requires manual handling of potential conflicts. Here's a general approach:

// 1. Sign in anonymously (assuming the user is already signed in anonymously)
const { data: anonData, error: anonError } = await supabase.auth.getSession()
// 2. Attempt to update the user with the existing email
const { data: updateData, error: updateError } = await supabase.auth.updateUser({
  email: 'valid.email@supabase.io',
})
// 3. Handle the error (since the email belongs to an existing user)
if (updateError) {
  console.log('This email belongs to an existing user. Please sign in to that account.')
  // 4. Sign in to the existing account
  const {
    data: { user: existingUser },
    error: signInError,
  } = await supabase.auth.signInWithPassword({
    email: 'valid.email@supabase.io',
    password: 'user_password',
  })
  if (existingUser) {
    // 5. Reassign entities tied to the anonymous user
    // This step will vary based on your specific use case and data model
    const { data: reassignData, error: reassignError } = await supabase
      .from('your_table')
      .update({ user_id: existingUser.id })
      .eq('user_id', anonData.session.user.id)
    // 6. Implement your chosen conflict resolution strategy
    // This could involve merging data, overwriting, or other custom logic
    await resolveDataConflicts(anonData.session.user.id, existingUser.id)
  }
}
// Helper function to resolve data conflicts (implement based on your strategy)
async function resolveDataConflicts(anonymousUserId, existingUserId) {
  // Implement your conflict resolution logic here
  // This could involve ignoring the anonymous user's metadata, overwriting the existing user's metadata, or merging the data of both the anonymous and existing user.
}
Abuse prevention and rate limits#
Since anonymous users are stored in your database, bad actors can abuse the endpoint to increase your database size drastically. It is strongly recommended to enable invisible CAPTCHA or Cloudflare Turnstile to prevent abuse for anonymous sign-ins. An IP-based rate limit is enforced at 30 requests per hour which can be modified in your dashboard. You can refer to the full list of rate limits here.

Automatic cleanup#
Automatic cleanup of anonymous users is currently not available. Instead, you can delete anonymous users from your project by running the following SQL:

-- deletes anonymous users created more than 30 days ago
delete from auth.users
where is_anonymous is true and created_at < now() - interval '30 days';
Resources#
Supabase - Get started for free
Supabase JS Client
Supabase Flutter Client
Supabase Kotlin Client
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Sign in anonymously
Convert an anonymous user to a permanent user
Link an email / phone identity
Link an OAuth identity
Access control
Resolving identity conflicts
Linking an anonymous user to an existing account
Abuse prevention and rate limits
Automatic cleanup
Resources
Need some help?

Contact support
Latest product updates?

See Changelog

Auth
Flows (How-tos)
Web3 (Ethereum or Solana)
Sign in with Web3

Use your Web3 wallet to authenticate users with Supabase

Enable Sign In with Web3 to allow users to sign in to your application using only their Web3 wallet.

Supported Web3 wallets:

All Solana wallets
All Ethereum wallets
How does it work?#
Sign in with Web3 utilizes the EIP 4361 standard to authenticate wallet addresses off-chain. This standard is widely supported by the Ethereum and Solana ecosystems, making it the best choice for verifying wallet ownership.

Authentication works by asking the Web3 wallet application to sign a predefined message with the user's wallet. This message is parsed both by the Web3 wallet application and Supabase Auth to verify its validity and purpose, before creating a user account or session.

An example of such a message is:

example.com wants you to sign in with your Ethereum account:
0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2
I accept the ExampleOrg Terms of Service: https://example.com/tos
URI: https://example.com/login
Version: 1
Chain ID: 1
Nonce: 32891756
Issued At: 2021-09-30T16:25:24Z
Resources:
- https://example.com/my-web2-claim.json
It defines the wallet address, timestamp, browser location where the sign-in occurred and includes a customizable statement (I accept...) which you can use to ask consent from the user.

Most Web3 wallets are able to recognize these messages and show a dedicated "Confirm Sign In" dialog validating and presenting the information in the message in a secure and responsible way to the user. Even if the wallet does not directly support these messages, it will use the message signature dialog instead.

Finally the Supabase Auth server validates both the message's contents and signature before issuing a valid User session to your application. Validation rules include:

Message structure validation
Cryptographic signature verification
Timestamp validation, ensuring the signature was created within 10 minutes of the sign-in call
URI and Domain validation, ensuring these match your server's defined Redirect URLs
The wallet address is used as the identity identifier, and in the identity data you can also find the statement and additional metadata.

Enable the Web3 provider#
In the dashboard navigate to your project's Authentication Providers section and enable the Web3 Wallet provider.

In the CLI add the following config to your supabase/config.toml file:

[auth.web3.solana]
enabled = true
[auth.web3.ethereum]
enabled = true
Potential for abuse#
User accounts that sign in with their Web3 wallet will not have an email address or phone number associated with them. This can open your project to abuse as creating a Web3 wallet account is free and easy to automate and difficult to correlate with a real person's identity.

Control your project's exposure by configuring in the dashboard:

Rate Limits for Web3
Enable CAPTCHA protection
Or in the CLI:

[auth.rate_limit]
# Number of Web3 logins that can be made in a 5 minute interval per IP address.
web3 = 30
[auth.captcha]
enabled = true
provider = "hcaptcha" # or other supported providers
secret = "0x0000000000000000000000000000000000000000"
Many wallet applications will warn the user if the message sent for signing is not coming from the page they are currently visiting. To further prevent your Supabase project from receiving signed messages destined for other applications, you must register your application's URL using the Redirect URL settings.

For example if the user is signing in to the page https://example.com/sign-in you should add the following configurations in the Redirect URL settings:

https://example.com/sign-in/ (last slash is important)
Alternatively set up a glob pattern such as https://example.com/**
Sign in with Ethereum#
Ethereum defines the window.ethereum global scope object that your app uses to interact with Ethereum Wallets. Additionally there is a wallet discovery mechanism (EIP-6963) that your app can use to discover all of the available wallets on the user's browser.

To sign in a user with their Ethereum wallet make sure that the user has installed a wallet application. There are two ways to do this:

Detect the window.ethereum global scope object and ensure it's defined. This only works if your user has only one wallet installed on their browser.
Use the wallet discovery mechanism (EIP-6963) to ask the user to choose a wallet before they continue to sign in. Read the MetaMask guide on the best way to support this.

Ethereum Window API (EIP-1193)

Ethereum Wallet API (EIP-6963)

Ethereum Message and Signature
Use the following code to sign in a user, implicitly relying on the window.ethereum global scope wallet API:

const { data, error } = await supabase.auth.signInWithWeb3({
  chain: 'ethereum',
  statement: 'I accept the Terms of Service at https://example.com/tos',
})
Sign in with Solana#

Solana Window API

Solana Wallet Adapter
Most Solana wallet applications expose their API via the window.solana global scope object in your web application.

Supabase's JavaScript Client Library provides built-in support for this API.

To sign in a user make sure that:

The user has installed a wallet application (by checking that the window.solana object is defined)
The wallet application is connected to your application by using the window.solana.connect() API
Use the following code to authenticate a user:

const { data, error } = await supabase.auth.signInWithWeb3({
  chain: 'solana',
  statement: 'I accept the Terms of Service at https://example.com/tos',
})
Providing a statement is required for most Solana wallets and this message will be shown to the user on the consent dialog. It will also be added to the identity data for your users.

If you are using a non-standard Solana wallet that does not register the window.solana object, or your user has multiple Solana wallets attached to the page you can disambiguate by providing the wallet object like so:

To use Brave Wallet with Solana:
const { data, error } = await supabase.auth.signInWithWeb3({
  chain: 'solana',
  statement: 'I accept the Terms of Service at https://example.com/tos',
  wallet: window.braveSolana,
})
To use Phantom with Solana:
const { data, error } = await supabase.auth.signInWithWeb3({
  chain: 'solana',
  statement: 'I accept the Terms of Service at https://example.com/tos',
  wallet: window.phantom,
})
Frequently asked questions#
How to associate an email address, phone number or social login to a user signing in with Web3?#
Web3 wallets don't expose any identifying information about the user other than their wallet address (public key). This is why accounts that were created using Sign in with Web3 don't have any email address or phone number associated.

To associate an email address, phone number or other social login with their account you can use the supabase.auth.updateUser() or supabase.auth.linkIdentity() APIs.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How does it work?
Enable the Web3 provider
Potential for abuse
Sign in with Ethereum
Sign in with Solana
Frequently asked questions
How to associate an email address, phone number or social login to a user signing in with Web3?
Need some help?

Contact support
Latest product updates?

See Changelog

Auth
Flows (How-tos)
Mobile Deep Linking
Native Mobile Deep Linking

Set up Deep Linking for mobile applications.

Many Auth methods involve a redirect to your app. For example:

Signup confirmation emails, Magic Link signins, and password reset emails contain a link that redirects to your app.
In OAuth signins, an automatic redirect occurs to your app.
With Deep Linking, you can configure this redirect to open a specific page. This is necessary if, for example, you need to display a form for password reset, or to manually exchange a token hash.

Setting up deep linking#

Expo React Native

Flutter

Swift

Android Kotlin
To link to your development build or standalone app, you need to specify a custom URL scheme for your app. You can register a scheme in your app config (app.json, app.config.js) by adding a string under the scheme key:

{
  "expo": {
    "scheme": "com.supabase"
  }
}
In your project's auth settings add the redirect URL, e.g. com.supabase://**.

Finally, implement the OAuth and linking handlers. See the supabase-js reference for instructions on initializing the supabase-js client in React Native.

import { Button } from "react-native";
import { makeRedirectUri } from "expo-auth-session";
import * as QueryParams from "expo-auth-session/build/QueryParams";
import * as WebBrowser from "expo-web-browser";
import * as Linking from "expo-linking";
import { supabase } from "app/utils/supabase";
WebBrowser.maybeCompleteAuthSession(); // required for web only
const redirectTo = makeRedirectUri();
const createSessionFromUrl = async (url: string) => {
  const { params, errorCode } = QueryParams.getQueryParams(url);
  if (errorCode) throw new Error(errorCode);
  const { access_token, refresh_token } = params;
  if (!access_token) return;
  const { data, error } = await supabase.auth.setSession({
    access_token,
    refresh_token,
  });
  if (error) throw error;
  return data.session;
};
const performOAuth = async () => {
  const { data, error } = await supabase.auth.signInWithOAuth({
    provider: "github",
    options: {
      redirectTo,
      skipBrowserRedirect: true,
    },
  });
  if (error) throw error;
  const res = await WebBrowser.openAuthSessionAsync(
    data?.url ?? "",
    redirectTo
  );
  if (res.type === "success") {
    const { url } = res;
    await createSessionFromUrl(url);
  }
};
const sendMagicLink = async () => {
  const { error } = await supabase.auth.signInWithOtp({
    email: "valid.email@supabase.io",
    options: {
      emailRedirectTo: redirectTo,
    },
  });
  if (error) throw error;
  // Email sent.
};
export default function Auth() {
  // Handle linking into app from email app.
  const url = Linking.useURL();
  if (url) createSessionFromUrl(url);
  return (
    <>
      <Button onPress={performOAuth} title="Sign in with GitHub" />
      <Button onPress={sendMagicLink} title="Send Magic Link" />
    </>
  );
}
For the best user experience it is recommended to use universal links which require a more elaborate setup. You can find the detailed setup instructions in the Expo docs.

Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Setting up deep linking
Need some help?

Contact support
Latest product updates?

See Changelog

Identity Linking

Manage the identities associated with your user

Identity linking strategies#
Currently, Supabase Auth supports 2 strategies to link an identity to a user:

Automatic Linking
Manual Linking
Automatic linking#
Supabase Auth automatically links identities with the same email address to a single user. This helps to improve the user experience when multiple OAuth login options are presented since the user does not need to remember which OAuth account they used to sign up with. When a new user signs in with OAuth, Supabase Auth will attempt to look for an existing user that uses the same email address. If a match is found, the new identity is linked to the user.

In order for automatic linking to correctly identify the user for linking, Supabase Auth needs to ensure that all user emails are unique. It would also be an insecure practice to automatically link an identity to a user with an unverified email address since that could lead to pre-account takeover attacks. To prevent this from happening, when a new identity can be linked to an existing user, Supabase Auth will remove any other unconfirmed identities linked to an existing user.

Users that signed up with SAML SSO will not be considered as targets for automatic linking.

Manual linking (beta)#

JavaScript

Dart

Swift

Kotlin

Python
Supabase Auth allows a user to initiate identity linking with a different email address when they are logged in. To link an OAuth identity to the user, call linkIdentity():

const { data, error } = await supabase.auth.linkIdentity({ provider: 'google' })
In the example above, the user will be redirected to Google to complete the OAuth2.0 flow. Once the OAuth2.0 flow has completed successfully, the user will be redirected back to the application and the Google identity will be linked to the user. You can enable manual linking from your project's authentication configuration options or by setting the environment variable GOTRUE_SECURITY_MANUAL_LINKING_ENABLED: true when self-hosting.

Link identity with native OAuth (ID token)#

JavaScript

Dart
For native mobile applications, you can link an identity using an ID token obtained from a third-party OAuth provider. This is useful when you want to use native OAuth flows (like Google Sign-In or Sign in with Apple) rather than web-based OAuth redirects.

// Example with Google Sign-In (using a native Google Sign-In library)
const idToken = 'ID_TOKEN_FROM_GOOGLE'
const accessToken = 'ACCESS_TOKEN_FROM_GOOGLE'
const { data, error } = await supabase.auth.linkIdentity({
  provider: 'google',
  token: idToken,
  access_token: accessToken,
})
Unlink an identity#

JavaScript

Dart

Swift

Kotlin

Python
You can use getUserIdentities() to fetch all the identities linked to a user. Then, call unlinkIdentity() to unlink the identity. The user needs to be logged in and have at least 2 linked identities in order to unlink an existing identity.

// retrieve all identities linked to a user
const { data: identities, error: identitiesError } = await supabase.auth.getUserIdentities()
if (!identitiesError) {
  // find the google identity linked to the user
  const googleIdentity = identities.identities.find((identity) => identity.provider === 'google')
  if (googleIdentity) {
    // unlink the google identity from the user
    const { data, error } = await supabase.auth.unlinkIdentity(googleIdentity)
  }
}
Frequently asked questions#
How to add email/password login to an OAuth account?#
Call the updateUser({ password: 'validpassword'}) to add email with password authentication to an account created with an OAuth provider (Google, GitHub, etc.).

Can you sign up with email if already using OAuth?#
If you try to create an email account after previously signing up with OAuth using the same email, you'll receive an obfuscated user response with no verification email sent. This prevents user enumeration attacks.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Identity linking strategies
Automatic linking
Manual linking (beta)
Link identity with native OAuth (ID token)
Unlink an identity
Frequently asked questions
How to add email/password login to an OAuth account?
Can you sign up with email if already using OAuth?
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author St

Auth

More
Multi-Factor Authentication
Overview
Multi-Factor Authentication

Multi-factor authentication (MFA), sometimes called two-factor authentication (2FA), adds an additional layer of security to your application by verifying their identity through additional verification steps.

It is considered a best practice to use MFA for your applications.

Users with weak passwords or compromised social login accounts are prone to malicious account takeovers. These can be prevented with MFA because they require the user to provide proof of both of these:

Something they know. Password, or access to a social-login account.
Something they have. Access to an authenticator app (a.k.a. TOTP) or a mobile phone.
Overview#
Supabase Auth implements MFA via two methods: App Authenticator, which makes use of a Time based-one Time Password, and phone messaging, which makes use of a code generated by Supabase Auth.

Applications using MFA require two important flows:

Enrollment flow. This lets users set up and control MFA in your app.
Authentication flow. This lets users sign in using any factors after the conventional login step.
Supabase Auth provides:

Enrollment API - build rich user interfaces for adding and removing factors.
Challenge and Verify APIs - securely verify that the user has access to a factor.
List Factors API - build rich user interfaces for signing in with additional factors.
You can control access to the Enrollment API as well as the Challenge and Verify APIs via the Supabase Dashboard. A setting of Verification Disabled will disable both the challenge API and the verification API.

These sets of APIs let you control the MFA experience that works for you. You can create flows where MFA is optional, mandatory for all, or only specific groups of users.

Once users have enrolled or signed-in with a factor, Supabase Auth adds additional metadata to the user's access token (JWT) that your application can use to allow or deny access.

This information is represented by an Authenticator Assurance Level, a standard measure about the assurance of the user's identity Supabase Auth has for that particular session. There are two levels recognized today:

Assurance Level 1: aal1 Means that the user's identity was verified using a conventional login method such as email+password, magic link, one-time password, phone auth or social login.
Assurance Level 2: aal2 Means that the user's identity was additionally verified using at least one second factor, such as a TOTP code or One-Time Password code.
This assurance level is encoded in the aal claim in the JWT associated with the user. By decoding this value you can create custom authorization rules in your frontend, backend, and database that will enforce the MFA policy that works for your application. JWTs without an aal claim are at the aal1 level.

Adding to your app#
Adding MFA to your app involves these four steps:

Add enrollment flow. You need to provide a UI within your app that your users will be able to set-up MFA in. You can add this right after sign-up, or as part of a separate flow in the settings portion of your app.
Add unenroll flow. You need to support a UI through which users can see existing devices and unenroll devices which are no longer relevant.
Add challenge step to login. If a user has set-up MFA, your app's login flow needs to present a challenge screen to the user asking them to prove they have access to the additional factor.
Enforce rules for MFA logins. Once your users have a way to enroll and log in with MFA, you need to enforce authorization rules across your app: on the frontend, backend, API servers or Row-Level Security policies.
The enrollment flow and the challenge steps differ by factor and are covered on a separate page. Visit the Phone or App Authenticator pages to see how to add the flows for the respective factors. You can combine both flows and allow for use of both Phone and App Authenticator Factors.

Add unenroll flow#
The unenroll process is the same for both Phone and TOTP factors.

An unenroll flow provides a UI for users to manage and unenroll factors linked to their accounts. Most applications do so via a factor management page where users can view and unlink selected factors.

When a user unenrolls a factor, call supabase.auth.mfa.unenroll() with the ID of the factor. For example, call:

supabase.auth.mfa.unenroll({ factorId: 'd30fd651-184e-4748-a928-0a4b9be1d429' })
to unenroll a factor with ID d30fd651-184e-4748-a928-0a4b9be1d429.

Enforce rules for MFA logins#
Adding MFA to your app's UI does not in-and-of-itself offer a higher level of security to your users. You also need to enforce the MFA rules in your application's database, APIs, and server-side rendering.

Depending on your application's needs, there are three ways you can choose to enforce MFA.

Enforce for all users (new and existing). Any user account will have to enroll MFA to continue using your app. The application will not allow access without going through MFA first.
Enforce for new users only. Only new users will be forced to enroll MFA, while old users will be encouraged to do so. The application will not allow access for new users without going through MFA first.
Enforce only for users that have opted-in. Users that want MFA can enroll in it and the application will not allow access without going through MFA first.
Example: React#
Below is an example that creates a new UnenrollMFA component that illustrates the important pieces of the MFA enrollment flow. Note that users can only unenroll a factor after completing the enrollment flow and obtaining an aal2 JWT claim. Here are some points of note:

When the component appears on screen, the supabase.auth.mfa.listFactors() endpoint fetches all existing factors together with their details.
The existing factors for a user are displayed in a table.
Once the user has selected a factor to unenroll, they can type in the factorId and click Unenroll which creates a confirmation modal.
Unenrolling a factor will downgrade the assurance level from aal2 to aal1 only after the refresh interval has lapsed. For an immediate downgrade from aal2 to aal1 after enrolling one will need to manually call refreshSession()

/**
 * UnenrollMFA shows a simple table with the list of factors together with a button to unenroll.
 * When a user types in the factorId of the factor that they wish to unenroll and clicks unenroll
 * the corresponding factor will be unenrolled.
 */
export function UnenrollMFA() {
  const [factorId, setFactorId] = useState('')
  const [factors, setFactors] = useState([])
  const [error, setError] = useState('') // holds an error message
  useEffect(() => {
    ;(async () => {
      const { data, error } = await supabase.auth.mfa.listFactors()
      if (error) {
        throw error
      }
      setFactors([...data.totp, ...data.phone])
    })()
  }, [])
  return (
    <>
      {error && <div className="error">{error}</div>}
      <tbody>
        <tr>
          <td>Factor ID</td>
          <td>Friendly Name</td>
          <td>Factor Status</td>
          <td>Phone Number</td>
        </tr>
        {factors.map((factor) => (
          <tr>
            <td>{factor.id}</td>
            <td>{factor.friendly_name}</td>
            <td>{factor.factor_type}</td>
            <td>{factor.status}</td>
            <td>{factor.phone}</td>
          </tr>
        ))}
      </tbody>
      <input type="text" value={verifyCode} onChange={(e) => setFactorId(e.target.value.trim())} />
      <button onClick={() => supabase.auth.mfa.unenroll({ factorId })}>Unenroll</button>
    </>
  )
}
Database#
Your app should sufficiently deny or allow access to tables or rows based on the user's current and possible authenticator levels.

Postgres has two types of policies: permissive and restrictive. This guide uses restrictive policies. Make sure you don't omit the as restrictive clause.

Enforce for all users (new and existing)
If your app falls under this case, this is a template Row Level Security policy you can apply to all your tables:

create policy "Policy name."
  on table_name
  as restrictive
  to authenticated
  using ((select auth.jwt()->>'aal') = 'aal2');
Here the policy will not accept any JWTs with an aal claim other than aal2, which is the highest authenticator assurance level.
Using as restrictive ensures this policy will restrict all commands on the table regardless of other policies!
Enforce for new users only
If your app falls under this case, the rules get more complex. User accounts created past a certain timestamp must have a aal2 level to access the database.

create policy "Policy name."
  on table_name
  as restrictive -- very important!
  to authenticated
  using
    (array[(select auth.jwt()->>'aal')] <@ (
       select
         case
           when created_at >= '2022-12-12T00:00:00Z' then array['aal2']
           else array['aal1', 'aal2']
         end as aal
       from auth.users
       where (select auth.uid()) = id));
The policy will accept both aal1 and aal2 for users with a created_at timestamp prior to 12th December 2022 at 00:00 UTC, but will only accept aal2 for all other timestamps.
The <@ operator is PostgreSQL's "contained in" operator.
Using as restrictive ensures this policy will restrict all commands on the table regardless of other policies!
Enforce only for users that have opted-in
Users that have enrolled MFA on their account are expecting that your
application only works for them if they've gone through MFA.

create policy "Policy name."
  on table_name
  as restrictive -- very important!
  to authenticated
  using (
    array[(select auth.jwt()->>'aal')] <@ (
      select
          case
            when count(id) > 0 then array['aal2']
            else array['aal1', 'aal2']
          end as aal
        from auth.mfa_factors
        where ((select auth.uid()) = user_id) and status = 'verified'
    ));
The policy will only accept only aal2 when the user has at least one MFA factor verified.
Otherwise, it will accept both aal1 and aal2.
The <@ operator is PostgreSQL's "contained in" operator.
Using as restrictive ensures this policy will restrict all commands on the table regardless of other policies!
Server-Side Rendering#
When using the Supabase JavaScript library in a server-side rendering context, make sure you always create a new object for each request! This will prevent you from accidentally rendering and serving content belonging to different users.

It is possible to enforce MFA on the Server-Side Rendering level. However, this can be tricky do to well.

You can use the supabase.auth.mfa.getAuthenticatorAssuranceLevel() and supabase.auth.mfa.listFactors() APIs to identify the AAL level of the session and any factors that are enabled for a user, similar to how you would use these on the browser.

However, encountering a different AAL level on the server may not actually be a security problem. Consider these likely scenarios:

User signed-in with a conventional method but closed their tab on the MFA flow.
User forgot a tab open for a very long time. (This happens more often than you might imagine.)
User has lost their authenticator device and is confused about the next steps.
We thus recommend you redirect users to a page where they can authenticate using their additional factor, instead of rendering a HTTP 401 Unauthorized or HTTP 403 Forbidden content.

APIs#
If your application uses the Supabase Database, Storage or Edge Functions, just using Row Level Security policies will give you sufficient protection. In the event that you have other APIs that you wish to protect, follow these general guidelines:

Use a good JWT verification and parsing library for your language. This will let you securely parse JWTs and extract their claims.
Retrieve the aal claim from the JWT and compare its value according to your needs. If you've encountered an AAL level that can be increased, ask the user to continue the login process instead of logging them out.
Use the https://<project-ref>.supabase.co/rest/v1/auth/factors REST endpoint to identify if the user has enrolled any MFA factors. Only verified factors should be acted upon.
Frequently asked questions#

How do I check when a user went through MFA?
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Adding to your app
Add unenroll flow
Enforce rules for MFA logins
Server-Side Rendering
APIs
Frequently asked questions
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author St

Auth

More
Multi-Factor Authentication
App Authenticator (TOTP)
Multi-Factor Authentication (TOTP)

How does app authenticator multi-factor authentication work?#
App Authenticator (TOTP) multi-factor authentication involves a timed one-time password generated from an authenticator app in the control of users. It uses a QR Code which to transmit a shared secret used to generate a One Time Password. A user can scan a QR code with their phone to capture a shared secret required for subsequent authentication.

The use of a QR code was initially introduced by Google Authenticator but is now universally accepted by all authenticator apps. The QR code has an alternate representation in URI form following the otpauth scheme such as: otpauth://totp/supabase:alice@supabase.com?secret=<secret>&issuer=supabase which a user can manually input in cases where there is difficulty rendering a QR Code.

Below is a flow chart illustrating how the Enrollment, Challenge, and Verify APIs work in the context of MFA (TOTP).

Diagram showing the flow of Multi-Factor authentication
TOTP MFA API is free to use and is enabled on all Supabase projects by default.

Add enrollment flow#
An enrollment flow provides a UI for users to set up additional authentication factors. Most applications add the enrollment flow in two places within their app:

Right after login or sign up. This lets users quickly set up MFA immediately after they log in or create an account. We recommend encouraging all users to set up MFA if that makes sense for your application. Many applications offer this as an opt-in step in an effort to reduce onboarding friction.
From within a settings page. Allows users to set up, disable or modify their MFA settings.
Enrolling a factor for use with MFA takes three steps:

Call supabase.auth.mfa.enroll(). This method returns a QR code and a secret. Display the QR code to the user and ask them to scan it with their authenticator application. If they are unable to scan the QR code, show the secret in plain text which they can type or paste into their authenticator app.
Calling the supabase.auth.mfa.challenge() API. This prepares Supabase Auth to accept a verification code from the user and returns a challenge ID. In the case of Phone MFA this step also sends the verification code to the user.
Calling the supabase.auth.mfa.verify() API. This verifies that the user has indeed added the secret from step (1) into their app and is working correctly. If the verification succeeds, the factor immediately becomes active for the user account. If not, you should repeat steps 2 and 3.
Example: React#
Below is an example that creates a new EnrollMFA component that illustrates the important pieces of the MFA enrollment flow.

When the component appears on screen, the supabase.auth.mfa.enroll() API is called once to start the process of enrolling a new factor for the current user.
This API returns a QR code in the SVG format, which is shown on screen using a normal <img> tag by encoding the SVG as a data URL.
Once the user has scanned the QR code with their authenticator app, they should enter the verification code within the verifyCode input field and click on Enable.
A challenge is created using the supabase.auth.mfa.challenge() API and the code from the user is submitted for verification using the supabase.auth.mfa.verify() challenge.
onEnabled is a callback that notifies the other components that enrollment has completed.
onCancelled is a callback that notifies the other components that the user has clicked the Cancel button.
/**
 * EnrollMFA shows a simple enrollment dialog. When shown on screen it calls
 * the `enroll` API. Each time a user clicks the Enable button it calls the
 * `challenge` and `verify` APIs to check if the code provided by the user is
 * valid.
 * When enrollment is successful, it calls `onEnrolled`. When the user clicks
 * Cancel the `onCancelled` callback is called.
 */
export function EnrollMFA({
  onEnrolled,
  onCancelled,
}: {
  onEnrolled: () => void
  onCancelled: () => void
}) {
  const [factorId, setFactorId] = useState('')
  const [qr, setQR] = useState('') // holds the QR code image SVG
  const [verifyCode, setVerifyCode] = useState('') // contains the code entered by the user
  const [error, setError] = useState('') // holds an error message
  const onEnableClicked = () => {
    setError('')
    ;(async () => {
      const challenge = await supabase.auth.mfa.challenge({ factorId })
      if (challenge.error) {
        setError(challenge.error.message)
        throw challenge.error
      }
      const challengeId = challenge.data.id
      const verify = await supabase.auth.mfa.verify({
        factorId,
        challengeId,
        code: verifyCode,
      })
      if (verify.error) {
        setError(verify.error.message)
        throw verify.error
      }
      onEnrolled()
    })()
  }
  useEffect(() => {
    ;(async () => {
      const { data, error } = await supabase.auth.mfa.enroll({
        factorType: 'totp',
      })
      if (error) {
        throw error
      }
      setFactorId(data.id)
      // Supabase Auth returns an SVG QR code which you can convert into a data
      // URL that you can place in an <img> tag.
      setQR(data.totp.qr_code)
    })()
  }, [])
  return (
    <>
      {error && <div className="error">{error}</div>}
      <img src={qr} />
      <input
        type="text"
        value={verifyCode}
        onChange={(e) => setVerifyCode(e.target.value.trim())}
      />
      <input type="button" value="Enable" onClick={onEnableClicked} />
      <input type="button" value="Cancel" onClick={onCancelled} />
    </>
  )
}
Add a challenge step to login#
Once a user has logged in via their first factor (email+password, magic link, one time password, social login etc.) you need to perform a check if any additional factors need to be verified.

This can be done by using the supabase.auth.mfa.getAuthenticatorAssuranceLevel() API. When the user signs in and is redirected back to your app, you should call this method to extract the user's current and next authenticator assurance level (AAL).

Therefore if you receive a currentLevel which is aal1 but a nextLevel of aal2, the user should be given the option to go through MFA.

Below is a table that explains the combined meaning.

Current Level	Next Level	Meaning
aal1	aal1	User does not have MFA enrolled.
aal1	aal2	User has an MFA factor enrolled but has not verified it.
aal2	aal2	User has verified their MFA factor.
aal2	aal1	User has disabled their MFA factor. (Stale JWT.)
Example: React#
Adding the challenge step to login depends heavily on the architecture of your app. However, a fairly common way to structure React apps is to have a large component (often named App) which contains most of the authenticated application logic.

This example will wrap this component with logic that will show an MFA challenge screen if necessary, before showing the full application. This is illustrated in the AppWithMFA example below.

function AppWithMFA() {
  const [readyToShow, setReadyToShow] = useState(false)
  const [showMFAScreen, setShowMFAScreen] = useState(false)
  useEffect(() => {
    ;(async () => {
      try {
        const { data, error } = await supabase.auth.mfa.getAuthenticatorAssuranceLevel()
        if (error) {
          throw error
        }
        console.log(data)
        if (data.nextLevel === 'aal2' && data.nextLevel !== data.currentLevel) {
          setShowMFAScreen(true)
        }
      } finally {
        setReadyToShow(true)
      }
    })()
  }, [])
  if (readyToShow) {
    if (showMFAScreen) {
      return <AuthMFA />
    }
    return <App />
  }
  return <></>
}
supabase.auth.mfa.getAuthenticatorAssuranceLevel() does return a promise. Don't worry, this is a very fast method (microseconds) as it rarely uses the network.
readyToShow only makes sure the AAL check completes before showing any application UI to the user.
If the current level can be upgraded to the next one, the MFA screen is shown.
Once the challenge is successful, the App component is finally rendered on screen.
Below is the component that implements the challenge and verify logic.

function AuthMFA() {
  const [verifyCode, setVerifyCode] = useState('')
  const [error, setError] = useState('')
  const onSubmitClicked = () => {
    setError('')
    ;(async () => {
      const factors = await supabase.auth.mfa.listFactors()
      if (factors.error) {
        throw factors.error
      }
      const totpFactor = factors.data.totp[0]
      if (!totpFactor) {
        throw new Error('No TOTP factors found!')
      }
      const factorId = totpFactor.id
      const challenge = await supabase.auth.mfa.challenge({ factorId })
      if (challenge.error) {
        setError(challenge.error.message)
        throw challenge.error
      }
      const challengeId = challenge.data.id
      const verify = await supabase.auth.mfa.verify({
        factorId,
        challengeId,
        code: verifyCode,
      })
      if (verify.error) {
        setError(verify.error.message)
        throw verify.error
      }
    })()
  }
  return (
    <>
      <div>Please enter the code from your authenticator app.</div>
      {error && <div className="error">{error}</div>}
      <input
        type="text"
        value={verifyCode}
        onChange={(e) => setVerifyCode(e.target.value.trim())}
      />
      <input type="button" value="Submit" onClick={onSubmitClicked} />
    </>
  )
}
You can extract the available MFA factors for the user by calling supabase.auth.mfa.listFactors(). Don't worry this method is also very quick and rarely uses the network.
If listFactors() returns more than one factor (or of a different type) you should present the user with a choice. For simplicity this is not shown in the example.
Each time the user presses the "Submit" button a new challenge is created for the chosen factor (in this case the first one) and it is immediately verified. Any errors are displayed to the user.
On successful verification, the client library will refresh the session in the background automatically and finally call the onSuccess callback, which will show the authenticated App component on screen.
Frequently asked questions#

What's inside the QR code?

How long is the TOTP code valid for?
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How does app authenticator multi-factor authentication work?
Add enrollment flow
Add a challenge step to login
Frequently asked questions
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author St

Auth

More
Multi-Factor Authentication
Phone
Multi-Factor Authentication (Phone)

How does phone multi-factor-authentication work?#
Phone multi-factor authentication involves a shared code generated by Supabase Auth and the end user. The code is delivered via a messaging channel, such as SMS or WhatsApp, and the user uses the code to authenticate to Supabase Auth.

The phone messaging configuration for MFA is shared with phone auth login. The same provider configuration that is used for phone login is used for MFA. You can also use the Send SMS Hook if you need to use an MFA (Phone) messaging provider different from what is supported natively.

Below is a flow chart illustrating how the Enrollment and Verify APIs work in the context of MFA (Phone).

Diagram showing the flow of Multi-Factor authentication
Add enrollment flow#
An enrollment flow provides a UI for users to set up additional authentication factors. Most applications add the enrollment flow in two places within their app:

Right after login or sign up. This allows users quickly set up Multi Factor Authentication (MFA) post login or account creation. Where possible, encourage all users to set up MFA. Many applications offer this as an opt-in step in an effort to reduce onboarding friction.
From within a settings page. Allows users to set up, disable or modify their MFA settings.
As far as possible, maintain a generic flow that you can reuse in both cases with minor modifications.

Enrolling a factor for use with MFA takes three steps for phone MFA:

Call supabase.auth.mfa.enroll().
Calling the supabase.auth.mfa.challenge() API. This sends a code via SMS or WhatsApp and prepares Supabase Auth to accept a verification code from the user.
Calling the supabase.auth.mfa.verify() API. supabase.auth.mfa.challenge() returns a challenge ID. This verifies that the code issued by Supabase Auth matches the code input by the user. If the verification succeeds, the factor immediately becomes active for the user account. If not, you should repeat steps 2 and 3.
Example: React#
Below is an example that creates a new EnrollMFA component that illustrates the important pieces of the MFA enrollment flow.

When the component appears on screen, the supabase.auth.mfa.enroll() API is called once to start the process of enrolling a new factor for the current user.
A challenge is created using the supabase.auth.mfa.challenge() API and the code from the user is submitted for verification using the supabase.auth.mfa.verify() challenge.
onEnabled is a callback that notifies the other components that enrollment has completed.
onCancelled is a callback that notifies the other components that the user has clicked the Cancel button.
export function EnrollMFA({
  onEnrolled,
  onCancelled,
}: {
  onEnrolled: () => void
  onCancelled: () => void
}) {
  const [phoneNumber, setPhoneNumber] = useState('')
  const [factorId, setFactorId] = useState('')
  const [verifyCode, setVerifyCode] = useState('')
  const [error, setError] = useState('')
  const [challengeId, setChallengeId] = useState('')
  const onEnableClicked = () => {
    setError('')
    ;(async () => {
      const verify = await auth.mfa.verify({
        factorId,
        challengeId,
        code: verifyCode,
      })
      if (verify.error) {
        setError(verify.error.message)
        throw verify.error
      }
      onEnrolled()
    })()
  }
  const onEnrollClicked = async () => {
    setError('')
    try {
      const factor = await auth.mfa.enroll({
        phone: phoneNumber,
        factorType: 'phone',
      })
      if (factor.error) {
        setError(factor.error.message)
        throw factor.error
      }
      setFactorId(factor.data.id)
    } catch (error) {
      setError('Failed to Enroll the Factor.')
    }
  }
  const onSendOTPClicked = async () => {
    setError('')
    try {
      const challenge = await auth.mfa.challenge({ factorId })
      if (challenge.error) {
        setError(challenge.error.message)
        throw challenge.error
      }
      setChallengeId(challenge.data.id)
    } catch (error) {
      setError('Failed to resend the code.')
    }
  }
  return (
    <>
      {error && <div className="error">{error}</div>}
      <input
        type="text"
        placeholder="Phone Number"
        value={phoneNumber}
        onChange={(e) => setPhoneNumber(e.target.value.trim())}
      />
      <input
        type="text"
        placeholder="Verification Code"
        value={verifyCode}
        onChange={(e) => setVerifyCode(e.target.value.trim())}
      />
      <input type="button" value="Enroll" onClick={onEnrollClicked} />
      <input type="button" value="Submit Code" onClick={onEnableClicked} />
      <input type="button" value="Send OTP Code" onClick={onSendOTPClicked} />
      <input type="button" value="Cancel" onClick={onCancelled} />
    </>
  )
}
Add a challenge step to login#
Once a user has logged in via their first factor (email+password, magic link, one time password, social login etc.) you need to perform a check if any additional factors need to be verified.

This can be done by using the supabase.auth.mfa.getAuthenticatorAssuranceLevel() API. When the user signs in and is redirected back to your app, you should call this method to extract the user's current and next authenticator assurance level (AAL).

Therefore if you receive a currentLevel which is aal1 but a nextLevel of aal2, the user should be given the option to go through MFA.

Below is a table that explains the combined meaning.

Current Level	Next Level	Meaning
aal1	aal1	User does not have MFA enrolled.
aal1	aal2	User has an MFA factor enrolled but has not verified it.
aal2	aal2	User has verified their MFA factor.
aal2	aal1	User has disabled their MFA factor. (Stale JWT.)
Example: React#
Adding the challenge step to login depends heavily on the architecture of your app. However, a fairly common way to structure React apps is to have a large component (often named App) which contains most of the authenticated application logic.

This example will wrap this component with logic that will show an MFA challenge screen if necessary, before showing the full application. This is illustrated in the AppWithMFA example below.

function AppWithMFA() {
  const [readyToShow, setReadyToShow] = useState(false)
  const [showMFAScreen, setShowMFAScreen] = useState(false)
  useEffect(() => {
    ;(async () => {
      try {
        const { data, error } = await supabase.auth.mfa.getAuthenticatorAssuranceLevel()
        if (error) {
          throw error
        }
        console.log(data)
        if (data.nextLevel === 'aal2' && data.nextLevel !== data.currentLevel) {
          setShowMFAScreen(true)
        }
      } finally {
        setReadyToShow(true)
      }
    })()
  }, [])
  if (readyToShow) {
    if (showMFAScreen) {
      return <AuthMFA />
    }
    return <App />
  }
  return <></>
}
supabase.auth.mfa.getAuthenticatorAssuranceLevel() does return a promise. Don't worry, this is a very fast method (microseconds) as it rarely uses the network.
readyToShow only makes sure the AAL check completes before showing any application UI to the user.
If the current level can be upgraded to the next one, the MFA screen is shown.
Once the challenge is successful, the App component is finally rendered on screen.
Below is the component that implements the challenge and verify logic.

function AuthMFA() {
  const [verifyCode, setVerifyCode] = useState('')
  const [error, setError] = useState('')
  const [factorId, setFactorId] = useState('')
  const [challengeId, setChallengeId] = useState('')
  const [phoneNumber, setPhoneNumber] = useState('')
  const startChallenge = async () => {
    setError('')
    try {
      const factors = await supabase.auth.mfa.listFactors()
      if (factors.error) {
        throw factors.error
      }
      const phoneFactor = factors.data.phone[0]
      if (!phoneFactor) {
        throw new Error('No phone factors found!')
      }
      const factorId = phoneFactor.id
      setFactorId(factorId)
      setPhoneNumber(phoneFactor.phone)
      const challenge = await supabase.auth.mfa.challenge({ factorId })
      if (challenge.error) {
        setError(challenge.error.message)
        throw challenge.error
      }
      setChallengeId(challenge.data.id)
    } catch (error) {
      setError(error.message)
    }
  }
  const verifyCode = async () => {
    setError('')
    try {
      const verify = await supabase.auth.mfa.verify({
        factorId,
        challengeId,
        code: verifyCode,
      })
      if (verify.error) {
        setError(verify.error.message)
        throw verify.error
      }
    } catch (error) {
      setError(error.message)
    }
  }
  return (
    <>
      <div>Please enter the code sent to your phone.</div>
      {phoneNumber && <div>Phone number: {phoneNumber}</div>}
      {error && <div className="error">{error}</div>}
      <input
        type="text"
        value={verifyCode}
        onChange={(e) => setVerifyCode(e.target.value.trim())}
      />
      {!challengeId ? (
        <input type="button" value="Start Challenge" onClick={startChallenge} />
      ) : (
        <input type="button" value="Verify Code" onClick={verifyCode} />
      )}
    </>
  )
}
You can extract the available MFA factors for the user by calling supabase.auth.mfa.listFactors(). Don't worry this method is also very quick and rarely uses the network.
If listFactors() returns more than one factor (or of a different type) you should present the user with a choice. For simplicity this is not shown in the example.
Phone numbers are unique per user. Users can only have one verified phone factor with a given phone number. Attempting to enroll a new phone factor alongside an existing verified factor with the same number will result in an error.
Each time the user presses the "Submit" button a new challenge is created for the chosen factor (in this case the first one)
On successful verification, the client library will refresh the session in the background automatically and finally call the onSuccess callback, which will show the authenticated App component on screen.
Security configuration#
Each code is valid for up to 5 minutes, after which a new one can be sent. Successive codes remain valid until expiry. When possible choose the longest code length acceptable to your use case, at a minimum of 6. This can be configured in the Authentication Settings.

Be aware that Phone MFA is vulnerable to SIM swap attacks where an attacker will call a mobile provider and ask to port the target's phone number to a new SIM card and then use the said SIM card to intercept an MFA code. Evaluate the your application's tolerance for such an attack. You can read more about SIM swapping attacks here

Pricing#
$0.1027 per hour ($75 per month) for the first project. $0.0137 per
hour ($10 per month) for every additional project.

Plan	Project 1 per month	Project 2 per month	Project 3 per month
Pro	$75	$10	$10
Team	$75	$10	$10
Enterprise	Custom	Custom	Custom
For a detailed breakdown of how charges are calculated, refer to Manage Advanced MFA Phone usage.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How does phone multi-factor-authentication work?
Add enrollment flow
Add a challenge step to login
Security configuration
Pricing
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Author St

Signing out

Signing out a user

Signing out a user works the same way no matter what method they used to sign in.

Call the sign out method from the client library. It removes the active session and clears Auth data from the storage medium.


JavaScript

Dart

Swift

Kotlin

Python
async function signOut() {
  const { error } = await supabase.auth.signOut()
}
Sign out and scopes#
Supabase Auth allows you to specify three different scopes for when a user invokes the sign out API in your application:

global (default) when all sessions active for the user are terminated.
local which only terminates the current session for the user but keep sessions on other devices or browsers active.
others to terminate all but the current session for the user.
You can invoke these by providing the scope option:


JavaScript

Dart

Kotlin
// defaults to the global scope
await supabase.auth.signOut()
// sign out from the current session only
await supabase.auth.signOut({ scope: 'local' })
Upon sign out, all refresh tokens and potentially other database objects related to the affected sessions are destroyed and the client library removes the session stored in the local storage medium.

Access Tokens of revoked sessions remain valid until their expiry time, encoded in the exp claim. The user won't be immediately logged out and will only be logged out when the Access Token expires.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Sign out and scopes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Debugging
Error Codes
Error Codes

Learn about the Auth error codes and how to resolve them

Auth error codes#
Supabase Auth can return various errors when using its API. This guide explains how to handle these errors effectively across different programming languages.

Error types#
Supabase Auth errors are generally categorized into two main types:

API Errors: Originate from the Supabase Auth API.
Client Errors: Originate from the client library's state.
Client errors differ by language so do refer to the appropriate section below:


JavaScript

Dart

Swift

Python

Kotlin
All errors originating from the supabase.auth namespace of the client library will be wrapped by the AuthError class.

Error objects are split in a few classes:

AuthApiError -- errors which originate from the Supabase Auth API.
Use isAuthApiError instead of instanceof checks to see if an error you caught is of this type.
CustomAuthError -- errors which generally originate from state in the client library.
Use the name property on the error to identify the class of error received.
Errors originating from the server API classed as AuthApiError always have a code property that can be used to identify the error returned by the server. The status property is also present, encoding the HTTP status code received in the response.

HTTP status codes#
Below are the most common HTTP status codes you might encounter, along with their meanings in the context of Supabase Auth:

403 Forbidden#
Sent out in rare situations where a certain Auth feature is not available for the user, and you as the developer are not checking a precondition whether that API is available for the user.

422 Unprocessable Entity#
Sent out when the API request is accepted, but cannot be processed because the user or Auth server is in a state where it cannot satisfy the request.

429 Too Many Requests#
Sent out when rate-limits are breached for an API. You should handle this status code often, especially in functions that authenticate a user.

500 Internal Server Error#
Indicate that the Auth server's service is degraded. Most often it points to issues in your database setup such as a misbehaving trigger on a schema, function, view or other database object.

501 Not Implemented#
Sent out when a feature is not enabled on the Auth server, and you are trying to use an API which requires it.

Auth error codes table#
The following table provides a comprehensive list of error codes you may encounter when working with Supabase Auth. Each error code is associated with a specific issue and includes a description to help you understand and resolve the problem efficiently.

Error code	Description
anonymous_provider_disabled	
Anonymous sign-ins are disabled.

bad_code_verifier	
Returned from the PKCE flow where the provided code verifier does not match the expected one. Indicates a bug in the implementation of the client library.

bad_json	
Usually used when the HTTP body of the request is not valid JSON.

bad_jwt	
JWT sent in the Authorization header is not valid.

bad_oauth_callback	
OAuth callback from provider to Auth does not have all the required attributes (state). Indicates an issue with the OAuth provider or client library implementation.

bad_oauth_state	
OAuth state (data echoed back by the OAuth provider to Supabase Auth) is not in the correct format. Indicates an issue with the OAuth provider integration.

captcha_failed	
CAPTCHA challenge could not be verified with the CAPTCHA provider. Check your CAPTCHA integration.

conflict	
General database conflict, such as concurrent requests on resources that should not be modified concurrently. Can often occur when you have too many session refresh requests firing off at the same time for a user. Check your app for concurrency issues, and if detected, back off exponentially.

email_address_invalid	
Example and test domains are currently not supported. Use a different email address.

email_address_not_authorized	
Email sending is not allowed for this address as your project is using the default SMTP service. Emails can only be sent to members in your Supabase organization. If you want to send emails to others, set up a custom SMTP provider.

Learn more:

Setting up a custom SMTP provider
email_conflict_identity_not_deletable	
Unlinking this identity causes the user's account to change to an email address which is already used by another user account. Indicates an issue where the user has two different accounts using different primary email addresses. You may need to migrate user data to one of their accounts in this case.

email_exists	
Email address already exists in the system.

email_not_confirmed	
Signing in is not allowed for this user as the email address is not confirmed.

email_provider_disabled	
Signups are disabled for email and password.

flow_state_expired	
PKCE flow state to which the API request relates has expired. Ask the user to sign in again.

flow_state_not_found	
PKCE flow state to which the API request relates no longer exists. Flow states expire after a while and are progressively cleaned up, which can cause this error. Retried requests can cause this error, as the previous request likely destroyed the flow state. Ask the user to sign in again.

hook_payload_invalid_content_type	
Payload from Auth does not have a valid Content-Type header.

hook_payload_over_size_limit	
Payload from Auth exceeds maximum size limit.

hook_timeout	
Unable to reach hook within maximum time allocated.

hook_timeout_after_retry	
Unable to reach hook after maximum number of retries.

identity_already_exists	
The identity to which the API relates is already linked to a user.

identity_not_found	
Identity to which the API call relates does not exist, such as when an identity is unlinked or deleted.

insufficient_aal	
To call this API, the user must have a higher Authenticator Assurance Level. To resolve, ask the user to solve an MFA challenge.

Learn more:

MFA
invalid_credentials	
Login credentials or grant type not recognized.

invite_not_found	
Invite is expired or already used.

manual_linking_disabled	
Calling the supabase.auth.linkUser() and related APIs is not enabled on the Auth server.

mfa_challenge_expired	
Responding to an MFA challenge should happen within a fixed time period. Request a new challenge when encountering this error.

mfa_factor_name_conflict	
MFA factors for a single user should not have the same friendly name.

mfa_factor_not_found	
MFA factor no longer exists.

mfa_ip_address_mismatch	
The enrollment process for MFA factors must begin and end with the same IP address.

mfa_phone_enroll_not_enabled	
Enrollment of MFA Phone factors is disabled.

mfa_phone_verify_not_enabled	
Login via Phone factors and verification of new Phone factors is disabled.

mfa_totp_enroll_not_enabled	
Enrollment of MFA TOTP factors is disabled.

mfa_totp_verify_not_enabled	
Login via TOTP factors and verification of new TOTP factors is disabled.

mfa_verification_failed	
MFA challenge could not be verified -- wrong TOTP code.

mfa_verification_rejected	
Further MFA verification is rejected. Only returned if the MFA verification attempt hook returns a reject decision.

Learn more:

MFA verification hook
mfa_verified_factor_exists	
Verified phone factor already exists for a user. Unenroll existing verified phone factor to continue.

mfa_web_authn_enroll_not_enabled	
Enrollment of MFA Web Authn factors is disabled.

mfa_web_authn_verify_not_enabled	
Login via WebAuthn factors and verification of new WebAuthn factors is disabled.

no_authorization	
This HTTP request requires an Authorization header, which is not provided.

not_admin	
User accessing the API is not admin, i.e. the JWT does not contain a role claim that identifies them as an admin of the Auth server.

oauth_provider_not_supported	
Using an OAuth provider which is disabled on the Auth server.

otp_disabled	
Sign in with OTPs (magic link, email OTP) is disabled. Check your server's configuration.

otp_expired	
OTP code for this sign-in has expired. Ask the user to sign in again.

over_email_send_rate_limit	
Too many emails have been sent to this email address. Ask the user to wait a while before trying again.

over_request_rate_limit	
Too many requests have been sent by this client (IP address). Ask the user to try again in a few minutes. Sometimes can indicate a bug in your application that mistakenly sends out too many requests (such as a badly written useEffect React hook).

Learn more:

React useEffect hook
over_sms_send_rate_limit	
Too many SMS messages have been sent to this phone number. Ask the user to wait a while before trying again.

phone_exists	
Phone number already exists in the system.

phone_not_confirmed	
Signing in is not allowed for this user as the phone number is not confirmed.

phone_provider_disabled	
Signups are disabled for phone and password.

provider_disabled	
OAuth provider is disabled for use. Check your server's configuration.

provider_email_needs_verification	
Not all OAuth providers verify their user's email address. Supabase Auth requires emails to be verified, so this error is sent out when a verification email is sent after completing the OAuth flow.

reauthentication_needed	
A user needs to reauthenticate to change their password. Ask the user to reauthenticate by calling the supabase.auth.reauthenticate() API.

reauthentication_not_valid	
Verifying a reauthentication failed, the code is incorrect. Ask the user to enter a new code.

refresh_token_already_used	
Refresh token has been revoked and falls outside the refresh token reuse interval. See the documentation on sessions for further information.

Learn more:

Auth sessions
refresh_token_not_found	
Session containing the refresh token not found.

request_timeout	
Processing the request took too long. Retry the request.

same_password	
A user that is updating their password must use a different password than the one currently used.

saml_assertion_no_email	
SAML assertion (user information) was received after sign in, but no email address was found in it, which is required. Check the provider's attribute mapping and/or configuration.

saml_assertion_no_user_id	
SAML assertion (user information) was received after sign in, but a user ID (called NameID) was not found in it, which is required. Check the SAML identity provider's configuration.

saml_entity_id_mismatch	
(Admin API.) Updating the SAML metadata for a SAML identity provider is not possible, as the entity ID in the update does not match the entity ID in the database. This is equivalent to creating a new identity provider, and you should do that instead.

saml_idp_already_exists	
(Admin API.) Adding a SAML identity provider that is already added.

saml_idp_not_found	
SAML identity provider not found. Most often returned after IdP-initiated sign-in with an unregistered SAML identity provider in Supabase Auth.

saml_metadata_fetch_failed	
(Admin API.) Adding or updating a SAML provider failed as its metadata could not be fetched from the provided URL.

saml_provider_disabled	
Using Enterprise SSO with SAML 2.0 is not enabled on the Auth server.

Learn more:

Enterprise SSO
saml_relay_state_expired	
SAML relay state is an object that tracks the progress of a supabase.auth.signInWithSSO() request. The SAML identity provider should respond after a fixed amount of time, after which this error is shown. Ask the user to sign in again.

saml_relay_state_not_found	
SAML relay states are progressively cleaned up after they expire, which can cause this error. Ask the user to sign in again.

session_expired	
Session to which the API request relates has expired. This can occur if an inactivity timeout is configured, or the session entry has exceeded the configured timebox value. See the documentation on sessions for more information.

Learn more:

Auth sessions
session_not_found	
Session to which the API request relates no longer exists. This can occur if the user has signed out, or the session entry in the database was deleted in some other way.

signup_disabled	
Sign ups (new account creation) are disabled on the server.

single_identity_not_deletable	
Every user must have at least one identity attached to it, so deleting (unlinking) an identity is not allowed if it's the only one for the user.

sms_send_failed	
Sending an SMS message failed. Check your SMS provider configuration.

sso_domain_already_exists	
(Admin API.) Only one SSO domain can be registered per SSO identity provider.

sso_provider_not_found	
SSO provider not found. Check the arguments in supabase.auth.signInWithSSO().

too_many_enrolled_mfa_factors	
A user can only have a fixed number of enrolled MFA factors.

unexpected_audience	
(Deprecated feature not available via Supabase client libraries.) The request's X-JWT-AUD claim does not match the JWT's audience.

unexpected_failure	
Auth service is degraded or a bug is present, without a specific reason.

user_already_exists	
User with this information (email address, phone number) cannot be created again as it already exists.

user_banned	
User to which the API request relates has a banned_until property which is still active. No further API requests should be attempted until this field is cleared.

user_not_found	
User to which the API request relates no longer exists.

user_sso_managed	
When a user comes from SSO, certain fields of the user cannot be updated (like email).

validation_failed	
Provided parameters are not in the expected format.

weak_password	
User is signing up or changing their password without meeting the password strength criteria. Use the AuthWeakPasswordError class to access more information about what they need to do to make the password pass.

Best practices for error handling#
Always use error.code and error.name to identify errors, not string matching on error messages.
Avoid relying solely on HTTP status codes, as they may change unexpectedly.
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Auth error codes
Error types
HTTP status codes
403 Forbidden
422 Unprocessable Entity
429 Too Many Requests
500 Internal Server Error
501 Not Implemented
Auth error codes table
Best practices for error handling
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth Troubleshooting
Search or browse troubleshooting guides for common authentication issues, including login problems, session management, and provider configuration.

Search and filter

Error codes

Tags
Search by keyword
Reset filters
Matching troubleshooting entries
All about Supabase Egress
Platform
Database
Functions
Storage
Realtime
Auth
Supavisor
Nov 6
An "invalid response was received from the upstream server" error when querying auth
Auth
Sep 9
Auth error: {401: invalid claim: missing sub}
401, 403
Auth
Feb 4
Change email associated with Supabase account
Platform
Auth
Sep 22
Change Project Region
Platform
Database
Auth
Mar 12
Check usage for monthly active users (MAU)
Auth
Platform
Sep 9
Converted GitHub account to organization / GitHub Account was deleted - Lost Supabase account access
Auth
Nov 3
Customizing Emails by Language
Auth
Sep 9
Errors when creating / updating / deleting users
500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure, 500 unexpected_failure,
Auth
Database
Studio
Nov 10
Database error saving new user
Auth
Studio
Jan 17
Email template not updating
templatemailer_template_body_parse_error
Auth
Studio
Nov 12
Error: "Invalid TOTP code entered"
Auth
Feb 21
Fetch requests to API endpoints aren't showing the session
Auth
Jan 18
Google Auth fails for some users
500 server_error, 401 UNAUTHENTICATED
Auth
Jan 17
How do I check GoTrue/API version of a Supabase project?
Auth
Platform
Jan 17
How do I make the cookies HttpOnly?
Auth
Jan 17
How do you troubleshoot Next.js - Supabase Auth issues?
Auth
Oct 27
How to Migrate from Supabase Auth Helpers to SSR package
Auth
Sep 9
I am not receiving password reset emails for Supabase dashboard
Auth
Platform
Feb 21
Lost access/Forgot the MFA device
Auth
Platform
Sep 9
Migrating Auth Users Between Supabase Projects
Auth
Sep 9
Next.js 13/14 stale data when changing RLS or table data.
Auth
Platform
Feb 21
Not receiving Auth emails from the Supabase project
Auth
Platform
Sep 9
OAuth sign in isn't redirecting on the server side
Auth
Feb 4
Performing administration tasks on the server side with the service_role secret
Auth
Platform
May 27
PGRST106: "The schema must be one of the following..." error when querying an exposed schema
PGRST106
Auth
Database
Nov 12
Resolving 500 Status Authentication Errors
500
Auth
Database
Sep 9
RLS Performance and Best Practices
Auth
Database
Apr 4
RLS Simplified
Database
Auth
Feb 21
Rotating Anon, Service, and JWT Secrets
Auth
Platform
Sep 9
Security of Anonymous Sign-ins
Auth
Platform
Sep 9
Should I set a shorter Max-Age parameter on the cookies?
Auth
Nov 3
Supabase CLI: "failed SASL auth" or "invalid SCRAM server-final-message"
Auth
Cli
Database
Supavisor
Nov 12
Using Google SMTP with Supabase Custom SMTP
Auth
Platform
Feb 21
Why am I being redirected to the wrong url when using auth redirectTo option?
Auth
Oct 29
Why do I see Auth & API requests in the dashboard? My app has no users
Auth
Platform
Database
Realtime
Functions
Feb 4
Why is my select returning an empty data array and I have data in the table?
Database
Auth
Feb 21
Why is my service role key client getting RLS errors or not returning data?
Auth
Database
Feb 4
Why is my supabase API call not returning?
Auth
Platform
Feb 4
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Third-party auth
Overview
Third-party auth

First-class support for authentication providers

Supabase has first-class support for these third-party authentication providers:

Clerk
Firebase Auth
Auth0
AWS Cognito (with or without AWS Amplify)
WorkOS
You can use these providers alongside Supabase Auth, or on their own, to access the Data API (REST and GraphQL), Storage, Realtime and Functions from your existing apps.

If you already have production apps using one of these authentication providers, and would like to use a Supabase feature, you no longer need to migrate your users to Supabase Auth or use workarounds like translating JWTs into the Supabase Auth format and using your project's signing secret.

How does it work?#
To use Supabase products like Data APIs for your Postgres database, Storage or Realtime, you often need to send access tokens or JWTs via the Supabase client libraries or via the REST API. Third-party auth support means that when you add a new integration with one of these providers, the API will trust JWTs issued by the provider similar to how it trusts JWTs issued by Supabase Auth.

This is made possible if the providers are using JWTs signed with asymmetric keys, which means that the Supabase APIs will be able to only verify but not create JWTs.

Limitations#
There are some limitations you should be aware of when using third-party authentication providers with Supabase.

The third-party provider must use asymmetrically signed JWTs (exposed as an OIDC Issuer Discovery URL by the third-party authentication provider). The signed JWTs must have a kid header parameter to identify which key must be used. Using symmetrically signed JWTs is not possible at this time.
The JWT signing keys from the third-party provider are stored in the configuration of your project, and are checked for changes periodically. If you are rotating your keys (when supported) allow up to 30 minutes for the change to be picked up.
It is not possible to disable Supabase Auth at this time.
Pricing#
$0.00325 per Third-Party MAU. You are only charged for usage exceeding your subscription
plan's quota.

For a detailed breakdown of how charges are calculated, refer to Manage Monthly Active Third-Party Users usage.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How does it work?
Limitations
Pricing
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Third-party auth
Clerk
Clerk

Use Clerk with your Supabase project

Clerk can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.

Getting started#
Getting started is incredibly easy. Start off by visiting Clerk's Connect with Supabase page to configure your Clerk instance for Supabase compatibility.

Finally add a new Third-Party Auth integration with Clerk in the Supabase dashboard.

Configure for local development or self-hosting#
When developing locally or self-hosting with the Supabase CLI, add the following config to your supabase/config.toml file:

[auth.third_party.clerk]
enabled = true
domain = "example.clerk.accounts.dev"
You will still need to configure your Clerk instance for Supabase compatibility.

Manually configuring your Clerk instance#
If you are not able to use Clerk's Connect with Supabase page to configure your Clerk instance for working with Supabase, follow these steps.

Add the role claim to Clerk session tokens by customizing them. End-users who are authenticated should have the authenticated value for the claim. If you have an advanced Postgres setup where authenticated end-users use different Postgres roles to access the database, adjust the value to use the correct role name.
Once all Clerk session tokens for your instance contain the role claim, add a new Third-Party Auth integration with Clerk in the Supabase dashboard or register it in the CLI as instructed above.
Setup the Supabase client library#

TypeScript

Flutter

Swift (iOS)
const supabaseClient = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY!,
    {
      // Session accessed from Clerk SDK, either as Clerk.session (vanilla
      // JavaScript) or useSession (React)
      accessToken: async () => session?.getToken() ?? null,
    }
  )
View source
Using RLS policies#
Once you've configured the Supabase client library to use Clerk session tokens, you can use RLS policies to secure access to your project's database, Storage objects and Realtime channels.

The recommended way to design RLS policies with Clerk is to use claims present in your Clerk session token to allow or reject access to your project's data. Check Clerk's docs on the available JWT claims and their values.

Example: Check user organization role#
create policy "Only organization admins can insert in table"
on secured_table
for insert
to authenticated
with check (
  (((select auth.jwt()->>'org_role') = 'org:admin') or ((select auth.jwt()->'o'->>'rol') = 'admin'))
    and
  (organization_id = (select coalesce(auth.jwt()->>'org_id', auth.jwt()->'o'->>'id')))
);
View source
This RLS policy checks that the newly inserted row in the table has the user's declared organization ID in the organization_id column. Additionally it ensures that they're an org:admin.

This way only organization admins can add rows to the table, for organizations they're a member of.

Example: Check user has passed second factor verification#
create policy "Only users that have passed second factor verification can read from table"
on secured_table
as restrictive
for select
to authenticated
using (
  ((select auth.jwt()->'fva'->>1) != '-1')
);
View source
This example uses a restrictive RLS policy checks that the second factor verification age element in the fva claim is not '-1' indicating the user has passed through second factor verification.

Deprecated integration with JWT templates#
As of 1st April 2025 the previously available Clerk Integration with Supabase is considered deprecated and is no longer recommended for use. All projects using the deprecated integration will be excluded from Third-Party Monthly Active User (TP-MAU) charges until at least 1st January 2026.

This integration used low-level primitives that are still available in Supabase and Clerk, such as a configurable JWT secret and JWT templates from Clerk. This enables you to keep using it in an unofficial manner, though only limited support will be provided from Supabase.

Deprecation is done for the following reasons:

Sharing your project's JWT secret with a third-party is a problematic security practice
Rotating the project's JWT secret in this case almost always results in significant downtime for your application
Additional latency to generate a new JWT for use with Supabase, instead of using the Clerk session tokens
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Getting started
Configure for local development or self-hosting
Manually configuring your Clerk instance
Setup the Supabase client library
Using RLS policies
Example: Check user organization role
Example: Check user has passed second factor verification
Deprecated integration with JWT templates
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Third-party auth
Firebase Auth
Firebase Auth

Use Firebase Auth with your Supabase project

Firebase Auth can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.

Getting started#
First you need to add an integration to connect your Supabase project with your Firebase project. You will need to get the Project ID in the Firebase Console.
Add a new Third-party Auth integration in your project's Authentication settings.
If you are using Third Party Auth when self hosting, create and attach restrictive RLS policies to all tables in your public schema, Storage and Realtime to prevent unauthorized access from unrelated Firebase projects.
Assign the role: 'authenticated' custom user claim to all your users.
Finally set up the Supabase client in your application.
Setup the Supabase client library#

TypeScript

Flutter

Swift (iOS)

Kotlin (Android)

Kotlin (Multiplatform)
Creating a client for the Web is as easy as passing the accessToken async function. This function should return the Firebase Auth JWT of the current user (or null if no such user) is found.

import { createClient } from '@supabase/supabase-js'
const supabase = createClient(
  'https://<supabase-project>.supabase.co',
  'SUPABASE_PUBLISHABLE_KEY',
  {
    accessToken: async () => {
      return (await firebase.auth().currentUser?.getIdToken(/* forceRefresh */ false)) ?? null
    },
  }
)
Make sure the all users in your application have the role: 'authenticated' custom claim set. If you're using the onCreate Cloud Function to add this custom claim to newly signed up users, you will need to call getIdToken(/* forceRefresh */ true) immediately after sign up as the onCreate function does not run synchronously.

Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project's Authentication settings and find the Third-Party Auth section to add a new integration.

In the CLI add the following config to your supabase/config.toml file:

[auth.third_party.firebase]
enabled = true
project_id = "<id>"
Adding an extra layer of security to your project's RLS policies (self-hosting only)#
Follow this section carefully to prevent unauthorized access to your project's data when self-hosting.

When using the Supabase hosted platform, following this step is optional.

Firebase Auth uses a single set of JWT signing keys for all projects. This means that JWTs issued from an unrelated Firebase project to yours could access data in your Supabase project.

When using the Supabase hosted platform, JWTs coming from Firebase project IDs you have not registered will be rejected before they reach your database. When self-hosting implementing this mechanism is your responsibility. An easy way to guard against this is to create and maintain the following RLS policies for all of your tables in the public schema. You should also attach this policy to Storage buckets or Realtime channels.

It's recommended you use a restrictive Postgres Row-Level Security policy.

Restrictive RLS policies differ from regular (or permissive) policies in that they use the as restrictive clause when being defined. They do not grant permissions, but rather restrict any existing or future permissions. They're great for cases like this where the technical limitations of Firebase Auth remain separate from your app's logic.

Postgres has two types of policies: permissive and restrictive. This example uses restrictive policies so make sure you don't omit the as restrictive clause.

This is an example of such an RLS policy that will restrict access to only your project's (denoted with <firebase-project-id>) users, and not any other Firebase project.

create policy "Restrict access to Supabase Auth and Firebase Auth for project ID <firebase-project-id>"
  on table_name
  as restrictive
  to authenticated
  using (
    (auth.jwt()->>'iss' = 'https://<project-ref>.supabase.co/auth/v1')
    or
    (
        auth.jwt()->>'iss' = 'https://securetoken.google.com/<firebase-project-id>'
        and
        auth.jwt()->>'aud' = '<firebase-project-id>'
     )
  );
If you have a lot of tables in your app, or need to manage complex RLS policies for Storage or Realtime it can be useful to define a stable Postgres function that performs the check to cut down on duplicate code. For example:

create function public.is_supabase_or_firebase_project_jwt()
  returns bool
  language sql
  stable
  returns null on null input
  return (
    (auth.jwt()->>'iss' = 'https://<project-ref>.supabase.co/auth/v1')
    or
    (
        auth.jwt()->>'iss' = concat('https://securetoken.google.com/<firebase-project-id>')
        and
        auth.jwt()->>'aud' = '<firebase-project-id>'
     )
  );
Make sure you substitute <project-ref> with your Supabase project's ID and the <firebase-project-id> to your Firebase Project ID. Then the restrictive policies on all your tables, buckets and channels can be simplified to be:

create policy "Restrict access to correct Supabase and Firebase projects"
  on table_name
  as restrictive
  to authenticated
  using ((select public.is_supabase_or_firebase_project_jwt()) is true);
Assign the "role" custom claim#
Your Supabase project inspects the role claim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Firebase JWTs do not contain a role claim in them. If you were to send such a JWT to your Supabase project, the anon role would be assigned when executing the Postgres query. Most of your app's logic will be accessible by the authenticated role.

Use Firebase Authentication functions to assign the authenticated role#
You have two choices to set up a Firebase Authentication function depending on your Firebase project's configuration:

Easiest: Use a blocking Firebase Authentication function but this is only available if your project uses Firebase Authentication with Identity Platform.
Manually assign the custom claims to all users with the admin SDK and define an onCreate Firebase Authentication Cloud Function to persist the role to all newly created users.

Node.js (Blocking Functions Gen 2)

Python (Blocking Functions Gen 2)

onCreate Cloud Function in Node.js
import { beforeUserCreated, beforeUserSignedIn } from 'firebase-functions/v2/identity'
export const beforecreated = beforeUserCreated((event) => {
  return {
    customClaims: {
      // The Supabase project will use this role to assign the `authenticated`
      // Postgres role.
      role: 'authenticated',
    },
  }
})
export const beforesignedin = beforeUserSignedIn((event) => {
  return {
    customClaims: {
      // The Supabase project will use this role to assign the `authenticated`
      // Postgres role.
      role: 'authenticated',
    },
  }
})
Note that instead of using customClaims you can instead use sessionClaims. The difference is that session_claims are not saved in the Firebase user profile, but remain valid for as long as the user is signed in.

Finally deploy your functions for the changes to take effect:

firebase deploy --only functions
Note that these functions are only called on new sign-ups and sign-ins. Existing users will not have these claims in their ID tokens. You will need to use the admin SDK to assign the role custom claim to all users. Make sure you do this after the blocking Firebase Authentication functions as described above are deployed.

Use the admin SDK to assign the role custom claim to all users#
You need to run a script that will assign the role: 'authenticated' custom claim to all of your existing Firebase Authentication users. You can do this by combining the list users and set custom user claims admin APIs. An example script is provided below:

'use strict';
const { initializeApp } = require('firebase-admin/app');
const { getAuth } = require('firebase-admin/auth');
initializeApp();
async function setRoleCustomClaim() => {
  let nextPageToken = undefined
  do {
    const listUsersResult = await getAuth().listUsers(1000, nextPageToken)
    nextPageToken = listUsersResult.pageToken
    await Promise.all(listUsersResult.users.map(async (userRecord) => {
      try {
        await getAuth().setCustomUserClaims(userRecord.id, {
          role: 'authenticated'
        })
      } catch (error) {
        console.error('Failed to set custom role for user', userRecord.id)
      }
    })
  } while (nextPageToken);
};
setRoleCustomClaim().then(() => process.exit(0))
After all users have received the role: 'authenticated' claim, it will appear in all newly issued ID tokens for the user.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Getting started
Setup the Supabase client library
Add a new Third-Party Auth integration to your project
Adding an extra layer of security to your project's RLS policies (self-hosting only)
Assign the "role" custom claim
Use Firebase Authentication functions to assign the authenticated role
Use the admin SDK to assign the role custom claim to all users
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Third-party auth
Auth0
Auth0

Use Auth0 with your Supabase project

Auth0 can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.

Getting started#
First you need to add an integration to connect your Supabase project with your Auth0 tenant. You will need your tenant ID (and in some cases region ID).
Add a new Third-party Auth integration in your project's Authentication settings.
Assign the role: 'authenticated' custom claim to all JWTs by using an Auth0 Action.
Finally setup the Supabase client in your application.
Setup the Supabase client library#

TypeScript

Swift (iOS)

Flutter

Kotlin
import { createClient } from '@supabase/supabase-js'
import Auth0Client from '@auth0/auth0-spa-js'
const auth0 = new Auth0Client({
  domain: '<AUTH0_DOMAIN>',
  clientId: '<AUTH0_CLIENT_ID>',
  authorizationParams: {
    redirect_uri: '<MY_CALLBACK_URL>',
  },
})
const supabase = createClient(
  'https://<supabase-project>.supabase.co',
  'SUPABASE_PUBLISHABLE_KEY',
  {
    accessToken: async () => {
      const accessToken = await auth0.getTokenSilently()
      // Alternatively you can use (await auth0.getIdTokenClaims()).__raw to
      // use an ID token instead.
      return accessToken
    },
  }
)
Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project's Authentication settings and find the Third-Party Auth section to add a new integration.

In the CLI add the following config to your supabase/config.toml file:

[auth.third_party.auth0]
enabled = true
tenant = "<id>"
tenant_region = "<region>" # if your tenant has a region
Use an Auth0 Action to assign the authenticated role#
Your Supabase project inspects the role claim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Auth0 JWTs (both access token and ID token) do not contain a role claim in them. If you were to send such a JWT to your Supabase project, the anon role would be assigned when executing the Postgres query. Most of your app's logic will be accessible by the authenticated role.

A recommended approach to do this is to configure the onExecutePostLogin Auth0 Action which will add the custom claim:

exports.onExecutePostLogin = async (event, api) => {
  api.accessToken.setCustomClaim('role', 'authenticated')
}
Limitations#
At this time, Auth0 tenants with the following signing algorithms are not supported:

HS256 (HMAC with SHA-256) -- also known as symmetric JWTs
PS256 (RSA-PSS with SHA-256)
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Getting started
Setup the Supabase client library
Add a new Third-Party Auth integration to your project
Use an Auth0 Action to assign the authenticated role
Limitations
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut
Auth
Third-party auth
AWS Cognito (Amplify)
Amazon Cognito (Amplify)

Use Amazon Cognito via Amplify or standalone with your Supabase project

Amazon Cognito User Pools (via AWS Amplify or on its own) can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.

Getting started#
First you need to add an integration to connect your Supabase project with your Amazon Cognito User Pool. You will need the pool's ID and region.
Add a new Third-party Auth integration in your project's Authentication settings or configure it in the CLI.
Assign the role: 'authenticated' custom claim to all JWTs by using a Pre-Token Generation Trigger.
Finally setup the Supabase client in your application.
Setup the Supabase client library#

TypeScript (Amplify)

Swift (iOS)

Flutter

Kotlin
import { fetchAuthSession, Hub } from 'aws-amplify/auth'
const supabase = createClient(
  'https://<supabase-project>.supabase.co',
  'SUPABASE_PUBLISHABLE_KEY',
  {
    accessToken: async () => {
      const tokens = await fetchAuthSession()
      // Alternatively you can use tokens?.idToken instead.
      return tokens?.accessToken
    },
  }
)
// if you're using Realtime you also need to set up a listener for Cognito auth changes
Hub.listen('auth', () => {
  fetchAuthSession().then((tokens) => supabase.realtime.setAuth(tokens?.accessToken))
})
Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project's Authentication settings and find the Third-Party Auth section to add a new integration.

In the CLI add the following config to your supabase/config.toml file:

[auth.third_party.aws_cognito]
enabled = true
user_pool_id = "<id>"
user_pool_region = "<region>"
Use a pre-token generation trigger to assign the authenticated role#
Your Supabase project inspects the role claim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

By default, Amazon Cognito JWTs (both ID token and access tokens) do not contain a role claim in them. If you were to send such a JWT to your Supabase project, the anon role would be assigned when executing the Postgres query. Most of your app's logic will be accessible by the authenticated role.

A recommended approach to do this is to configure a Pre-Token Generation Trigger either V1_0 (ID token only) or V2_0 (both access and ID token). To do this you will need to create a new Lambda function (in any language and runtime) and assign it to the Amazon Cognito User Pool's Lambda Triggers configuration. For example, the Lambda function should look similar to this:


Node.js
export const handler = async (event) => {
  event.response = {
    claimsOverrideDetails: {
      claimsToAddOrOverride: {
        role: 'authenticated',
      },
    },
  }
  return event
}
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Getting started
Setup the Supabase client library
Add a new Third-Party Auth integration to your project
Use a pre-token generation trigger to assign the authenticated role
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Third-party auth
WorkOS
WorkOS

Use WorkOS with your Supabase project

WorkOS can be used as a third-party authentication provider alongside Supabase Auth, or standalone, with your Supabase project.

Getting started#
First you need to add an integration to connect your Supabase project with your WorkOS tenant. You will need your WorkOS issuer. The issuer is https://api.workos.com/user_management/<your-client-id>. Substitute your custom auth domain for "api.workos.com" if configured.
Add a new Third-party Auth integration in your project's Authentication settings.
Set up a JWT template to assign the role: 'authenticated' claim to your access token.
Setup the Supabase client library#

TypeScript
import { createClient } from '@supabase/supabase-js'
import { createClient as createAuthKitClient } from '@workos-inc/authkit-js'
const authkit = await createAuthKitClient('WORKOS_CLIENT_ID', {
  apiHostname: '<WORKOS_AUTH_DOMAIN>',
})
const supabase = createClient(
  'https://<supabase-project>.supabase.co',
  'SUPABASE_PUBLISHABLE_KEY',
  {
    accessToken: async () => {
      return authkit.getAccessToken()
    },
  }
)
Add a new Third-Party Auth integration to your project#
In the dashboard navigate to your project's Authentication settings and find the Third-Party Auth section to add a new integration.

Set up a JWT template to add the authenticated role.#
Your Supabase project inspects the role claim present in all JWTs sent to it, to assign the correct Postgres role when using the Data API, Storage or Realtime authorization.

WorkOS JWTs already contain a role claim that corresponds to the user's role in their organization. It is necessary to adjust the role claim to be "authenticated" like Supabase expects. This can be done using JWT templates (navigate to Authentication -> Sessions -> JWT Template in the WorkOS Dashboard).

This template overrides the role claim to meet Supabase's expectations, and adds the WorkOS role in a new user_role claim:

{
  "role": "authenticated",
  "user_role": {{organization_membership.role}}
}
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Getting started
Setup the Supabase client library
Add a new Third-Party Auth integration to your project
Set up a JWT template to add the authenticated role.
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Configuration
General Configuration
General configuration

General configuration options for Supabase Auth

This section covers the general configuration options for Supabase Auth. If you are looking for another type of configuration, you may be interested in one of the following sections:

Policies to manage Row Level Security policies for your tables.
Sign In / Providers to configure authentication providers and login methods for your users.
Third Party Auth to use third-party authentication (TPA) systems based on JWTs to access your project.
Sessions to configure settings for user sessions and refresh tokens.
Rate limits to safeguard against bursts of incoming traffic to prevent abuse and maximize stability.
Email Templates to configure what emails your users receive.
Custom SMTP to configure how emails are sent.
Multi-Factor to require users to provide additional verification factors to authenticate.
URL Configuration to configure site URL and redirect URLs for authentication. Read more in the redirect URLs documentation.
Attack Protection to configure security settings to protect your project from attacks.
Auth Hooks (BETA) to use Postgres functions or HTTP endpoints to customize the behavior of Supabase Auth to meet your needs.
Audit Logs (BETA) to track and monitor auth events in your project.
Advanced to configure advanced authentication server settings.
Supabase Auth provides these general configuration options to control user access to your application:

Allow new users to sign up: Users will be able to sign up. If this config is disabled, only existing users can sign in.

Confirm Email: Users will need to confirm their email address before signing in for the first time.

Having Confirm Email disabled assumes that the user's email does not need to be verified in order to login and implicitly confirms the user's email in the database.
This option can be found in the email provider under the provider-specific configuration.
Allow anonymous sign-ins: Allow anonymous users to be created.

Allow manual linking: Allow users to link their accounts manually.

Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Configuration
Email Templates
Email Templates

Learn how to manage the email templates in Supabase.

Email templates in Supabase fall into two categories: authentication and security notifications.

Authentication emails:

Confirm sign up
Invite user
Magic link
Change email address
Reset password
Reauthentication
Security notification emails:

Password changed
Email address changed
Phone number changed
Identity linked
Identity unlinked
Multi-factor authentication (MFA) method added
Multi-factor authentication (MFA) method removed
Security emails are only sent to users if the respective security notifications have been enabled at a project-level.

Terminology#
The templating system provides the following variables for use:

Name	Description
{{ .ConfirmationURL }}	Contains the confirmation URL. For example, a signup confirmation URL would look like: https://project-ref.supabase.co/auth/v1/verify?token={{ .TokenHash }}&type=email&redirect_to=https://example.com/path.
{{ .Token }}	Contains a 6-digit One-Time-Password (OTP) that can be used instead of the {{. ConfirmationURL }}.
{{ .TokenHash }}	Contains a hashed version of the {{ .Token }}. This is useful for constructing your own email link in the email template.
{{ .SiteURL }}	Contains your application's Site URL. This can be configured in your project's authentication settings.
{{ .RedirectTo }}	Contains the redirect URL passed when signUp, signInWithOtp, signInWithOAuth, resetPasswordForEmail or inviteUserByEmail is called. The redirect URL allow list can be configured in your project's authentication settings.
{{ .Data }}	Contains metadata from auth.users.user_metadata. Use this to personalize the email message.
{{ .Email }}	Contains the original email address of the user. Empty when when trying to link an email address to an anonymous user.
{{ .NewEmail }}	Contains the new email address of the user. This variable is only supported in the "Change email address" template.
{{ .OldEmail }}	Contains the old email address of the user. This variable is only supported in the "Email address changed notification" template.
{{ .Phone }}	Contains the new phone number of the user. This variable is only supported in the "Phone number changed notification" template.
{{ .OldPhone }}	Contains the old phone address of the user. This variable is only supported in the "Phone number changed notification" template.
{{ .Provider }}	Contains the provider of the newly linked/unlinked identity. This variable is only supported in the "Identity linked notification" and "Identity unlinked notification" templates.
{{ .FactorType }}	Contains the type of the newly enrolled/unenrolled MFA method. This variable is only supported in the "MFA method added notification" and "MFA method removed notification" templates.
Editing email templates#
On hosted Supabase projects, edit your email templates on the Email Templates page. On self-hosted projects or in local development, edit your configuration files.

You can also manage email templates using the Management API:

# Get your access token from https://supabase.com/dashboard/account/tokens
export SUPABASE_ACCESS_TOKEN="your-access-token"
export PROJECT_REF="your-project-ref"
# Get current email templates
curl -X GET "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  | jq 'to_entries | map(select(.key | startswith("mailer_templates"))) | from_entries'
# Update email templates
curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
      "mailer_subjects_confirmation": "Confirm your signup",
      "mailer_templates_confirmation_content": "<h2>Confirm your signup</h2><p>Follow this link to confirm your user:</p><p><a href=\"{{ .ConfirmationURL }}\">Confirm your email</a></p>",
      "mailer_subjects_magic_link": "Your Magic Link",
      "mailer_templates_magic_link_content": "<h2>Magic Link</h2><p>Follow this link to login:</p><p><a href=\"{{ .ConfirmationURL }}\">Log In</a></p>",
      "mailer_subjects_recovery": "Rest Your Password",
      "mailer_templates_recovery_content": "<h2>Reset Password</h2><p>Follow this link to reset the password for your user:</p><p><a href=\"{{ .ConfirmationURL }}\">Reset Password</a></p>",
      "mailer_subjects_invite": "You have been invited",
      "mailer_templates_invite_content": "<h2>You have been invited</h2><p>You have been invited to create a user on {{ .SiteURL }}. Follow this link to accept the invite:</p><p><a href=\"{{ .ConfirmationURL }}\">Accept the invite</a></p>",
      "mailer_subjects_reauthentication": "Confirm reauthentication",
      "mailer_templates_reauthentication_content": "<h2>Confirm reauthentication</h2><p>Enter the code: {{token}}</p>",
      "mailer_subjects_email_change": "Confirm email change",
      "mailer_templates_email_change_content": "<h2>Confirm email change</h2><p>Follow this link to confirm the update of your email:</p><p><a href=\"{{ .ConfirmationURL }}\">Change email</a></p>",
      "mailer_notifications_password_changed_enabled": true,
      "mailer_subjects_password_changed_notification": "Your password has been changed",
      "mailer_templates_password_changed_notification_content": "<h2>Your password has been changed</h2>\n\n<p>This is a confirmation that the password for your account {{ .Email }} has just been changed.</p>\n<p>If you did not make this change, please contact support.</p>",
      "mailer_notifications_email_changed_enabled": true,
      "mailer_subjects_email_changed_notification": "Your email address has been changed",
      "mailer_templates_email_changed_notification_content": "<h2>Your email address has been changed</h2>\n\n<p>The email address for your account has been changed from {{ .OldEmail }} to {{ .Email }}.</p>\n<p>If you did not make this change, please contact support.</p>",
      "mailer_notifications_phone_changed_enabled": true,
      "mailer_subjects_phone_changed_notification": "Your phone number has been changed",
      "mailer_templates_phone_changed_notification_content": "<h2>Your phone number has been changed</h2>\n\n<p>The phone number for your account {{ .Email }} has been changed from {{ .OldPhone }} to {{ .Phone }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",
      "mailer_notifications_mfa_factor_enrolled_enabled": true,
      "mailer_subjects_mfa_factor_enrolled_notification": "A new MFA factor has been enrolled",
      "mailer_templates_mfa_factor_enrolled_notification_content": "<h2>A new MFA factor has been enrolled</h2>\n\n<p>A new factor ({{ .FactorType }}) has been enrolled for your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",
      "mailer_notifications_mfa_factor_unenrolled_enabled": true,
      "mailer_subjects_mfa_factor_unenrolled_notification": "An MFA factor has been unenrolled",
      "mailer_templates_mfa_factor_unenrolled_notification_content": "<h2>An MFA factor has been unenrolled</h2>\n\n<p>A factor ({{ .FactorType }}) has been unenrolled for your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",
      "mailer_notifications_identity_linked_enabled": true,
      "mailer_subjects_identity_linked_notification": "A new identity has been linked",
      "mailer_templates_identity_linked_notification_content": "<h2>A new identity has been linked</h2>\n\n<p>A new identity ({{ .Provider }}) has been linked to your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>",
      "mailer_notifications_identity_unlinked_enabled": true,
      "mailer_subjects_identity_unlinked_notification": "An identity has been unlinked",
      "mailer_templates_identity_unlinked_notification_content": "<h2>An identity has been unlinked</h2>\n\n<p>An identity ({{ .Provider }}) has been unlinked from your account {{ .Email }}.</p>\n<p>If you did not make this change, please contact support immediately.</p>"
  }'
Mobile deep linking#
For mobile applications, you might need to link or redirect to a specific page within your app. See the Mobile Deep Linking guide to set this up.

Limitations#
Email prefetching#
Certain email providers may have spam detection or other security features that prefetch URL links from incoming emails (e.g. Safe Links in Microsoft Defender for Office 365).
In this scenario, the {{ .ConfirmationURL }} sent will be consumed instantly which leads to a "Token has expired or is invalid" error.
To guard against this there are the options below:

Option 1

Use an email OTP instead by including {{ .Token }} in the email template
Create your own custom email link to redirect the user to a page where they can enter with their email and token to login
<a href="{{ .SiteURL }}/confirm-signup">Confirm your signup</a>
Log them in by verifying the OTP token value with their email e.g. with supabase.auth.verifyOtp show below
const { data, error } = await supabase.auth.verifyOtp({ email, token, type: 'email' })
Option 2

Create your own custom email link to redirect the user to a page where they can click on a button to confirm the action
<a href="{{ .SiteURL }}/confirm-signup?confirmation_url={{ .ConfirmationURL }}"
  >Confirm your signup</a
>
The button should contain the actual confirmation link which can be obtained from parsing the confirmation_url={{ .ConfirmationURL }} query parameter in the URL.
Email tracking#
If you are using an external email provider that enables "email tracking", the links inside the Supabase email templates will be overwritten and won't perform as expected. We recommend disabling email tracking to ensure email links are not overwritten.

Redirecting the user to a server-side endpoint#
If you intend to use Server-side rendering, you might want the email link to redirect the user to a server-side endpoint to check if they are authenticated before returning the page. However, the default email link will redirect the user after verification to the redirect URL with the session in the query fragments. Since the session is returned in the query fragments by default, you won't be able to access it on the server-side.

You can customize the email link in the email template to redirect the user to a server-side endpoint successfully. For example:

<a
  href="https://api.example.com/v1/authenticate?token_hash={{ .TokenHash }}&type=invite&redirect_to={{ .RedirectTo }}"
  >Accept the invite
</a>
When the user clicks on the link, the request will hit https://api.example.com/v1/authenticate and you can grab the token_hash, type and redirect_to query parameters from the URL. Then, you can call the verifyOtp method to get back an authenticated session before redirecting the user back to the client. Since the verifyOtp method makes a POST request to Supabase Auth to verify the user, the session will be returned in the response body, which can be read by the server. For example:

const { token_hash, type } = Object.fromEntries(new URLSearchParams(window.location.search))
const {
  data: { session },
  error,
} = await supabase.auth.verifyOtp({ token_hash, type: type as EmailOtpType })
// subsequently redirect the user back to the client using the redirect_to param
// ...
Customization#
Supabase Auth makes use of Go Templates. This means it is possible to conditionally render information based on template properties.

Send different email to early access users#
Send a different email to users who signed up via an early access domain (https://www.earlyaccess.trial.com).

{{ if eq .Data.Domain "https://www.example.com" }}
<h1>Welcome to Our Database Service!</h1>
  <p>Dear Developer,</p>
  <p>Welcome to Billy, the scalable developer platform!</p>
  <p>Best Regards,<br>
Billy Team</p>
{{ else if eq .Data.Domain "https://www.earlyaccess.trial.com" }}
<h1>Welcome to Our Database Service!</h1>
  <p>Dear Developer,</p>
  <p>Welcome Billy, the scalable developer platform!</p>
  <p> As an early access member, you have access to select features like Point To Space Restoration.</p>
  <p>Best Regards,<br>
Billy Team</p>
{{ end }}
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Terminology
Editing email templates
Mobile deep linking
Limitations
Email prefetching
Email tracking
Redirecting the user to a server-side endpoint
Customization
Send different email to early access users
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Configuration
Redirect URLs
Redirect URLs

Set up redirect urls with Supabase Auth.

Overview#
Supabase Auth allows you to control how the user sessions are handled by your application.

When using passwordless sign-ins or third-party providers, the Supabase client library provides a redirectTo parameter to specify where to redirect the user after authentication. The URL in redirectTo should match the Redirect URLs list configuration.

To configure allowed redirect URLs, go to the URL Configuration page. Once you've added necessary URLs, you can use the URL you want the user to be redirected to in the redirectTo parameter.

The Site URL in URL Configuration defines the default redirect URL when no redirectTo is specified in the code. Change this from http://localhost:3000 to your production URL (e.g., https://example.com). This setting is critical for email confirmations and password resets.

When using Sign in with Web3, the message signed by the user in the Web3 wallet application will indicate the URL on which the signature took place. Supabase Auth will reject messages that are signed for URLs that are not on the allowed list.

In local development or self-hosted projects, use the configuration file. See below for more information on configuring SITE_URL when deploying to Vercel or Netlify.

Use wildcards in redirect URLs#
Supabase allows you to specify wildcards when adding redirect URLs to the allow list. You can use wildcard match patterns to support preview URLs from providers like Netlify and Vercel.

Wildcard	Description
*	matches any sequence of non-separator characters
**	matches any sequence of characters
?	matches any single non-separator character
c	matches character c (c != *, **, ?, \, [, {, })
\c	matches character c
[!{ character-range }]	matches any sequence of characters not in the { character-range }. For example, [!a-z] will not match any characters ranging from a-z.
The separator characters in a URL are defined as . and /. Use this tool to test your patterns.

Recommendation
While the "globstar" (**) is useful for local development and preview URLs, we recommend setting the exact redirect URL path for your site URL in production.

Redirect URL examples with wildcards#
Redirect URL	Description
http://localhost:3000/*	matches http://localhost:3000/foo, http://localhost:3000/bar but not http://localhost:3000/foo/bar or http://localhost:3000/foo/ (note the trailing slash)
http://localhost:3000/**	matches http://localhost:3000/foo, http://localhost:3000/bar and http://localhost:3000/foo/bar
http://localhost:3000/?	matches http://localhost:3000/a but not http://localhost:3000/foo
http://localhost:3000/[!a-z]	matches http://localhost:3000/1 but not http://localhost:3000/a
Netlify preview URLs#
For deployments with Netlify, set the SITE_URL to your official site URL. Add the following additional redirect URLs for local development and deployment previews:

http://localhost:3000/**
https://**--my_org.netlify.app/**
Vercel preview URLs#
For deployments with Vercel, set the SITE_URL to your official site URL. Add the following additional redirect URLs for local development and deployment previews:

http://localhost:3000/**
https://*-<team-or-account-slug>.vercel.app/**
Vercel provides an environment variable for the URL of the deployment called NEXT_PUBLIC_VERCEL_URL. See the Vercel docs for more details. You can use this variable to dynamically redirect depending on the environment. You should also set the value of the environment variable called NEXT_PUBLIC_SITE_URL, this should be set to your site URL in production environment to ensure that redirects function correctly.

const getURL = () => {
  let url =
    process?.env?.NEXT_PUBLIC_SITE_URL ?? // Set this to your site URL in production env.
    process?.env?.NEXT_PUBLIC_VERCEL_URL ?? // Automatically set by Vercel.
    'http://localhost:3000/'
  // Make sure to include `https://` when not localhost.
  url = url.startsWith('http') ? url : `https://${url}`
  // Make sure to include a trailing `/`.
  url = url.endsWith('/') ? url : `${url}/`
  return url
}
const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'github',
  options: {
    redirectTo: getURL(),
  },
})
Email templates when using redirectTo#
When using a redirectTo option, you may need to replace the {{ .SiteURL }} with {{ .RedirectTo }} in your email templates. See the Email Templates guide for more information.

For example, change the following:

<!-- Old -->
<a href="{{ .SiteURL }}/auth/confirm?token_hash={{ .TokenHash }}&type=email">Confirm your mail</a>
<!-- New -->
<a href="{{ .RedirectTo }}/auth/confirm?token_hash={{ .TokenHash }}&type=email"
  >Confirm your mail</a
>
Mobile deep linking URIs#
For mobile applications you can use deep linking URIs. For example, for your SITE_URL you can specify something like com.supabase://login-callback/ and for additional redirect URLs something like com.supabase.staging://login-callback/ if needed.

Read more about deep linking and find code examples for different frameworks here.

Error handling#
When authentication fails, the user will still be redirected to the redirect URL provided. However, the error details will be returned as query fragments in the URL. You can parse these query fragments and show a custom error message to the user. For example:

const params = new URLSearchParams(window.location.hash.slice())
if (params.get('error_code').startsWith('4')) {
  // show error message if error is a 4xx error
  window.alert(params.get('error_description'))
}
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Use wildcards in redirect URLs
Redirect URL examples with wildcards
Netlify preview URLs
Vercel preview URLs
Email templates when using redirectTo
Mobile deep linking URIs
Error handling
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Overview
Auth Hooks

Use HTTP or Postgres Functions to customize your authentication flow

What is a hook#
A hook is an endpoint that allows you to alter the default Supabase Auth flow at specific execution points. Developers can use hooks to add custom behavior that's not supported natively.

Hooks help you:

Track the origin of user signups by adding metadata
Improve security by adding additional checks to password and multi-factor authentication
Support legacy systems by integrating with identity credentials from external authentication systems
Add additional custom claims to your JWT
Send authentication emails or SMS messages through a custom provider
The following hooks are available:

Hook	Available on Plan
Before User Created	Free, Pro
Custom Access Token	Free, Pro
Send SMS	Free, Pro
Send Email	Free, Pro
MFA Verification Attempt	Teams and Enterprise
Password Verification Attempt	Teams and Enterprise
Supabase supports 2 ways to configure a hook in your project:


Postgres Function

HTTP Endpoint
A Postgres function can be configured as a hook. The function should take in a single argument -- the event of type JSONB -- and return a JSONB object. Since the Postgres function runs on your database, the request does not leave your project's instance.

Security model#
Sign the payload and grant permissions selectively in order to guard the integrity of the payload.


SQL

HTTP
When you configure a Postgres function as a hook, Supabase will automatically apply the following grants to the function for these reasons:

Allow the supabase_auth_admin role to execute the function. The supabase_auth_admin role is the Postgres role that is used by Supabase Auth to make requests to your database.
Revoke permissions from other roles (e.g. anon, authenticated, public) to ensure the function is not accessible by Supabase Data APIs.
-- Grant access to function to supabase_auth_admin
grant execute
  on function public.custom_access_token_hook
  to supabase_auth_admin;
-- Grant access to schema to supabase_auth_admin
grant usage on schema public to supabase_auth_admin;
-- Revoke function permissions from authenticated, anon and public
revoke execute
  on function public.custom_access_token_hook
  from authenticated, anon, public;
You will need to alter your row-level security (RLS) policies to allow the supabase_auth_admin role to access tables that you have RLS policies on. You can read more about RLS policies here.

Alternatively, you can create your Postgres function via the dashboard with the security definer tag. The security definer tag specifies that the function is to be executed with the privileges of the user that owns it.

Currently, functions created via the dashboard take on the postgres role. Read more about the security definer tag in our database guide

Using Hooks#
Developing#
Let us develop a Hook locally and then deploy it to the cloud. As a recap, here’s a list of available Hooks

Hook	Suggested Function Name	When it is called	What it Does
Send SMS	send_sms	Each time an SMS is sent	Allows you to customize message content and SMS Provider
Send Email	send_email	Each time an Email is sent	Allows you to customize message content and Email Provider
Custom Access Token	custom_access_token	Each time a new JWT is created	Returns the claims you wish to be present in the JWT.
MFA Verification Attempt	mfa_verification_attempt	Each time a user tries to verify an MFA factor.	Returns a decision on whether to reject the attempt and future ones, or to allow the user to keep trying.
Password Verification Attempt	password_verification_attempt	Each time a user tries to sign in with a password.	Return a decision whether to allow the user to reject the attempt, or to allow the user to keep trying.
Edit config.toml to set up the Auth Hook locally.


SQL

HTTP
Modify the auth.hook.<hook_name> field and set uri to a value of pg-functions://postgres/<schema>/<function_name>

[auth.hook.<hook_name>]
enabled = true
uri = "pg-functions://...."
You need to assign additional permissions so that Supabase Auth can access the hook as well as the tables it interacts with.

The supabase_auth_admin role does not have permissions to the public schema. You need to grant the role permission to execute your hook:

grant execute
  on function public.custom_access_token_hook
  to supabase_auth_admin;
You also need to grant usage to supabase_auth_admin:

grant usage on schema public to supabase_auth_admin;
Also revoke permissions from the authenticated and anon roles to ensure the function is not accessible by Supabase Serverless APIs.

revoke execute
  on function public.custom_access_token_hook
  from authenticated, anon;
For security, we recommend against the use the security definer tag. The security definer tag specifies that the function is to be executed with the privileges of the user that owns it. When a function is created via the Supabase dashboard with the tag, it will have the extensive permissions of the postgres role which make it easier for undesirable actions to occur.

We recommend that you do not use any tag and explicitly grant permissions to supabase_auth_admin as described above.

Read more about security definer tag in our database guide.

Once done, save your Auth Hook as a migration in order to version the Auth Hook and share it with other team members. Run supabase migration new to create a migration.

If you're using the Supabase SQL Editor, there's an issue when using the ? (Does the string exist as a top-level key within the JSON value?) operator. Use a direct connection to the database if you need to use it when defining a function.

Here is an example hook signature:

create or replace function public.custom_access_token_hook(event jsonb)
returns jsonb
language plpgsql
as $$
declare
  -- Insert variables here
begin
  -- Insert logic here
  return event;
end;
$$;
You can visit SQL Editor > Templates for hook templates.

Deploying#
In the dashboard, navigate to Authentication > Hooks and select the appropriate function type (SQL or HTTP) from the dropdown menu.

Error handling#
You should return an error when facing a runtime error. Runtime errors are specific to your application and arise from specific business rules rather than programmer errors.

Runtime errors could happen when:

The user does not have appropriate permissions
The event payload received does not have required claims.
The user has performed an action which violates a business rule.
The email or phone provider used in the webhook returned an error.

SQL

HTTP
The error is a JSON object and has the following properties:

error An object that contains information about the error.
http_code A number indicating the HTTP code to be returned. If not set, the code is HTTP 500 Internal Server Error.
message A message to be returned in the HTTP response. Required.
Here's an example:

{
  "error": {
    "http_code": 429,
    "message": "You can only verify a factor once every 10 seconds."
  }
}
Errors returned from a Postgres Hook are not retry-able. When an error is returned, the error is propagated from the hook to Supabase Auth and translated into a HTTP error which is returned to your application. Supabase Auth will only take into account the error and disregard the rest of the payload.

Outside of runtime errors, both HTTP Hooks and Postgres Hooks return timeout errors. Postgres Hooks have 2 seconds to complete processing while HTTP Hooks should complete in 5 seconds. Both HTTP Hooks and Postgres Hooks are run in a transaction do limit the duration of execution to avoid delays in authentication process.

Available Hooks#
Each Hook description contains an example JSON Schema which you can use in conjunction with JSON Schema Faker in order to generate a mock payload. For HTTP Hooks, you can also use the Standard Webhooks Testing Tool to simulate a request.

Custom Access Token

Customize the access token issued by Supabase Auth
Send SMS

Use a custom SMS provider to send authentication messages
Send Email

Use a custom email provider to send authentication messages
MFA Verification

Add additional checks to the MFA verification flow
Password verification

Add additional checks to the password verification flow
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
What is a hook
Security model
Using Hooks
Developing
Deploying
Error handling
Available Hooks
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Custom access token hook
Custom Access Token Hook

Customize the access token issued by Supabase Auth

The custom access token hook runs before a token is issued and allows you to add additional claims based on the authentication method used.

Claims returned must conform to our specification. Supabase Auth will check for these claims after the hook is run and return an error if they are not present.

These are the fields currently available on an access token:

Required Claims: iss, aud, exp, iat, sub, role, aal, session_id, email, phone, is_anonymous

Optional Claims: jti, nbf, app_metadata, user_metadata, amr,

Inputs

Field	Type	Description
user_id	string	Unique identifier for the user attempting to sign in.
claims	object	Claims which are included in the access token.
authentication_method	string	The authentication method used to request the access token. Possible values include: oauth, password, otp, totp, recovery, invite, sso/saml, magiclink, email/signup, email_change, token_refresh, anonymous.

JSON

JSON Schema
{
  "user_id": "8ccaa7af-909f-44e7-84cb-67cdccb56be6",
  "claims": {
    "aud": "authenticated",
    "exp": 1715690221,
    "iat": 1715686621,
    "sub": "8ccaa7af-909f-44e7-84cb-67cdccb56be6",
    "email": "",
    "phone": "",
    "app_metadata": {},
    "user_metadata": {},
    "role": "authenticated",
    "aal": "aal1",
    "amr": [ { "method": "anonymous", "timestamp": 1715686621 } ],
    "session_id": "4b938a09-5372-4177-a314-cfa292099ea2",
    "is_anonymous": true
  },
  "authentication_method": "anonymous"
}
Outputs

Return these only if your hook processed the input without errors.

Field	Type	Description
claims	object	The updated claims after the hook has been run.

SQL

HTTP

Minimal JWT

Add admin role

Restrict access to SSO users
Sometimes the size of the JWT can be a problem especially if you're using a Server-Side Rendering framework. Common situations where the JWT can get too large include:

The user has a particularly large name, email address or phone number
The default JWT has too many claims coming from OAuth providers
A large avatar URL is included
To lower the size of the JWT you can define a Custom Access Token hook like the one below which will instruct the Auth server to issue a JWT with only the listed claims. Check the documentation above on what JWT claims must be present and cannot be removed.

Refer to the Postgres JSON functions on how to manipulate jsonb objects.

create or replace function public.custom_access_token_hook(event jsonb)
returns jsonb
language plpgsql
as $$
  declare
    original_claims jsonb;
    new_claims jsonb;
    claim text;
  begin
    original_claims = event->'claims';
    new_claims = '{}'::jsonb;
    foreach claim in array array[
      -- add claims you want to keep here
      'iss',
      'aud',
      'exp',
      'iat',
      'sub',
      'role',
      'aal',
      'session_id',
      'email',
      'phone',
      'is_anonymous'
   ] loop
      if original_claims ? claim then
        -- original_claims contains one of the listed claims, set it on new_claims
        new_claims = jsonb_set(new_claims, array[claim], original_claims->claim);
      end if;
    end loop;
    return jsonb_build_object('claims', new_claims);
  end
$$;
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Send SMS hook
Send SMS Hook

Use a custom SMS provider to send authentication messages

Runs before a message is sent. Use the hook to:

Use a regional SMS Provider
Use alternate messaging channels such as WhatsApp
Adjust the message body to include platform specific fields such as the AppHash
Inputs

Field	Type	Description
user	User	The user attempting to sign in.
sms	object	Metadata specific to the SMS sending process. Includes the OTP.

JSON

JSON Schema
{
  "user": {
    "id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",
    "aud": "authenticated",
    "role": "authenticated",
    "email": "",
    "phone": "+1333363128",
    "phone_confirmed_at": "2024-05-13T11:52:48.157306Z",
    "confirmation_sent_at": "2024-05-14T12:31:52.824573Z",
    "confirmed_at": "2024-05-13T11:52:48.157306Z",
    "phone_change_sent_at": "2024-05-13T11:47:02.183064Z",
    "last_sign_in_at": "2024-05-13T11:52:48.162518Z",
    "app_metadata": {
      "provider": "phone",
      "providers": ["phone"]
    },
    "user_metadata": {},
    "identities": [
      {
        "identity_id": "3be5e552-65aa-41d9-9db9-2a502f845459",
        "id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",
        "user_id": "6481a5c1-3d37-4a56-9f6a-bee08c554965",
        "identity_data": {
          "email_verified": false,
          "phone": "+1612341244428",
          "phone_verified": true,
          "sub": "6481a5c1-3d37-4a56-9f6a-bee08c554965"
        },
        "provider": "phone",
        "last_sign_in_at": "2024-05-13T11:52:48.155562Z",
        "created_at": "2024-05-13T11:52:48.155599Z",
        "updated_at": "2024-05-13T11:52:48.159391Z"
      }
    ],
    "created_at": "2024-05-13T11:45:33.7738Z",
    "updated_at": "2024-05-14T12:31:52.82475Z",
    "is_anonymous": false
  },
  "sms": {
    "otp": "561166"
  }
}
Outputs

No outputs are required. An empty response with a status code of 200 is taken as a successful response.

SQL

HTTP

Queue SMS Messages
Your company uses a worker to manage all messaging related jobs. For performance reasons, the messaging system sends messages in intervals via a job queue. Instead of sending a message immediately, messages are queued and sent in periodic intervals via pg_cron.

Create a table to store jobs

create table job_queue (
  job_id uuid primary key default gen_random_uuid(),
  job_data jsonb not null,
  created_at timestamp default now(),
  status text default 'pending',
  priority int default 0,
  retry_count int default 0,
  max_retries int default 2,
  scheduled_at timestamp default now()
);
Create the hook:

create or replace function send_sms(event jsonb) returns void as $$
declare
    job_data jsonb;
    scheduled_time timestamp;
    priority int;
begin
    -- extract phone and otp from the event json
    job_data := jsonb_build_object(
        'phone', event->'user'->>'phone',
        'otp', event->'sms'->>'otp'
    );
    -- calculate the nearest 5-minute window for scheduled_time
    scheduled_time := date_trunc('minute', now()) + interval '5 minute' * floor(extract('epoch' from (now() - date_trunc('minute', now())) / 60) / 5);
    -- assign priority dynamically (example logic: higher priority for earlier scheduled time)
    priority := extract('epoch' from (scheduled_time - now()))::int;
    -- insert the job into the job_queue table
    insert into job_queue (job_data, priority, scheduled_at, max_retries)
    values (job_data, priority, scheduled_time, 2);
end;
$$ language plpgsql;
grant all
  on table public.job_queue
  to supabase_auth_admin;
revoke all
  on table public.job_queue
  from authenticated, anon;
Create a function to periodically run and dequeue all jobs

create or replace function dequeue_and_run_jobs() returns void as $$
declare
    job record;
begin
    for job in
        select * from job_queue
        where status = 'pending'
          and scheduled_at <= now()
        order by priority desc, created_at
        for update skip locked
    loop
        begin
            -- add job processing logic here.
            -- for demonstration, we'll just update the job status to 'completed'.
            update job_queue
            set status = 'completed'
            where job_id = job.job_id;
        exception when others then
            -- handle job failure and retry logic
            if job.retry_count < job.max_retries then
                update job_queue
                set retry_count = retry_count + 1,
                    scheduled_at = now() + interval '1 minute'  -- delay retry by 1 minute
                where job_id = job.job_id;
            else
                update job_queue
                set status = 'failed'
                where job_id = job.job_id;
            end if;
        end;
    end loop;
end;
$$ language plpgsql;
grant execute
  on function public.dequeue_and_run_jobs
  to supabase_auth_admin;
revoke execute
  on function public.dequeue_and_run_jobs
  from authenticated, anon;
Configure pg_cron to run the job on an interval. You can use a tool like crontab.guru to check that your job is running on an appropriate schedule. Ensure that pg_cron is enabled under Database > Extensions

select
  cron.schedule(
    '* * * * *', -- this cron expression means every minute.
    'select dequeue_and_run_jobs();'
  );
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Send email hook
Send Email Hook

Use a custom email provider to send authentication messages.

The Send Email Hook runs before an email is sent and allows for flexibility around email sending. You can use this hook to configure a backup email provider or to add internationalization to your emails.

Inputs

Field	Type	Description
user	User	The user account taking the action
email	object	Metadata specific to the email sending process

JSON

JSON Schema
{
  "user": {
    "id": "8484b834-f29e-4af2-bf42-80644d154f76",
    "aud": "authenticated",
    "role": "authenticated",
    "email": "valid.email@supabase.io",
    "phone": "",
    "app_metadata": {
      "provider": "email",
      "providers": ["email"]
    },
    "user_metadata": {
      "email": "valid.email@supabase.io",
      "email_verified": false,
      "phone_verified": false,
      "sub": "8484b834-f29e-4af2-bf42-80644d154f76"
    },
    "identities": [
      {
        "identity_id": "bc26d70b-517d-4826-bce4-413a5ff257e7",
        "id": "8484b834-f29e-4af2-bf42-80644d154f76",
        "user_id": "8484b834-f29e-4af2-bf42-80644d154f76",
        "identity_data": {
          "email": "valid.email@supabase.io",
          "email_verified": false,
          "phone_verified": false,
          "sub": "8484b834-f29e-4af2-bf42-80644d154f76"
        },
        "provider": "email",
        "last_sign_in_at": "2024-05-14T12:56:33.824231484Z",
        "created_at": "2024-05-14T12:56:33.824261Z",
        "updated_at": "2024-05-14T12:56:33.824261Z",
        "email": "valid.email@supabase.io"
      }
    ],
    "created_at": "2024-05-14T12:56:33.821567Z",
    "updated_at": "2024-05-14T12:56:33.825595Z",
    "is_anonymous": false
  },
  "email_data": {
    "token": "305805",
    "token_hash": "7d5b7b1964cf5d388340a7f04f1dbb5eeb6c7b52ef8270e1737a58d0",
    "redirect_to": "http://localhost:3000/",
    "email_action_type": "signup",
    "site_url": "http://localhost:9999",
    "token_new": "",
    "token_hash_new": "",
    "old_email": "",
    "old_phone": "",
    "provider": "",
    "factor_type": ""
  }
}
Outputs

No outputs are required. An empty response with a status code of 200 is taken as a successful response.
Email sending behavior#
Email sending depends on two settings: Email Provider and Auth Hook status.

Email Provider	Auth Hook	Result
Enabled	Enabled	Auth Hook handles email sending (SMTP not used)
Enabled	Disabled	SMTP handles email sending (custom if configured, default otherwise)
Disabled	Enabled	Email signups disabled
Disabled	Disabled	Email signups disabled
Email change behavior and token hash mapping#
When email_action_type is email_change, the hook payload can include one or two OTPs and their hashes. This depends on your Secure Email Change setting.

Secure Email Change enabled: two OTPs are generated, one for the current email (user.email) and one for the new email (user.email_new). You must send two emails.
Secure Email Change disabled: only one OTP is generated for the new email. You send a single email.
Counterintuitive field naming
The token hash field names are reversed due to backward compatibility. Pay careful attention to which token/hash pair goes with which email address:

token_hash_new → use with the current email address (user.email) and token
token_hash → use with the new email address (user.email_new) and token_new
Do not assume the _new suffix refers to the new email address.

What to send#
When Secure Email Change is enabled (both token/hash pairs present):

Send to current email address (user.email): use token with token_hash_new
Send to new email address (user.email_new): use token_new with token_hash
When Secure Email Change is disabled (only one token/hash pair present):

Send a single email to the new email address. Use token with token_hash or token_new with token_hash, depending on which fields are present in the payload.

SQL

HTTP

Use Resend as an email provider

Add Internationalization for Email Templates
You can configure Resend as the custom email provider through the "Send Email" hook. This allows you to take advantage of Resend's developer-friendly APIs to send emails and leverage React Email for managing your email templates. For a more advanced React Email tutorial, refer to this guide.

If you want to send emails through the Supabase Resend integration, which uses Resend's SMTP server, check out this integration instead.

Create a .env file with the following environment variables:

RESEND_API_KEY="your_resend_api_key"
SEND_EMAIL_HOOK_SECRET="v1,whsec_<base64_secret>"
You can generate the secret in the Auth Hooks section of the Supabase dashboard.

Set the secrets in your Supabase project:

supabase secrets set --env-file .env
Create a new edge function:

supabase functions new send-email
Add the following code to your edge function:

import { Webhook } from "https://esm.sh/standardwebhooks@1.0.0";
import { Resend } from "npm:resend";
const resend = new Resend(Deno.env.get("RESEND_API_KEY") as string);
const hookSecret = (Deno.env.get("SEND_EMAIL_HOOK_SECRET") as string).replace("v1,whsec_", "");
Deno.serve(async (req) => {
  if (req.method !== "POST") {
    return new Response("not allowed", { status: 400 });
  }
  const payload = await req.text();
  const headers = Object.fromEntries(req.headers);
  const wh = new Webhook(hookSecret);
  try {
    const { user, email_data } = wh.verify(payload, headers) as {
      user: {
        email: string;
      };
      email_data: {
        token: string;
        token_hash: string;
        redirect_to: string;
        email_action_type: string;
        site_url: string;
        token_new: string;
        token_hash_new: string;
      };
    };
    const { error } = await resend.emails.send({
      from: "welcome <onboarding@example.com>",
      to: [user.email],
      subject: "Welcome to my site!",
      text: `Confirm you signup with this code: ${email_data.token}`,
    });
    if (error) {
      throw error;
    }
  } catch (error) {
    return new Response(
      JSON.stringify({
        error: {
          http_code: error.code,
          message: error.message,
        },
      }),
      {
        status: 401,
        headers: { "Content-Type": "application/json" },
      },
    );
  }
  const responseHeaders = new Headers();
  responseHeaders.set("Content-Type", "application/json");
  return new Response(JSON.stringify({}), {
    status: 200,
    headers: responseHeaders,
  });
});
Deploy your edge function and configure it as a hook:

supabase functions deploy send-email --no-verify-jwt
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Email sending behavior
Email change behavior and token hash mapping
What to send
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
MFA verification hook
MFA Verification Hook

You can add additional checks to the Supabase MFA implementation with hooks. For example, you can:

Limit the number of verification attempts performed over a period of time.
Sign out users who have too many invalid verification attempts.
Count, rate limit, or ban sign-ins.
Inputs

Supabase Auth will send a payload containing these fields to your hook:

Field	Type	Description
factor_id	string	Unique identifier for the MFA factor being verified
factor_type	string	totp or phone
user_id	string	Unique identifier for the user
valid	boolean	Whether the verification attempt was valid. For TOTP, this means that the six digit code was correct (true) or incorrect (false).

JSON

JSON Schema
{
  "factor_id": "6eab6a69-7766-48bf-95d8-bd8f606894db",
  "user_id": "3919cb6e-4215-4478-a960-6d3454326cec",
  "valid": true
}
Outputs

Return this if your hook processed the input without errors.

Field	Type	Description
decision	string	The decision on whether to allow authentication to move forward. Use reject to deny the verification attempt and log the user out of all active sessions. Use continue to use the default Supabase Auth behavior.
message	string	The message to show the user if the decision was reject.
{
  "decision": "reject",
  "message": "You have exceeded maximum number of MFA attempts."
}

SQL

Limit failed MFA verification attempts
Your company requires that a user can input an incorrect MFA Verification code no more than once every 2 seconds.

Create a table to record the last time a user had an incorrect MFA verification attempt for a factor.

create table public.mfa_failed_verification_attempts (
  user_id uuid not null,
  factor_id uuid not null,
  last_failed_at timestamp not null default now(),
  primary key (user_id, factor_id)
);
Create a hook to read and write information to this table. For example:

create function public.hook_mfa_verification_attempt(event jsonb)
  returns jsonb
  language plpgsql
as $$
  declare
    last_failed_at timestamp;
  begin
    if event->'valid' is true then
      -- code is valid, accept it
      return jsonb_build_object('decision', 'continue');
    end if;
    select last_failed_at into last_failed_at
      from public.mfa_failed_verification_attempts
      where
        user_id = event->'user_id'
          and
        factor_id = event->'factor_id';
    if last_failed_at is not null and now() - last_failed_at < interval '2 seconds' then
      -- last attempt was done too quickly
      return jsonb_build_object(
        'error', jsonb_build_object(
          'http_code', 429,
          'message',   'Please wait a moment before trying again.'
        )
      );
    end if;
    -- record this failed attempt
    insert into public.mfa_failed_verification_attempts
      (
        user_id,
        factor_id,
        last_refreshed_at
      )
      values
      (
        event->'user_id',
        event->'factor_id',
        now()
      )
      on conflict do update
        set last_refreshed_at = now();
    -- finally let Supabase Auth do the default behavior for a failed attempt
    return jsonb_build_object('decision', 'continue');
  end;
$$;
-- Assign appropriate permissions and revoke access
grant all
  on table public.mfa_failed_verification_attempts
  to supabase_auth_admin;
revoke all
  on table public.mfa_failed_verification_attempts
  from authenticated, anon, public;
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Password verification hook
Password Verification Hook

Your company wishes to increase security beyond the requirements of the default password implementation in order to fulfill security or compliance requirements. You plan to track the status of a password sign-in attempt and take action via an email or a restriction on logins where necessary.

As this hook runs on unauthenticated requests, malicious users can abuse the hook by calling it multiple times. Pay extra care when using the hook as you can unintentionally block legitimate users from accessing your application.

Check if a password is valid prior to taking any additional action to ensure the user is legitimate. Where possible, send an email or notification instead of blocking the user.

Inputs

Field	Type	Description
user_id	string	Unique identifier for the user attempting to sign in. Correlate this to the auth.users table.
valid	boolean	Whether the password verification attempt was valid.

JSON

JSON Schema
{
  "user_id": "3919cb6e-4215-4478-a960-6d3454326cec",
  "valid": true
}
Outputs

Return these only if your hook processed the input without errors.

Field	Type	Description
decision	string	The decision on whether to allow authentication to move forward. Use reject to deny the verification attempt and log the user out of all active sessions. Use continue to use the default Supabase Auth behavior.
message	string	The message to show the user if the decision was reject.
should_logout_user	boolean	Whether to log out the user if a reject decision is issued. Has no effect when a continue decision is issued.
{
  "decision": "reject",
  "message": "You have exceeded maximum number of password sign-in attempts.",
  "should_logout_user": "false"
}

SQL

Limit failed password verification attempts

Send email notification on failed password attempts
As part of new security measures within the company, users can only input an incorrect password every 10 seconds and not more than that. You want to write a hook to enforce this.

Create a table to record each user's last incorrect password verification attempt.

create table public.password_failed_verification_attempts (
  user_id uuid not null,
  last_failed_at timestamp not null default now(),
  primary key (user_id)
);
Create a hook to read and write information to this table. For example:

create function public.hook_password_verification_attempt(event jsonb)
returns jsonb
language plpgsql
as $$
  declare
    last_failed_at timestamp;
  begin
    if event->'valid' is true then
      -- password is valid, accept it
      return jsonb_build_object('decision', 'continue');
    end if;
    select last_failed_at into last_failed_at
      from public.password_failed_verification_attempts
      where
        user_id = event->'user_id';
    if last_failed_at is not null and now() - last_failed_at < interval '10 seconds' then
      -- last attempt was done too quickly
      return jsonb_build_object(
        'error', jsonb_build_object(
          'http_code', 429,
          'message',   'Please wait a moment before trying again.'
        )
      );
    end if;
    -- record this failed attempt
    insert into public.password_failed_verification_attempts
      (
        user_id,
        last_failed_at
      )
      values
      (
        event->'user_id',
        now()
      )
      on conflict do update
        set last_failed_at = now();
    -- finally let Supabase Auth do the default behavior for a failed attempt
    return jsonb_build_object('decision', 'continue');
  end;
$$;
-- Assign appropriate permissions
grant all
  on table public.password_failed_verification_attempts
  to supabase_auth_admin;
revoke all
  on table public.password_failed_verification_attempts
  from authenticated, anon, public;
Edit this page on GitHub
Is this helpful?

No

Yes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
Auth Hooks
Before User Created hook
Before User Created Hook

Prevent unwanted signups by inspecting and rejecting user creation requests

This hook runs before a new user is created. It allows developers to inspect the incoming user object and optionally reject the request. Use this to enforce custom signup policies that Supabase Auth does not handle natively - such as blocking disposable email domains, restricting access by region or IP, or requiring that users belong to a specific email domain.

You can implement this hook using an HTTP endpoint or a Postgres function. If the hook returns an error object, the signup is denied and the user is not created. If the hook responds successfully (HTTP 200 or 204 with no error), the request proceeds as usual. This gives you full control over which users are allowed to register — and the flexibility to apply that logic server-side.

Inputs#
Supabase Auth will send a payload containing these fields to your hook:

Field	Type	Description
metadata	object	Metadata about the request. Includes IP address, request ID, and hook type.
user	object	The user record that is about to be created. Matches the shape of the auth.users table.
Because the hook is ran just before the insertion into the database, this user will not be found in Postgres at the time the hook is called.


JSON

JSON Schema
{
  "metadata": {
    "uuid": "8b34dcdd-9df1-4c10-850a-b3277c653040",
    "time": "2025-04-29T13:13:24.755552-07:00",
    "name": "before-user-created",
    "ip_address": "127.0.0.1"
  },
  "user": {
    "id": "ff7fc9ae-3b1b-4642-9241-64adb9848a03",
    "aud": "authenticated",
    "role": "",
    "email": "valid.email@supabase.com",
    "phone": "",
    "app_metadata": {
      "provider": "email",
      "providers": ["email"]
    },
    "user_metadata": {},
    "identities": [],
    "created_at": "0001-01-01T00:00:00Z",
    "updated_at": "0001-01-01T00:00:00Z",
    "is_anonymous": false
  }
}
Outputs#
Your hook must return a response that either allows or blocks the signup request.

Field	Type	Description
error	object	(Optional) Return this to reject the signup. Includes a code, message, and optional HTTP status code.
Returning an empty object with a 200 or 204 status code allows the request to proceed. Returning a JSON response with an error object and a 4xx status code blocks the request and propagates the error message to the client. See the error handling documentation for more details.

Allow the signup#
{}
or with a 204 No Content response:

HTTP/1.1 204 No Content
Reject the signup with an error#
{
  "error": {
    "http_code": 400,
    "message": "Only company emails are allowed to sign up."
  }
}
This response will block the user creation and return the error message to the client that attempted signup.

Examples#
Each of the following examples shows how to use the before-user-created hook to control signup behavior. Each use case includes both a HTTP implementation (e.g. using an Edge Function) and a SQL implementation (Postgres function).


SQL

HTTP

Allow by Domain

Block by OAuth Provider

Allow/Deny by IP or CIDR
Allow signups only from specific domains like supabase.com or example.test. Reject all others. This is useful for private/internal apps, enterprise gating, or invite-only beta access.

The before-user-created hook solves this by:

Detecting that a user is about to be created
Providing the email address in the user.email field
Run the following snippet in your project's SQL Editor. This will create a signup_email_domains table with some sample data and a hook_restrict_signup_by_email_domain function to be called by the before-user-created auth hook.

-- Create ENUM type for domain rule classification
do $$ begin
  create type signup_email_domain_type as enum ('allow', 'deny');
exception
  when duplicate_object then null;
end $$;
-- Create the signup_email_domains table
create table if not exists public.signup_email_domains (
  id serial primary key,
  domain text not null,
  type signup_email_domain_type not null,
  reason text default null,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
-- Create a trigger to maintain updated_at
create or replace function update_signup_email_domains_updated_at()
returns trigger as $$
begin
  new.updated_at = now();
  return new;
end;
$$ language plpgsql;
drop trigger if exists trg_signup_email_domains_set_updated_at on public.signup_email_domains;
create trigger trg_signup_email_domains_set_updated_at
before update on public.signup_email_domains
for each row
execute procedure update_signup_email_domains_updated_at();
-- Seed example data
insert into public.signup_email_domains (domain, type, reason) values
  ('supabase.com', 'allow', 'Internal signups'),
  ('gmail.com', 'deny', 'Public email provider'),
  ('yahoo.com', 'deny', 'Public email provider');
-- Create the function
create or replace function public.hook_restrict_signup_by_email_domain(event jsonb)
returns jsonb
language plpgsql
as $$
declare
  email text;
  domain text;
  is_allowed int;
  is_denied int;
begin
  email := event->'user'->>'email';
  domain := split_part(email, '@', 2);
  -- Check for allow match
  select count(*) into is_allowed
  from public.signup_email_domains
  where type = 'allow' and lower(domain) = lower($1);
  if is_allowed > 0 then
    return '{}'::jsonb;
  end if;
  -- Check for deny match
  select count(*) into is_denied
  from public.signup_email_domains
  where type = 'deny' and lower(domain) = lower($1);
  if is_denied > 0 then
    return jsonb_build_object(
      'error', jsonb_build_object(
        'message', 'Signups from this email domain are not allowed.',
        'http_code', 403
      )
    );
  end if;
  -- No match, allow by default
  return '{}'::jsonb;
end;
$$;
-- Permissions
grant execute
  on function public.hook_restrict_signup_by_email_domain
  to supabase_auth_admin;
revoke execute
  on function public.hook_restrict_signup_by_email_domain
  from authenticated, anon, public;
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Inputs
Outputs
Allow the signup
Reject the signup with an error
Examples
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Configuration
Custom SMTP
Send emails with custom SMTP

If you're using Supabase Auth with the following configuration:

Email and password accounts
Passwordless accounts using one-time passwords or links sent over email (OTP, magic link, invites)
Email-based user invitations from the Users page or from the Auth admin APIs
Social login with email confirmation
You will need to set up a custom SMTP server to handle the delivery of messages to your users.

To get you started and let you explore and set up email message templates for your application, Supabase provides a simple SMTP server for all projects. This server imposes a few important restrictions and is not meant for production use.

Send messages only to pre-authorized addresses.

Unless you configure a custom SMTP server for your project, Supabase Auth will refuse to deliver messages to addresses that are not part of the project's team. You can manage this in the Team tab of the organization's settings.

For example, if your project's organization has these member accounts person-a@example.com, person-b@example.com and person-c@example.com then Supabase Auth will only send messages to these addresses. All other addresses will fail with the error message Email address not authorized.

Significant rate-limits that can change over time.

To maintain the health and reputation of the default SMTP sending service, the number of messages your project can send is limited and can change without notice. Currently this value is set to 2 messages per hour.

No SLA guarantee on message delivery or uptime for the default SMTP service.

The default SMTP service is provided as best-effort only and intended for the following non-production use cases:

Exploring and getting started with Supabase Auth
Setting up and testing email templates with the members of the project's team
Building toy projects, demos or any non-mission-critical application
We urge all customers to set up custom SMTP server for all other use cases.

How to set up a custom SMTP server?#
Supabase Auth works with any email sending service that supports the SMTP protocol. First you will need to choose a service, create an account (if you already do not have one) and obtain the SMTP server settings and credentials for your account. These include: the SMTP server host, port, user and password. You will also need to choose a default From address, usually something like no-reply@example.com.

A non-exhaustive list of services that work with Supabase Auth is:

Resend
AWS SES
Postmark
Twilio SendGrid
ZeptoMail
Brevo
Once you've set up your account with an email sending service, head to the Authentication settings page to enable and configure custom SMTP.

You can also configure custom SMTP using the Management API:

# Get your access token from https://supabase.com/dashboard/account/tokens
export SUPABASE_ACCESS_TOKEN="your-access-token"
export PROJECT_REF="your-project-ref"
# Configure custom SMTP
curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "external_email_enabled": true,
    "mailer_secure_email_change_enabled": true,
    "mailer_autoconfirm": false,
    "smtp_admin_email": "no-reply@example.com",
    "smtp_host": "smtp.example.com",
    "smtp_port": 587,
    "smtp_user": "your-smtp-user",
    "smtp_pass": "your-smtp-password",
    "smtp_sender_name": "Your App Name"
  }'
Once you save these settings, your project's Auth server will send messages to all addresses. To protect the reputation of your newly set up service a low rate-limit of 30 messages per hour is imposed. To adjust this to an acceptable value for your use case head to the Rate Limits configuration page.

Dealing with abuse: How to maintain the sending reputation of your SMTP server?#
As you make your application known to the public and it grows in popularity, you can expect to see a few types of abuse that can negatively impact the reputation of your sending domain.

A common source of abuse is bots or attackers signing up users to your application.

They use lists of known email addresses to sign up users to your project with pre-determined passwords. These can vary in scale and intensity: sometimes the bots slowly send sign up requests over many months, or they send a lot of requests at once.

Usually the goal for this behavior is:

To negatively affect your email sending reputation, after which they might ask for a ransom promising to stop the behavior.
To cause a short-term or even long-term Denial of Service attack on your service, by preventing new account creation, signins with magic links or one-time passwords, or to severely impact important security flows in your application (such as reset password or forgot password).
To force you to reduce the security posture of your project, such as by disabling email confirmations. At that point, they may target specific or a broad number of users by creating an account in their name. Then they can use social engineering techniques to trick them to use your application in such a way that both attacker and victim have access to the same account.
Mitigation strategies:

Configure CAPTCHA protection for your project, which is the most effective way to control bots in this scenario. You can use CAPTCHA services which provide invisible challenges where real users won't be asked to solve puzzles most of the time.
Prefer social login (OAuth) or SSO with SAML instead of email-based authentication flows in your apps.
Prefer passwordless authentication (one-time password) as this limits the attacker's value to gain from this behavior.
Do not disable email confirmations under pressure.
Additional best practices#
Set up and maintain DKIM, DMARC and SPF configurations.

Work with your email sending service to configure DKIM, DMARC and SPF for your sending domain. This will significantly increase the deliverability of your messages.

Set up a custom domain.

Authentication messages often contain links to your project's Auth server. Setting up a custom domain will reduce the likelihood of your messages being picked up as spam due to another Supabase project's bad reputation.

Don't mix Auth emails with marketing emails.

Use separate services for Auth and marketing messages. If the reputation of one falls, it won't affect your whole application or operation.

This includes:

Use a separate sending domain for authentication -- auth.example.com and a separate domain for marketing marketing.example.com.
Use a separate From address -- no-reply@auth.example.com vs no-reply@marketing.example.com.
Have another SMTP service set up on stand-by.

In case the primary SMTP service you're using is experiencing difficulty, or your account is under threat of being blocked due to spam, you have another service to quickly turn to.

Use consistent branding and focused content.

Make sure you've separated out authentication messages from marketing messages.

Don't include promotional content as part of authentication messages.
Avoid talking about what your application is inside authentication messages. This can be picked up by automated spam filters which will classify the message as marketing and increase its chances of being regarded as spam. This problem is especially apparent if your project is related to: Web3, Blockchain, AI, NFTs, Gambling, Pornography.
Avoid taglines or other short-form marketing material in authentication messages.
Reduce the number of links and call-to-actions in authentication messages.
Change the authentication messages templates infrequently. Prefer a single big change over multiple smaller changes.
Avoid A/B testing content in authentication messages.
Use a separate base template (HTML) from your marketing messages.
Avoid the use of email signatures in authentication messages. If you do, make sure the signatures are different in style and content from your marketing messages.
Use short and to-the-point subject lines. Avoid or reduce the number of emojis in subjects.
Reduce the number of images placed in authentication messages.
Avoid including user-provided data such as names, usernames, email addresses or salutations in authentication messages. If you do, make sure they are sanitized.
Prepare for large surges ahead of time.

If you are planning on having a large surge of users coming at a specific time, work with your email sending service to adjust the rate limits and their expectations accordingly. Most email sending services dislike spikes in the number of messages being sent, and this may affect your sending reputation.

Consider implementing additional protections for such events:

Build a queuing or waitlist system instead of allowing direct sign-up, which will help you control the number of messages being sent from the email sending service.
Disable email-based sign ups for the event and use social login only. Alternatively you can deprioritize the email-based sign-up flows for the event by hiding them in the UI or making them harder to reach.
Use the Send Email Auth Hook for more control.

If you need more control over the sending process, instead of using a SMTP server you can use the Send Email Auth Hook. This can be useful in advanced scenarios such as:

You want to use React or a different email templating engine.
You want to use an email sending service that does not provide an SMTP service, or the non-SMTP API is more powerful.
You want to queue up messages instead of sending them immediately, in an effort to smooth out spikes in email sending or do additional filtering (avoid repetitive messages).
You want to use multiple email sending services to increase reliability (if primary service is unavailable, use backup service automatically).
You want to use different email sending services based on the email address or user data (e.g. service A for users in the USA, service B for users in the EU, service C for users in China).
You want to add or include additional email headers in messages, for tracking or other reasons.
You want to add attachments to the messages (generally not recommended).
You want to add S/MIME signatures to messages.
You want to use an email server not open to the Internet, such as some corporate or government mail servers.
Increase the duration of user sessions.

Having short lived user sessions can be problematic for email sending, as it forces active users to sign-in frequently, increasing the number of messages needed to be sent. Consider increasing the maximum duration of user sessions. If you do see an unnecessary increase in logins without a clear cause, check your frontend application for bugs.

If you are using a SSR framework on the frontend and are seeing an increased number of user logins without a clear cause, check your set up. Make sure to keep the @supabase/ssr package up to date and closely follow the guides we publish. Make sure that the middleware components of your SSR frontend works as intended and matches the guides we've published. Sometimes a misplaced return or conditional can cause early session termination.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
How to set up a custom SMTP server?
Dealing with abuse: How to maintain the sending reputation of your SMTP server?
Additional best practices
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Configuration
User Management
User Management

View, delete, and export user information.

You can view your users on the Users page of the Dashboard. You can also view the contents of the Auth schema in the Table Editor.

Accessing user data via API#
For security, the Auth schema is not exposed in the auto-generated API. If you want to access users data via the API, you can create your own user tables in the public schema.

Make sure to protect the table by enabling Row Level Security. Reference the auth.users table to ensure data integrity. Specify on delete cascade in the reference. For example, a public.profiles table might look like this:

create table public.profiles (
  id uuid not null references auth.users on delete cascade,
  first_name text,
  last_name text,
  primary key (id)
);
alter table public.profiles enable row level security;
Only use primary keys as foreign key references for schemas and tables like auth.users which are managed by Supabase. Postgres lets you specify a foreign key reference for columns backed by a unique index (not necessarily primary keys).

Primary keys are guaranteed not to change. Columns, indices, constraints or other database objects managed by Supabase may change at any time and you should be careful when referencing them directly.

To update your public.profiles table every time a user signs up, set up a trigger. If the trigger fails, it could block signups, so test your code thoroughly.

-- inserts a row into public.profiles
create function public.handle_new_user()
returns trigger
language plpgsql
security definer set search_path = ''
as $$
begin
  insert into public.profiles (id, first_name, last_name)
  values (new.id, new.raw_user_meta_data ->> 'first_name', new.raw_user_meta_data ->> 'last_name');
  return new;
end;
$$;
-- trigger the function every time a user is created
create trigger on_auth_user_created
  after insert on auth.users
  for each row execute procedure public.handle_new_user();
Adding and retrieving user metadata#
You can assign metadata to users on sign up:


JavaScript

Dart

Swift

Kotlin
const { data, error } = await supabase.auth.signUp({
  email: 'valid.email@supabase.io',
  password: 'example-password',
  options: {
    data: {
      first_name: 'John',
      age: 27,
    },
  },
})
User metadata is stored on the raw_user_meta_data column of the auth.users table. To view the metadata:


JavaScript

Dart

Swift

Kotlin
const {
  data: { user },
} = await supabase.auth.getUser()
let metadata = user?.user_metadata
Deleting users#
You may delete users directly or via the management console at Authentication > Users. Note that deleting a user from the auth.users table does not automatically sign out a user. As Supabase makes use of JSON Web Tokens (JWT), a user's JWT will remain "valid" until it has expired.

You cannot delete a user if they are the owner of any objects in Supabase Storage.

You will encounter an error when you try to delete an Auth user that owns any Storage objects. If this happens, try deleting all the objects for that user, or reassign ownership to another user.

Exporting users#
As Supabase is built on top of Postgres, you can query the auth.users and auth.identities table via the SQL Editor tab to extract all users:

select * from auth.users;
You can then export the results as CSV.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Accessing user data via API
Adding and retrieving user metadata
Deleting users
Exporting users
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut
Auth
Security
Password Security
Password security

Help your users to protect their password security

A password is more secure if it is harder to guess or brute-force. In theory, a password is harder to guess if it is longer. It is also harder to guess if it uses a larger set of characters (for example, digits, lowercase and uppercase letters, and symbols).

This table shows the minimum number of guesses that need to be tried to access a user's account:

Required characters	Length	Guesses
Digits only	8	~ 227
Digits and letters	8	~ 241
Digits, lower and uppercase letters	8	~ 248
Digits, lower and uppercase letters, symbols	8	~ 252
In reality though, passwords are not always generated at random. They often contain variations of names, words, dates, and common phrases. Malicious actors can use these properties to guess a password in fewer attempts.

There are hundreds of millions (and growing!) known passwords out there. Malicious actors can use these lists of leaked passwords to automate login attempts (known as credential stuffing) and steal or access sensitive user data.

Password strength and leaked password protection#
To help protect your users, Supabase Auth allows you fine-grained control over the strength of the passwords used on your project. You can configure these in your project's Auth settings:

Set a large minimum password length. Anything less than 8 characters is not recommended.
Set the required characters that must appear at least once in a user's password. Use the strongest option of requiring digits, lowercase and uppercase letters, and symbols. The allowed symbols are: !@#$%^&*()_+-=[]{};'\:"|<>?,./`~
Prevent the use of leaked passwords. Supabase Auth uses the open-source HaveIBeenPwned.org Pwned Passwords API to reject passwords that have been leaked and are known by malicious actors.
Leaked password protection is available on the Pro Plan and above.

Additional recommendations#
In addition to choosing suitable password strength settings and preventing the use of leaked passwords, consider asking your users to:

Use a password manager to store and generate passwords.
Avoid password reuse across websites and apps.
Avoid using personal information in passwords.
Use Multi-Factor Authentication.
Frequently asked questions#
How are passwords stored?#
Supabase Auth uses bcrypt, a strong password hashing function, to store hashes of users' passwords. Only hashed passwords are stored. You cannot impersonate a user with the password hash. Each hash is accompanied by a randomly generated salt parameter for extra security.

The hash is stored in the encrypted_password column of the auth.users table. The column's name is a misnomer (cryptographic hashing is not encryption), but is kept for backward compatibility.

How will strengthened password requirements affect current users?#
Existing users can still sign in with their current password even if it doesn't meet the new, strengthened password requirements. However, if their password falls short of these updated standards, they will encounter a WeakPasswordError during the signInWithPassword process, explaining why it's considered weak. This change is also applicable to new users and existing users changing their passwords, ensuring everyone adheres to the enhanced security standards.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Password strength and leaked password protection
Additional recommendations
Frequently asked questions
How are passwords stored?
How will strengthened password requirements affect current users?
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Security
Rate Limits
Rate limits

Rate limits protect your services from abuse

Supabase Auth enforces rate limits on endpoints to prevent abuse. Some rate limits are customizable.

You can also manage rate limits using the Management API:

# Get your access token from https://supabase.com/dashboard/account/tokens
export SUPABASE_ACCESS_TOKEN="your-access-token"
export PROJECT_REF="your-project-ref"
# Get current rate limits
curl -X GET "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  | jq 'to_entries | map(select(.key | startswith("rate_limit_"))) | from_entries'
# Update rate limits
curl -X PATCH "https://api.supabase.com/v1/projects/$PROJECT_REF/config/auth" \
  -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "rate_limit_anonymous_users": 10,
    "rate_limit_email_sent": 10,
    "rate_limit_sms_sent": 10,
    "rate_limit_verify": 10,
    "rate_limit_token_refresh": 10,
    "rate_limit_otp": 10,
    "rate_limit_web3": 10
  }'

Endpoint	Path	Limited By	Rate Limit
All endpoints that send emails	/auth/v1/signup /auth/v1/recover /auth/v1/user1	Sum of combined requests	Defaults to 4 emails per hour as of 14th July 2023. As of 21 Oct 2023, this has been updated to 2 emails per hour. You can only change this with your own custom SMTP setup.
All endpoints that send One-Time-Passwords (OTP)	/auth/v1/otp	Sum of combined requests	Defaults to 30 OTPs per hour. Is customizable.
Send OTPs or magic links	/auth/v1/otp	Last request of the user	Defaults to 60 seconds window before a new request is allowed to the same user. Is customizable.
Signup confirmation request	/auth/v1/signup	Last request of the user	Defaults to 60 seconds window before a new request is allowed to the same user. Is customizable.
Password Reset Request	/auth/v1/recover	Last request of the user	Defaults to 60 seconds window before a new request is allowed to the same user. Is customizable.
Verification requests	/auth/v1/verify	IP Address	360 requests per hour (with bursts up to 30 requests)
Token refresh requests	/auth/v1/token	IP Address	1800 requests per hour (with bursts up to 30 requests)
Create or Verify an MFA challenge	/auth/v1/factors/:id/challenge /auth/v1/factors/:id/verify	IP Address	15 requests per hour (with bursts up to requests)
Anonymous sign-ins	/auth/v1/signup2	IP Address	30 requests per hour (with bursts up to 30 requests)
Footnotes#
The rate limit is only applied on /auth/v1/user if this endpoint is called to update the user's email address. ↩

The rate limit is only applied on /auth/v1/signup if this endpoint is called without passing in an email or phone number in the request body. ↩

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Footnotes
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Security
Bot Detection (CAPTCHA)
Enable CAPTCHA Protection

Supabase provides you with the option of adding CAPTCHA to your sign-in, sign-up, and password reset forms. This keeps your website safe from bots and malicious scripts. Supabase authentication has support for hCaptcha and Cloudflare Turnstile.

Sign up for CAPTCHA#

HCaptcha

Turnstile
Go to the hCaptcha website and sign up for an account. On the Welcome page, copy the Sitekey and Secret key.

If you have already signed up and didn't copy this information from the Welcome page, you can get the Secret key from the Settings page.

site_secret_settings.png

The Sitekey can be found in the Settings of the active site you created.

sites_dashboard.png

In the Settings page, look for the Sitekey section and copy the key.

sitekey_settings.png

Enable CAPTCHA protection for your Supabase project#
Navigate to the Auth section of your Project Settings in the Supabase Dashboard and find the Enable CAPTCHA protection toggle under Settings > Authentication > Bot and Abuse Protection > Enable CAPTCHA protection.

Select your CAPTCHA provider from the dropdown, enter your CAPTCHA Secret key, and click Save.

Add the CAPTCHA frontend component#
The frontend requires some changes to provide the CAPTCHA on-screen for the user. This example uses React and the corresponding CAPTCHA React component, but both CAPTCHA providers can be used with any JavaScript framework.


HCaptcha

Turnstile
Install @hcaptcha/react-hcaptcha in your project as a dependency.

npm install @hcaptcha/react-hcaptcha
Now import the HCaptcha component from the @hcaptcha/react-hcaptcha library.

import HCaptcha from '@hcaptcha/react-hcaptcha'
Let's create a empty state to store our captchaToken

const [captchaToken, setCaptchaToken] = useState()
Now lets add the HCaptcha component to the JSX section of our code

<HCaptcha />
We will pass it the sitekey we copied from the hCaptcha website as a property along with a onVerify property which takes a callback function. This callback function will have a token as one of its properties. Let's set the token in the state using setCaptchaToken

<HCaptcha
  sitekey="your-sitekey"
  onVerify={(token) => {
    setCaptchaToken(token)
  }}
/>
Now lets use the CAPTCHA token we receive in our Supabase signUp function.

await supabase.auth.signUp({
  email,
  password,
  options: { captchaToken },
})
We will also need to reset the CAPTCHA challenge after we have made a call to the function above.

Create a ref to use on our HCaptcha component.

const captcha = useRef()
Let's add a ref attribute on the HCaptcha component and assign the captcha constant to it.

<HCaptcha
  ref={captcha}
  sitekey="your-sitekey"
  onVerify={(token) => {
    setCaptchaToken(token)
  }}
/>
Reset the captcha after the signUp function is called using the following code:

captcha.current.resetCaptcha()
In order to test that this works locally we will need to use something like ngrok or add an entry to your hosts file. You can read more about this in the hCaptcha docs.

Run the application and you should now be provided with a CAPTCHA challenge.

Edit this page on GitHub

Watch video guide

Video guide preview
Is this helpful?

No

Yes
On this page
Sign up for CAPTCHA
Enable CAPTCHA protection for your Supabase project
Add the CAPTCHA frontend component
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth
Security
Audit Logs
Auth Audit Logs

Monitor and track authentication events with audit logging.

Auth audit logs provide comprehensive tracking of authentication events in your Supabase project. Audit logs are automatically captured for all authentication events and help you monitor user authentication activities, detect suspicious behavior, and maintain compliance with security requirements.

What gets logged#
Supabase auth audit logs automatically capture all authentication events including:

User signups and logins
Password changes and resets
Email verification events
Token refresh and logout events
Storage options#
By default, audit logs are stored in two places:

Your project's Postgres database - Stored in the auth.audit_log_entries table, searchable via SQL but uses database storage
External log storage - Cost-efficient storage accessible through the dashboard
You can disable Postgres storage to reduce database storage costs while keeping the external log storage.

Configuring audit log storage#
Navigate to your project dashboard
Go to Authentication
Find the Audit Logs under Configuration section
Toggle on "Disable writing auth audit logs to project database" to disable database storage
Disabling Postgres storage reduces your database storage costs. Audit logs will still be available through the dashboard.

Log format#
Audit logs contain detailed information about each authentication event:

{
  "timestamp": "2025-08-01T10:30:00Z",
  "user_id": "uuid",
  "action": "user_signedup",
  "ip_address": "192.168.1.1",
  "user_agent": "Mozilla/5.0...",
  "metadata": {
    "provider": "email"
  }
}
Log actions reference#
Action	Description
login	User login attempt
logout	User logout
invite_accepted	Team invitation accepted
user_signedup	New user registration
user_invited	User invitation sent
user_deleted	User account deleted
user_modified	User profile updated
user_recovery_requested	Password reset request
user_reauthenticate_requested	User reauthentication required
user_confirmation_requested	Email/phone confirmation requested
user_repeated_signup	Duplicate signup attempt
user_updated_password	Password change completed
token_revoked	Refresh token revoked
token_refreshed	Refresh token used to obtain new tokens
generate_recovery_codes	MFA recovery codes generated
factor_in_progress	MFA factor enrollment started
factor_unenrolled	MFA factor removed
challenge_created	MFA challenge initiated
verification_attempted	MFA verification attempt
factor_deleted	MFA factor deleted
recovery_codes_deleted	MFA recovery codes deleted
factor_updated	MFA factor settings updated
mfa_code_login	Login with MFA code
identity_unlinked	An identity unlinked from account
Limitations#
There may be a short delay before logs appear
Query capabilities are limited to the dashboard interface
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
What gets logged
Storage options
Configuring audit log storage
Log format
Log actions reference
Limitations
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

Auth

More
JSON Web Tokens (JWT)
Overview
JSON Web Token (JWT)

Information on how best to use JSON Web Tokens with Supabase

A JSON Web Token is a type of data structure, represented as a string, that usually contains identity and authorization information about a user. It encodes information about its lifetime and is signed with a cryptographic key to make it tamper-resistant.

Supabase Auth continuously issues a new JWT for each user session, for as long as the user remains signed in. Check the comprehensive guide on Sessions to find out how you can tailor this process for your needs.

JWTs provide the foundation for Row Level Security. Each Supabase product is able to securely decode and verify the validity of a JWT it receives before using Postgres policies and roles to authorize access to the project's data.

Supabase provides a comprehensive system of managing JWT Signing Keys used to create and verify JSON Web Tokens.

Introduction#
JWTs are strings that have the following structure:

<header>.<payload>.<signature>
Each part is a string of Base64-URL encoded JSON, or bytes for the signature.

Header

{
  "typ": "JWT",
  "alg": "<HS256 | ES256 | RS256>",
  "kid": "<unique key identifier>"
}
Gives some basic identifying information about the string, indicating its type typ, the cryptographic algorithm alg that can be used to verify the data, and optionally the unique key identifier that should be used when verifying it.

Payload

{
  "iss": "https://project_id.supabase.co/auth/v1",
  "exp": 12345678,
  "sub": "<user ID>",
  "role": "authenticated",
  "email": "someone@example.com",
  "phone": "+15552368"
  // ...
}
Provides identifying information (called "claims") about the user (or other entity) that is represented by the token. Usually a JWT conveys information about what the user can access (then called Access Token) or who the user is (then called ID Token). You can use a Custom Access Token Hook to add, remove or change claims present in the token. A few claims are important:

Claim	Description
iss	Identifies the server which issued the token. If you append /.well-known/jwks.json to this URL you'll get access to the public keys with which you can verify the token.
exp	Sets a time limit after which the token should not be trusted and is considered expired, even if it is properly signed.
sub	Means subject, is the unique ID of the user represented by the token.
role	The Postgres role to use when applying Row Level Security policies.
...	All other claims are useful for quick access to profile information without having to query the database or send a request to the Auth server.
Signature

A digital signature using a shared secret or public-key cryptography. The purpose of the signature is to verify the authenticity of the <header>.<payload> string without relying on database access, liveness or performance of the Auth server. To verify the signature avoid implementing the algorithms yourself and instead rely on supabase.auth.getClaims(), or other high-quality JWT verification libraries for your language.

Supabase and JWTs#
Supabase creates JWTs in these cases for you:

When using Supabase Auth, an access token (JWT) is created for each user while they remain signed in. These are short lived, so they are continuously issued as your user interacts with Supabase APIs.
As the legacy JWT-based API keys anon and service_role. These have a 10 year expiry and are signed with a shared secret, making them hard to rotate or expire. These JWTs express public access via the anon key, or elevated access via the service_role key. We strongly recommend switching to publishable and secret API keys.
On-the-fly when using publishable or secret API keys. Each API key is transformed into a short-lived JWT that is then used to authorize access to your data. Accessing these short-lived tokens is generally not possible.
In addition to creating JWTs, Supabase can also accept JWTs from other Auth servers via the Third-Party Auth feature or ones you've made yourself using the legacy JWT secret or if you've imported in JWT Signing Key.

Using custom or third-party JWTs#
The supabase.auth.getClaims() method is meant to be used only with JWTs issued by Supabase Auth. If you make your own JWTs using the legacy JWT secret or a key you've imported, the verification may fail. We strongly recommend using a JWT verification library for your language to verify this type of JWT based on the claims you're adding in them.

Your Supabase project accepts a JWT in the Authorization: Bearer <jwt> header. If you're using the Supabase client library, it does this for you.

If you are already using Supabase Auth, when a user is signed in, their access token JWT is automatically managed and sent for you with every API call.

If you wish to send a JWT from a Third-Party Auth provider, or one you made yourself by using the legacy JWT secret or a JWT signing key you imported, you can pass it to the client library using the accessToken option.


TypeScript

Flutter

Swift (iOS)

Kotlin

cURL
import { createClient } from '@supabase/supabase-js'
const supabase = createClient(
  'https://<supabase-project>.supabase.co',
  'SUPABASE_PUBLISHABLE_KEY',
  {
    accessToken: async () => {
      return '<your JWT here>'
    },
  }
)
In the past there was a recommendation to set custom headers on the Supabase client with the Authorization header including your custom JWT. This is no longer recommended as it's less flexible and causes confusion when combined with a user session from Supabase Auth.

Verifying a JWT from Supabase#
If you're not able to use the Supabase client libraries, the following can be used to help you securely verify JWTs issued by Supabase.

Supabase Auth exposes a JSON Web Key Set URL for each Supabase project:

GET https://project-id.supabase.co/auth/v1/.well-known/jwks.json
Which responds with JWKS object containing one or more asymmetric JWT signing keys (only their public keys). Be aware that this endpoint does not return any keys if you are not using asymmetric JWT signing keys.

{
  "keys": [
    {
      "kid": "<match with kid from JWT header>",
      "alg": "<match with alg from JWT header>",
      "kty": "<RSA|EC|OKP>",
      "key_ops": ["verify"]
      // public key fields
    }
  ]
}
This endpoint is served directly from the Auth server, but is also additionally cached by the Supabase Edge for 10 minutes, significantly speeding up access to this data regardless of where you're performing the verification. It's important to be aware of the cache expiry time to prevent unintentionally rejecting valid user access tokens. We recommend waiting at least 20 minutes when creating a standby signing key, or revoking a previously used key.

Make sure that you do not cache this data for longer in your application, as it might make revocation difficult. If you do, make sure to provide a way to purge this cache when rotating signing keys to avoid unintentionally rejecting valid user access tokens.

Below is an example of how to use the jose TypeScript JWT verification library with Supabase JWTs:

import { jwtVerify, createRemoteJWKSet } from 'jose'
const PROJECT_JWKS = createRemoteJWKSet(
  new URL('https://project-id.supabase.co/auth/v1/.well-known/jwks.json')
)
/**
 * Verifies the provided JWT against the project's JSON Web Key Set.
 */
async function verifyProjectJWT(jwt: string) {
  return jwtVerify(jwt, PROJECT_JWKS)
}
Verifying with the legacy JWT secret or a shared secret signing key#
If your project is still using the legacy JWT secret, or you're using a shared secret (HS256) signing key, we recommend always verifying a user access token directly with the Auth server by sending a request like so:

GET https://project-id.supabase.co/auth/v1/user
apikey: publishable or anon legacy API key
Authorization: Bearer <JWT>
If the server responds with HTTP 200 OK, the JWT is valid, otherwise it is not.

Because the Auth server runs only in your project's specified region and is not globally distributed, doing this check can be quite slow depending on where you're performing the check. Avoid doing checks like this from servers or functions running on the edge, and prefer routing to a server within the same geographical region as your project.

If you are using the legacy JWT secret, or you've imported your own shared secret (HS256) signing key, you may wish to verify using the shared secret. We strongly recommend against this approach.

There is almost no benefit from using a JWT signed with a shared secret. Although it's computationally more efficient and verification is simpler to code by hand, using this approach can expose your project's data to significant security vulnerabilities or weaknesses.

Consider the following:

Using a shared secret can make it more difficult to keep aligned with security compliance frameworks such as SOC2, PCI-DSS, ISO27000, HIPAA, etc.
A shared secret that is in the hands of a malicious actor can be used to impersonate your users, give them access to privileged actions or data.
It is difficult to detect or identify when or how a shared secret has been given to a malicious actor.
Consider who might have even accidental access to the shared secret: systems, staff, devices (and their disk encryption and vulnerability patch status).
A malicious actor can use a shared secret far into the future, so lacking current evidence of compromise does not mean your data is secure.
It can be very easy to accidentally leak the shared secret in publicly available source code such as in your website or frontend, mobile app package or other executable. This is especially true if you accidentally add the secret in environment variables prefixed with NEXT_PUBLIC_, VITE_, PUBLIC_ or other conventions by web frameworks.
Rotating shared secrets might require careful coordination to avoid downtime of your app.
Check the JWT verification libraries for your language on how to securely verify JWTs signed with the legacy JWT secret or a shared secret (HS256) signing key. We strongly recommend relying on the Auth server as described above, or switching to a different signing key based on public key cryptography (RSA, Elliptic Curves) instead.

Resources#
JWT debugger: https://jwt.io/
JWT Signing Keys
JWT Claims Reference - Complete reference for all JWT claims used by Supabase Auth
API keys
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Introduction
Supabase and JWTs
Using custom or third-party JWTs
Verifying a JWT from Supabase
Verifying with the legacy JWT secret or a shared secret signing key
Resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Aut

JWT Claims Reference

Complete reference for claims appearing in JWTs created by Supabase Auth

This page provides a comprehensive reference for all JWT claims used in Supabase authentication tokens. This information is essential for server-side JWT validation and serialization, especially when implementing authentication in languages like Rust where field names like ref are reserved keywords.

JWT structure overview#
Supabase JWTs follow the standard JWT structure with three parts:

Header: Contains algorithm and key information
Payload: Contains the claims (user data and metadata)
Signature: Cryptographic signature for verification
The payload contains various claims that provide user identity, authentication level, and authorization information.

Required claims#
These claims are always present in Supabase JWTs and cannot be removed:

Field	Type	Description	Example
iss	string	Issuer - The entity that issued the JWT	"https://project-ref.supabase.co/auth/v1"
aud	string | string[]	Audience - The intended recipient of the JWT	"authenticated" or "anon"
exp	number	Expiration Time - Unix timestamp when the token expires	1640995200
iat	number	Issued At - Unix timestamp when the token was issued	1640991600
sub	string	Subject - The user ID (UUID)	"123e4567-e89b-12d3-a456-426614174000"
role	string	Role - User's role in the system	"authenticated", "anon", "service_role"
aal	string	Authenticator Assurance Level - Authentication strength	"aal1", "aal2"
session_id	string	Session ID - Unique session identifier	"session-uuid"
email	string	Email - User's email address	"user@example.com"
phone	string	Phone - User's phone number	"+1234567890"
is_anonymous	boolean	Anonymous Flag - Whether the user is anonymous	false
Optional claims#
These claims may be present depending on the authentication context:

Field	Type	Description	Example
jti	string	JWT ID - Unique identifier for the JWT	"jwt-uuid"
nbf	number	Not Before - Unix timestamp before which the token is invalid	1640991600
app_metadata	object	App Metadata - Application-specific user data	{"provider": "email"}
user_metadata	object	User Metadata - User-specific data	{"name": "John Doe"}
amr	array	Authentication Methods Reference - List of authentication methods used	[{"method": "password", "timestamp": 1640991600}]
Special claims#
Field	Type	Description	Example	Context
ref	string	Project Reference - Supabase project identifier	"abcdefghijklmnopqrst"	Anon/Service role tokens only
Field value constraints#
Authenticator assurance level (aal)#
Value	Description
"aal1"	Single-factor authentication (password, OAuth, etc.)
"aal2"	Multi-factor authentication (password + TOTP, etc.)
Role values (role)#
Value	Description	Use Case
"anon"	Anonymous user	Public access with RLS policies
"authenticated"	Authenticated user	Standard user access
"service_role"	Service role	Admin privileges (server-side only)
Audience values (aud)#
Value	Description
"authenticated"	For authenticated user tokens
"anon"	For anonymous user tokens
Authentication methods (amr.method)#
Value	Description
"oauth"	OAuth provider authentication
"password"	Email/password authentication
"otp"	One-time password
"totp"	Time-based one-time password
"recovery"	Account recovery
"invite"	Invitation-based signup
"sso/saml"	SAML single sign-on
"magiclink"	Magic link authentication
"email/signup"	Email signup
"email_change"	Email change
"token_refresh"	Token refresh
"anonymous"	Anonymous authentication
JWT examples#
Authenticated user token#
{
  "aal": "aal1",
  "amr": [
    {
      "method": "password",
      "timestamp": 1640991600
    }
  ],
  "app_metadata": {
    "provider": "email",
    "providers": ["email"]
  },
  "aud": "authenticated",
  "email": "user@example.com",
  "exp": 1640995200,
  "iat": 1640991600,
  "iss": "https://abcdefghijklmnopqrst.supabase.co/auth/v1",
  "phone": "",
  "role": "authenticated",
  "session_id": "123e4567-e89b-12d3-a456-426614174000",
  "sub": "123e4567-e89b-12d3-a456-426614174000",
  "user_metadata": {
    "name": "John Doe"
  },
  "is_anonymous": false
}
Anonymous user token#
{
  "iss": "supabase",
  "ref": "abcdefghijklmnopqrst",
  "role": "anon",
  "iat": 1640991600,
  "exp": 1640995200
}
Service role token#
{
  "iss": "supabase",
  "ref": "abcdefghijklmnopqrst",
  "role": "service_role",
  "iat": 1640991600,
  "exp": 1640995200
}
Language-Specific considerations#
Rust#
In Rust, the ref field is a reserved keyword. When deserializing JWTs, you'll need to handle this:

use serde::{Deserialize, Serialize};
#[derive(Debug, Deserialize, Serialize)]
struct JwtClaims {
    iss: String,
    #[serde(rename = "ref")] // Handle reserved keyword
    project_ref: Option<String>,
    role: String,
    iat: i64,
    exp: i64,
    // ... other claims
}
TypeScript/JavaScript#
interface JwtClaims {
  iss: string
  aud: string | string[]
  exp: number
  iat: number
  sub: string
  role: string
  aal: 'aal1' | 'aal2'
  session_id: string
  email: string
  phone: string
  is_anonymous: boolean
  jti?: string
  nbf?: number
  app_metadata?: Record<string, any>
  user_metadata?: Record<string, any>
  amr?: Array<{
    method: string
    timestamp: number
  }>
  ref?: string // Only in anon/service role tokens
}
Python#
from typing import Optional, Union, List, Dict, Any
from dataclasses import dataclass
@dataclass
class AmrEntry:
    method: str
    timestamp: int
@dataclass
class JwtClaims:
    iss: str
    aud: Union[str, List[str]]
    exp: int
    iat: int
    sub: str
    role: str
    aal: str
    session_id: str
    email: str
    phone: str
    is_anonymous: bool
    jti: Optional[str] = None
    nbf: Optional[int] = None
    app_metadata: Optional[Dict[str, Any]] = None
    user_metadata: Optional[Dict[str, Any]] = None
    amr: Optional[List[AmrEntry]] = None
    ref: Optional[str] = None  # Only in anon/service role tokens
Go#
type AmrEntry struct {
    Method    string `json:"method"`
    Timestamp int64  `json:"timestamp"`
}
type JwtClaims struct {
    Iss         string                 `json:"iss"`
    Aud         interface{}            `json:"aud"` // string or []string
    Exp         int64                  `json:"exp"`
    Iat         int64                  `json:"iat"`
    Sub         string                 `json:"sub"`
    Role        string                 `json:"role"`
    Aal         string                 `json:"aal"`
    SessionID   string                 `json:"session_id"`
    Email       string                 `json:"email"`
    Phone       string                 `json:"phone"`
    IsAnonymous bool                   `json:"is_anonymous"`
    Jti         *string                `json:"jti,omitempty"`
    Nbf         *int64                 `json:"nbf,omitempty"`
    AppMetadata map[string]interface{} `json:"app_metadata,omitempty"`
    UserMetadata map[string]interface{} `json:"user_metadata,omitempty"`
    Amr         []AmrEntry             `json:"amr,omitempty"`
    Ref         *string                `json:"ref,omitempty"` // Only in anon/service role tokens
}
Validation guidelines#
When implementing JWT validation on your server:

Check Required Fields: Ensure all required claims are present
Validate Types: Verify field types match expected types
Check Expiration: Validate exp timestamp is in the future
Verify Issuer: Ensure iss matches your Supabase project
Check Audience: Validate aud matches expected audience
Handle Reserved Keywords: Use field renaming for languages like Rust
Security considerations#
Always validate the JWT signature before trusting any claims
Never expose service role tokens to client-side code
Validate all claims before trusting the JWT
Check token expiration on every request
Use HTTPS for all JWT transmission
Rotate JWT secrets regularly
Implement proper error handling for invalid tokens
Related documentation#
JWT Overview
Custom Access Token Hooks
Row Level Security
Server-Side Auth
Edit this page on GitHub
JWT Signing Keys

Best practices on managing keys used by Supabase Auth to create and verify JSON Web Tokens

Supabase Auth continuously issues a new JWT for each user session, for as long as the user remains signed in. JWT signing keys provide fine grained control over this important process for the security of your application.

Before continuing check the comprehensive guide on Sessions for all the details about how Auth creates tokens for a user's session. Read up on JWTs if you are not familiar with the basics.

Overview#
When a JWT is issued by Supabase Auth, the key used to create its signature is known as the signing key. Supabase provides two systems for dealing with signing keys: the Legacy system based on the JWT secret, and the new Signing keys system.

System	Type	Description
Legacy	JWT secret	Initially Supabase was designed to use a single shared secret key to sign all JWTs. This includes the anon and service_role keys, all user access tokens including some Storage pre-signed URLs. No longer recommended. Available for backward compatibility.
Signing keys	Asymmetric key (RSA, Elliptic Curves)	A JWT signing key based on public-key cryptography (RSA, Elliptic Curves) that follows industry best practices and significantly improves the security, reliability and performance of your applications.
Signing keys	Shared secret key	A JWT signing key based on a shared secret.
Benefits of the signing keys system#
We've designed the Signing keys system to address many problems the legacy system had. It goes hand-in-hand with the publishable and secret API keys.

Benefit	Legacy JWT secret	JWT signing keys
Performance	Increased app latency as JWT validation is done by Auth server.	If using asymmetric signing key, JWT validation is fast and does not involve Auth server.
Reliability	To ensure secure revocation, Auth server is in the hot path of your application.	If using asymmetric signing key, JWT validation is local and fast and does not involve Auth server.
Security	Requires changing of your application's backend components to fully revoke a compromised secret.	If using asymmetric signing key, revocation is automatic via the key discovery endpoint.
Zero-downtime rotation	Downtime, sometimes being significant. Requires careful coordination with API keys.	No downtime, as each rotation step is independent and reversible.
Users signed out during rotation	Currently active users get immediately signed out.	No users get signed out.
Independence from API keys	anon and service_role must be rotated simultaneously.	Publishable and secret API keys no longer are based on the JWT signing key and can be independently managed.
Security compliance frameworks (SOC2, etc.)	Difficult to remain aligned as the secret can be extracted from Supabase.	Easier alignment as the private key or shared secret can't be extracted. Row Level Security has strong key revocation guarantees.
Getting started#
You can start migrating away from the legacy JWT secret through the Supabase dashboard. This process does not cause downtime for your application.

Start off by clicking the Migrate JWT secret button on the JWT signing keys page. This step will import the existing legacy JWT secret into the new JWT signing keys system. Once this process completes, you will no longer be able to rotate the legacy JWT secret using the old system.
Simultaneously, we're creating a new asymmetric JWT signing key for you to rotate to. This key starts off as standby key -- meaning it's being advertised as a key that Supabase Auth will use in the future to create JWTs.
If you're not ready to switch away from the legacy JWT secret right now, you can stop here without any issue. If you wish to use a different signing key -- either to use a different signing algorithm (RSA, Elliptic Curve or shared secret) or to import a private key or shared secret you already have -- feel free to move the standby key to Previously used before finally moving it to Revoked.
If you do wish to start using the standby key for all new JWT use the Rotate keys button. A few important notes:
Make sure your app does not directly rely on the legacy JWT secret. If it's verifying every JWT against the legacy JWT secret (using a library like jose, jsonwebtoken or similar), continuing with the rotation might break those components.
If you're using Edge Functions that have the Verify JWT setting, continuing with the rotation might break your app. You will need to turn off this setting.
In both cases, change or add code to your app or Edge Function that verifies the JWT. Use the supabase.auth.getClaims() function or read more about Verifying a JWT from Supabase on the best way to do this.
Rotating the keys immediately causes the Auth server to issue new JWT access tokens for signed in users signed with the new key. Non-expired access tokens will remain to be accepted, so no users will be forcefully signed out.
Plan for revocation of the legacy JWT secret.
If your access token expiry time is configured to be 1 hour, wait at least 1 hour and 15 minutes before revoking the legacy JWT secret -- now under the Previously used section.
This prevents currently active users from being forcefully signed out.
In some situations, such as an active security incident you may want to revoke the legacy JWT secret immediately.
Rotating and revoking keys#
Key rotation and revocation are one of the most important processes for maintaining the security of your project and applications. The signing keys system allows you to efficiently execute these without causing downtime of your app, a deficiency present in the legacy system. Below are some common reasons when and why you should consider key rotation and revocation.

Malicious actors abusing the legacy JWT secret, or imported private key

The legacy JWT secret has been leaked in logs, committed to source control, or accidentally exposed in the frontend build of your application, a library, desktop or mobile app package, etc.
You suspect that a member of your organization has lost control of their devices, and a malicious actor may have accessed the JWT secret via the Supabase dashboard or by accessing your application's backend configuration.
You suspect that an ex-team-member of your organization may be a malicious actor, by abusing the power the legacy JWT secret provides.
Make sure you also switch to publishable and secret API keys and disable the anon and service_role keys.
If you've imported a private key, and you're suspecting that this private key has been compromised on your end similarly.
Closer alignment to security best practices and compliance frameworks (SOC2, PCI-DSS, ISO27000, HIPAA, ...)

It is always prudent to rotate signing keys at least once a year.
Some security compliance frameworks strongly encourage or require frequent cryptographic key rotation.
If you're using Supabase as part of a large enterprise, this may be required by your organization's security department.
Creating muscle memory for the time you'll need to respond to an active security incident.
Changing key algorithm for technical reasons

You may wish to switch signing algorithms due to compatibility problems or to simplify development on your end.
Lifetime of a signing key#
Diagram showing the state transitions of a signing key
A newly created key starts off as standby, before being rotated into in use (becoming the current key) while the existing current key becomes previously used.

At any point you can move a key from the previously used or revoked states back to being a standby key, and rotate to it. This gives you the confidence to revert back to an older key if you identify problems with the rotation, such as forgetting to update a component of your application that is relying on a specific key (for example, the legacy JWT secret).

Each action on a key is reversible (except permanent deletion).

Action	Accepted JWT signatures	Description
Create a new key	Current key only, new key has not created any JWTs yet.	When you initially create a key, after choosing the signing algorithm or importing a private key you already have, it starts out in the standby state. If using an asymmetric key (RSA, Elliptic Curve) its public key will be available in the discovery endpoint. Supabase Auth does not use this key to create new JWTs.
Rotate keys	Both keys in the rotation.	Rotation only changes the key used by Supabase Auth to create new JWTs, but the trust relationship with both keys remains.
Revoke key	Only from the current key.	Once all regularly valid JWTs have expired (or sooner) revoke the previously used key to revoke trust in it.
Move to standby from revoked	Current and previously revoked key.	If you've made a mistake or need more time to adjust your application, you can move a revoked key to standby. Follow up with a rotation to ensure Auth starts using the originally revoked key again to make new JWTs.
Move to standby from previously used	Both keys.	This only prepares the key from the last rotation to be used by Auth to make new JWTs with it.
Delete key	-	Permanently destroys the private key or shared secret of a key, so it will not be possible to re-use or rotate again into it.
Public key discovery and caching#
When your signing keys use an asymmetric algorithm based on public-key cryptography Supabase Auth exposes the public key in the JSON Web Key Set discovery endpoint, for anyone to see. This is an important security feature allowing you to rotate and revoke keys without needing to deploy new versions of your app's backend infrastructure.

Access the currently trusted signing keys at the following endpoint:

GET https://project-id.supabase.co/auth/v1/.well-known/jwks.json
Note that this is secure as public keys are irreversible and can only be used to verify the signature of JSON Web Tokens, but not create new ones.

This discovery endpoint is cached by Supabase's edge servers for 10 minutes. Furthermore the Supabase client libraries may cache the keys in memory for an additional 10 minutes. Your application may be using different caching behavior if you're not relying only on the Supabase client library.

This multi-level cache is a trade-off allowing fast JWT verification without placing the Auth server in the hot path of your application, increasing its reliability and performance.

Importantly Supabase products do not rely on this cache, so stronger security guarantees are provided especially when keys are revoked. If your application only uses Row Level Security policies and does not have any other backend components (such as APIs, Edge Functions, servers, etc.) key rotation and revocation are instantaneous.

Finally this multi-level cache is cleared every 20 minutes, or longer if you have a custom setup. Consider the following problems that may arise due to it:

Urgent key revocation. If you are in a security incident where a signing key must be urgently revoked, due to the multi-level cache your application components may still trust and authenticate JWTs signed with the revoked key. Supabase products (Auth, Data API, Storage, Realtime) do not rely on this cache and revocation is instantaneous. Should this be an issue for you, ensure you've built a cache busting mechanism as part of your app's backend infrastructure.
Quick key creation and rotation. If you're migrating away from the legacy JWT secret or when only using the supabase.auth.getClaims() method this case is handled for you automatically. If you're verifying JWTs on your own, without the help of the Supabase client library, ensure that all caches in your app have picked up the newly created standby key before proceeding to rotation.
Choosing the right signing algorithm#
To strike the right balance between performance, security and ease-of-use, JWT signing keys are based on capabilities available in the Web Crypto API.

Algorithm	JWT alg	Information
NIST P-256 Curve
(Asymmetric)	ES256	Elliptic Curves are a faster alternative than RSA, while providing comparable security. Especially important for Auth use cases is the fact that signatures using the P-256 curve are significantly shorter than those created by RSA, which reduces data transfer sizes and helps in managing cookie size. Web Crypto and most other cryptography libraries and runtimes support this curve.
RSA 2048
(Asymmetric)	RS256	RSA is the oldest and most widely supported public-key cryptosystem in use. While being easy to code by hand, it can be significantly slower than elliptic curves in certain aspects. We recommend using the P-256 elliptic curve instead.
Ed25519 Curve
(Asymmetric)	EdDSA	Coming soon. This algorithm is based on a different elliptic curve cryptosystem developed in the open, unlike the P-256 curve. Web Crypto or other crypto libraries may not support it in all runtimes, making it difficult to work with.
HMAC with shared secret
(Symmetric)	HS256	Not recommended for production applications. A shared secret uses a message authentication code to verify the authenticity of a JSON Web Token. This requires that both the creator of the JWT (Auth) and the system verifying the JWT know the secret. As there is no public key counterpart, revoking this key might require deploying changes to your app's backend infrastructure.
There is almost no benefit from using a JWT signed with a shared secret. Although it's computationally more efficient and verification is simpler to code by hand, using this approach can expose your project's data to significant security vulnerabilities or weaknesses.

Consider the following:

Using a shared secret can make it more difficult to keep aligned with security compliance frameworks such as SOC2, PCI-DSS, ISO27000, HIPAA, etc.
A shared secret that is in the hands of a malicious actor can be used to impersonate your users, give them access to privileged actions or data.
It is difficult to detect or identify when or how a shared secret has been given to a malicious actor.
Consider who might have even accidental access to the shared secret: systems, staff, devices (and their disk encryption and vulnerability patch status).
A malicious actor can use a shared secret far into the future, so lacking current evidence of compromise does not mean your data is secure.
It can be very easy to accidentally leak the shared secret in publicly available source code such as in your website or frontend, mobile app package or other executable. This is especially true if you accidentally add the secret in environment variables prefixed with NEXT_PUBLIC_, VITE_, PUBLIC_ or other conventions by web frameworks.
Rotating shared secrets might require careful coordination to avoid downtime of your app.
Frequently asked questions#
Why is it not possible to extract the private key or shared secret from Supabase?#
You can only extract the legacy JWT secret. Once you've moved to using the JWT signing keys feature extracting of the private key or shared secret from Supabase is not possible. This ensures that no one in your organization is able to impersonate your users or gain privileged access to your project's data.

This guarantee provides your application with close alignment with security compliance frameworks (SOC2, PCI-DSS, ISO27000, HIPAA) and security best practices.

How to create (mint) JWTs if access to the private key or shared secret is not possible?#
If you wish to make your own JWTs or have access to the private key or shared secret used by Supabase, you can create a new JWT signing key by importing a private key or setting a shared secret yourself.

Use the Supabase CLI to quickly and securely generate a private key ready for import:

supabase gen signing-key --algorithm ES256
Make sure you store this private key in a secure location, as it will not be extractable from Supabase.

To import the generated private key to your project, create a new standby key from the dashboard:

{
  "kty": "EC",
  "kid": "3a18cfe2-7226-43b0-bbb4-7c5242f2406e",
  "d": "RDbwqThwtGP4WnvACvO_0nL0oMMSmMFSYMPosprlAog",
  "crv": "P-256",
  "x": "gyLVvp9dyEgylYH7nR2E2qdQ_-9Pv5i1tk7c2qZD4Nk",
  "y": "CD9RfYOTyjR5U-PC9UDlsthRpc7vAQQQ2FTt8UsX0fY"
}
Once imported, click Rotate key to activate your new signing key. Any JWT signed by your old key will continue to be usable until your old signing key is manually revoked.

To mint a new JWT using the asymmetric signing key, you need to set the following JWT headers to match your generated private key.

{
  "alg": "ES256",
  "kid": "3a18cfe2-7226-43b0-bbb4-7c5242f2406e",
  "typ": "JWT"
}
The kid header is used to identify your public key for verification. You must use the same value when importing on platform.

In addition, you need to provide the following custom claims as the JWT payload.

{
  "sub": "ef0493c9-3582-425f-a362-aef909588df7",
  "role": "authenticated",
  "exp": 1757749466
}
sub is an optional UUID that uniquely identifies a user you want to impersonate in auth.users table.
role must be set to an existing Postgres role in your database, such as anon, authenticated, or service_role.
exp must be set to a timestamp in the future (seconds since 1970) when this token expires. Prefer shorter-lived tokens.
For simplicity, use the following CLI command to generate tokens with the desired header and payload.

supabase gen bearer-jwt --role authenticated --sub ef0493c9-3582-425f-a362-aef909588df7
Finally, you can use your newly minted JWT by setting the Authorization: Bearer <JWT> header to all Data API requests.

A separate apikey header is required to access your project's APIs. This can be a publishable, secret or the legacy anon or service_role keys. Using your minted JWT is not possible in this header.

Why is a 5 minute wait imposed when changing signing key states?#
Changing a JWT signing key's state sets off many changes inside the Supabase platform. To ensure a consistent setup, most actions that change the state of a JWT signing key are throttled for approximately 5 minutes.

Why is deleting the legacy JWT secret disallowed?#
This is to ensure you have the ability, should you need it, to go back to the legacy JWT secret. In the future this capability will be allowed from the dashboard.

Why does revoking the legacy JWT secret require disabling of anon and service_role API keys?#
Unfortunately anon and service_role are not just API keys, but are also valid JSON Web Tokens, signed by the legacy JWT secret. Revoking the legacy JWT secret means that your application no longer trusts any JWT signed with it. Therefore before you revoke the legacy JWT secret, you must disable the anon and service_role to ensure a consistent security setup.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Overview
Benefits of the signing keys system
Getting started
Rotating and revoking keys
Lifetime of a signing key
Public key discovery and caching
Choosing the right signing algorithm
Frequently asked questions
Why is it not possible to extract the private key or shared secret from Supabase?
How to create (mint) JWTs if access to the private key or shared secret is not possible?
Why is a 5 minute wait imposed when changing signing key states?
Why is deleting the legacy JWT secret disallowed?
Why does revoking the legacy JWT secret require disabling of anon and service_role API keys?
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Autho

Row Level Security

Secure your data using Postgres Row Level Security.

When you need granular authorization rules, nothing beats Postgres's Row Level Security (RLS).

Row Level Security in Supabase#
Supabase allows convenient and secure data access from the browser, as long as you enable RLS.

RLS must always be enabled on any tables stored in an exposed schema. By default, this is the public schema.

RLS is enabled by default on tables created with the Table Editor in the dashboard. If you create one in raw SQL or with the SQL editor, remember to enable RLS yourself:

alter table <schema_name>.<table_name>
enable row level security;
RLS is incredibly powerful and flexible, allowing you to write complex SQL rules that fit your unique business needs. RLS can be combined with Supabase Auth for end-to-end user security from the browser to the database.

RLS is a Postgres primitive and can provide "defense in depth" to protect your data from malicious actors even when accessed through third-party tooling.

Policies#
Policies are Postgres's rule engine. Policies are easy to understand once you get the hang of them. Each policy is attached to a table, and the policy is executed every time a table is accessed.

You can just think of them as adding a WHERE clause to every query. For example a policy like this ...

create policy "Individuals can view their own todos."
on todos for select
using ( (select auth.uid()) = user_id );
.. would translate to this whenever a user tries to select from the todos table:

select *
from todos
where auth.uid() = todos.user_id;
-- Policy is implicitly added.
Enabling Row Level Security#
You can enable RLS for any table using the enable row level security clause:

alter table "table_name" enable row level security;
Once you have enabled RLS, no data will be accessible via the API when using the public anon key, until you create policies.

`auth.uid()` Returns `null` When Unauthenticated
When a request is made without an authenticated user (e.g., no access token is provided or the session has expired), auth.uid() returns null.

This means that a policy like:

USING (auth.uid() = user_id)
will silently fail for unauthenticated users, because:

null = user_id
is always false in SQL.

To avoid confusion and make your intention clear, we recommend explicitly checking for authentication:

USING (auth.uid() IS NOT NULL AND auth.uid() = user_id)
Authenticated and unauthenticated roles#
Supabase maps every request to one of the roles:

anon: an unauthenticated request (the user is not logged in)
authenticated: an authenticated request (the user is logged in)
These are actually Postgres Roles. You can use these roles within your Policies using the TO clause:

create policy "Profiles are viewable by everyone"
on profiles for select
to authenticated, anon
using ( true );
-- OR
create policy "Public profiles are viewable only by authenticated users"
on profiles for select
to authenticated
using ( true );
Anonymous user vs the anon key
Using the anon Postgres role is different from an anonymous user in Supabase Auth. An anonymous user assumes the authenticated role to access the database and can be differentiated from a permanent user by checking the is_anonymous claim in the JWT.

Creating policies#
Policies are SQL logic that you attach to a Postgres table. You can attach as many policies as you want to each table.

Supabase provides some helpers that simplify RLS if you're using Supabase Auth. We'll use these helpers to illustrate some basic policies:

SELECT policies#
You can specify select policies with the using clause.

Let's say you have a table called profiles in the public schema and you want to enable read access to everyone.

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Public profiles are visible to everyone."
on profiles for select
to anon         -- the Postgres Role (recommended)
using ( true ); -- the actual Policy
Alternatively, if you only wanted users to be able to see their own profiles:

create policy "User can see their own profile only."
on profiles
for select using ( (select auth.uid()) = user_id );
INSERT policies#
You can specify insert policies with the with check clause. The with check expression ensures that any new row data adheres to the policy constraints.

Let's say you have a table called profiles in the public schema and you only want users to be able to create a profile for themselves. In that case, we want to check their User ID matches the value that they are trying to insert:

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can create a profile."
on profiles for insert
to authenticated                          -- the Postgres Role (recommended)
with check ( (select auth.uid()) = user_id );      -- the actual Policy
UPDATE policies#
You can specify update policies by combining both the using and with check expressions.

The using clause represents the condition that must be true for the update to be allowed, and with check clause ensures that the updates made adhere to the policy constraints.

Let's say you have a table called profiles in the public schema and you only want users to be able to update their own profile.

You can create a policy where the using clause checks if the user owns the profile being updated. And the with check clause ensures that, in the resultant row, users do not change the user_id to a value that is not equal to their User ID, maintaining that the modified profile still meets the ownership condition.

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can update their own profile."
on profiles for update
to authenticated                    -- the Postgres Role (recommended)
using ( (select auth.uid()) = user_id )       -- checks if the existing row complies with the policy expression
with check ( (select auth.uid()) = user_id ); -- checks if the new row complies with the policy expression
If no with check expression is defined, then the using expression will be used both to determine which rows are visible (normal USING case) and which new rows will be allowed to be added (WITH CHECK case).

To perform an UPDATE operation, a corresponding SELECT policy is required. Without a SELECT policy, the UPDATE operation will not work as expected.

DELETE policies#
You can specify delete policies with the using clause.

Let's say you have a table called profiles in the public schema and you only want users to be able to delete their own profile:

-- 1. Create table
create table profiles (
  id uuid primary key,
  user_id uuid references auth.users,
  avatar_url text
);
-- 2. Enable RLS
alter table profiles enable row level security;
-- 3. Create Policy
create policy "Users can delete a profile."
on profiles for delete
to authenticated                     -- the Postgres Role (recommended)
using ( (select auth.uid()) = user_id );      -- the actual Policy
Views#
Views bypass RLS by default because they are usually created with the postgres user. This is a feature of Postgres, which automatically creates views with security definer.

In Postgres 15 and above, you can make a view obey the RLS policies of the underlying tables when invoked by anon and authenticated roles by setting security_invoker = true.

create view <VIEW_NAME>
with(security_invoker = true)
as select <QUERY>
In older versions of Postgres, protect your views by revoking access from the anon and authenticated roles, or by putting them in an unexposed schema.

Helper functions#
Supabase provides some helper functions that make it easier to write Policies.

auth.uid()#
Returns the ID of the user making the request.

auth.jwt()#
Not all information present in the JWT should be used in RLS policies. For instance, creating an RLS policy that relies on the user_metadata claim can create security issues in your application as this information can be modified by authenticated end users.

Returns the JWT of the user making the request. Anything that you store in the user's raw_app_meta_data column or the raw_user_meta_data column will be accessible using this function. It's important to know the distinction between these two:

raw_user_meta_data - can be updated by the authenticated user using the supabase.auth.update() function. It is not a good place to store authorization data.
raw_app_meta_data - cannot be updated by the user, so it's a good place to store authorization data.
The auth.jwt() function is extremely versatile. For example, if you store some team data inside app_metadata, you can use it to determine whether a particular user belongs to a team. For example, if this was an array of IDs:

create policy "User is in team"
on my_table
to authenticated
using ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));
Keep in mind that a JWT is not always "fresh". In the example above, even if you remove a user from a team and update the app_metadata field, that will not be reflected using auth.jwt() until the user's JWT is refreshed.

Also, if you are using Cookies for Auth, then you must be mindful of the JWT size. Some browsers are limited to 4096 bytes for each cookie, and so the total size of your JWT should be small enough to fit inside this limitation.

MFA#
The auth.jwt() function can be used to check for Multi-Factor Authentication. For example, you could restrict a user from updating their profile unless they have at least 2 levels of authentication (Assurance Level 2):

create policy "Restrict updates."
on profiles
as restrictive
for update
to authenticated using (
  (select auth.jwt()->>'aal') = 'aal2'
);
Bypassing Row Level Security#
Supabase provides special "Service" keys, which can be used to bypass RLS. These should never be used in the browser or exposed to customers, but they are useful for administrative tasks.

Supabase will adhere to the RLS policy of the signed-in user, even if the client library is initialized with a Service Key.

You can also create new Postgres Roles which can bypass Row Level Security using the "bypass RLS" privilege:

alter role "role_name" with bypassrls;
This can be useful for system-level access. You should never share login credentials for any Postgres Role with this privilege.

RLS performance recommendations#
Every authorization system has an impact on performance. While row level security is powerful, the performance impact is important to keep in mind. This is especially true for queries that scan every row in a table - like many select operations, including those using limit, offset, and ordering.

Based on a series of tests, we have a few recommendations for RLS:

Add indexes#
Make sure you've added indexes on any columns used within the Policies which are not already indexed (or primary keys). For a Policy like this:

create policy "rls_test_select" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
You can add an index like:

create index userid
on test_table
using btree (user_id);
Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test1-indexed	171	< 0.1	99.94%	
Detalles
Call functions with select#
You can use select statement to improve policies that use functions. For example, instead of this:

create policy "rls_test_select" on test_table
to authenticated
using ( auth.uid() = user_id );
You can do:

create policy "rls_test_select" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
This method works well for JWT functions like auth.uid() and auth.jwt() as well as security definer Functions. Wrapping the function causes an initPlan to be run by the Postgres optimizer, which allows it to "cache" the results per-statement, rather than calling the function on each row.

You can only use this technique if the results of the query or function do not change based on the row data.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test2a-wrappedSQL-uid	179	9	94.97%	
Detalles
test2b-wrappedSQL-isadmin	11,000	7	99.94%	
Detalles
test2c-wrappedSQL-two-functions	11,000	10	99.91%	
Detalles
test2d-wrappedSQL-sd-fun	178,000	12	99.993%	
Detalles
test2e-wrappedSQL-sd-fun-array	173000	16	99.991%	
Detalles
Add filters to every query#
Policies are "implicit where clauses," so it's common to run select statements without any filters. This is a bad pattern for performance. Instead of doing this (JS client example):

const { data } = supabase
  .from('table')
  .select()
You should always add a filter:

const { data } = supabase
  .from('table')
  .select()
  .eq('user_id', userId)
Even though this duplicates the contents of the Policy, Postgres can use the filter to construct a better query plan.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test3-addfilter	171	9	94.74%	
Detalles
Use security definer functions#
A "security definer" function runs using the same role that created the function. This means that if you create a role with a superuser (like postgres), then that function will have bypassrls privileges. For example, if you had a policy like this:

create policy "rls_test_select" on test_table
to authenticated
using (
  exists (
    select 1 from roles_table
    where (select auth.uid()) = user_id and role = 'good_role'
  )
);
We can instead create a security definer function which can scan roles_table without any RLS penalties:

create function private.has_good_role()
returns boolean
language plpgsql
security definer -- will run as the creator
as $$
begin
  return exists (
    select 1 from roles_table
    where (select auth.uid()) = user_id and role = 'good_role'
  );
end;
$$;
-- Update our policy to use this function:
create policy "rls_test_select"
on test_table
to authenticated
using ( (select private.has_good_role()) );
Security-definer functions should never be created in a schema in the "Exposed schemas" inside your API settings`.

Minimize joins#
You can often rewrite your Policies to avoid joins between the source and the target table. Instead, try to organize your policy to fetch all the relevant data from the target table into an array or set, then you can use an IN or ANY operation in your filter.

For example, this is an example of a slow policy which joins the source test_table to the target team_user:

create policy "rls_test_select" on test_table
to authenticated
using (
  (select auth.uid()) in (
    select user_id
    from team_user
    where team_user.team_id = team_id -- joins to the source "test_table.team_id"
  )
);
We can rewrite this to avoid this join, and instead select the filter criteria into a set:

create policy "rls_test_select" on test_table
to authenticated
using (
  team_id in (
    select team_id
    from team_user
    where user_id = (select auth.uid()) -- no join
  )
);
In this case you can also consider using a security definer function to bypass RLS on the join table:

If the list exceeds 1000 items, a different approach may be needed or you may need to analyze the approach to ensure that the performance is acceptable.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test5-fixed-join	9,000	20	99.78%	
Detalles
Specify roles in your policies#
Always use the Role of inside your policies, specified by the TO operator. For example, instead of this query:

create policy "rls_test_select" on rls_test
using ( auth.uid() = user_id );
Use:

create policy "rls_test_select" on rls_test
to authenticated
using ( (select auth.uid()) = user_id );
This prevents the policy ( (select auth.uid()) = user_id ) from running for any anon users, since the execution stops at the to authenticated step.

Benchmarks#
Test	Before (ms)	After (ms)	% Improvement	Change
test6-To-role	170	< 0.1	99.78%	
Detalles
More resources#
Testing your database
Row Level Security and Supabase Auth
RLS Guide and Best Practices
Community repo on testing RLS using pgTAP and dbdev
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Row Level Security in Supabase
Policies
Enabling Row Level Security
Authenticated and unauthenticated roles
Creating policies
SELECT policies
INSERT policies
UPDATE policies
DELETE policies
Views
Helper functions
auth.uid()
auth.jwt()
MFA
Bypassing Row Level Security
RLS performance recommendations
Add indexes
Call functions with select
Add filters to every query
Use security definer functions
Minimize joins
Specify roles in your policies
More resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing

Column Level Security

PostgreSQL's Row Level Security (RLS) gives you granular control over who can access rows of data. However, it doesn't give you control over which columns they can access within rows. Sometimes you want to restrict access to specific columns in your database. Column Level Privileges allows you to do just that.

This is an advanced feature. We do not recommend using column-level privileges for most users. Instead, we recommend using RLS policies in combination with a dedicated table for handling user roles.

Restricted roles cannot use the wildcard operator (*) on the affected table. Instead of using SELECT * FROM <restricted_table>; or its API equivalent, you must specify the column names explicitly.

Policies at the row level#
Policies in Row Level Security (RLS) are used to restrict access to rows in a table. Think of them like adding a WHERE clause to every query.

For example, let's assume you have a posts table with the following columns:

id
user_id
title
content
created_at
updated_at
You can restrict updates to just the user who created it using RLS, with the following policy:

create policy "Allow update for owners" on posts for
update
  using ((select auth.uid()) = user_id);
However, this gives the post owner full access to update the row, including all of the columns.

Privileges at the column level#
To restrict access to columns, you can use Privileges.

There are two types of privileges in Postgres:

table-level: Grants the privilege on all columns in the table.
column-level Grants the privilege on a specific column in the table.
You can have both types of privileges on the same table. If you have both, and you revoke the column-level privilege, the table-level privilege will still be in effect.

By default, our table will have a table-level UPDATE privilege, which means that the authenticated role can update all the columns in the table.

revoke
update
  on table public.posts
from
  authenticated;
grant
update
  (title, content) on table public.posts to authenticated;
In the above example, we are revoking the table-level UPDATE privilege from the authenticated role and granting a column-level UPDATE privilege on just the title and content columns.

If we want to restrict access to updating the title column:

revoke
update
  (title) on table public.posts
from
  authenticated;
This time, we are revoking the column-level UPDATE privilege of the title column from the authenticated role. We didn't need to revoke the table-level UPDATE privilege because it's already revoked.

Manage column privileges in the Dashboard#
Column-level privileges are a powerful tool, but they're also quite advanced and in many cases, not the best fit for common access control needs. For that reason, we've intentionally moved the UI for this feature under the Feature Preview section in the dashboard.

You can view and edit the privileges in the Supabase Studio.

Column level privileges

Manage column privileges in migrations#
While you can manage privileges directly from the Dashboard, as your project grows you may want to manage them in your migrations. Read about database migrations in the Local Development guide.

1
Create a migration file
To get started, generate a new migration to store the SQL needed to create your table along with row and column-level privileges.

supabase migration new create_posts_table
2
Add the SQL to your migration file
This creates a new migration: supabase/migrations/<timestamp>
_create_posts_table.sql.

To that file, add the SQL to create this posts table with row and column-level privileges.

create table
posts (
id bigint primary key generated always as identity,
user_id text,
title text,
content text,
created_at timestamptz default now()
updated_at timestamptz default now()
);
-- Add row-level security
create policy "Allow update for owners" on posts for
update
using ((select auth.uid()) = user_id);
-- Add column-level security
revoke
update
(title) on table public.posts
from
authenticated;
Considerations when using column-level privileges#
If you turn off a column privilege you won't be able to use that column at all.
All operations (insert, update, delete) as well as using select * will fail.
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Policies at the row level
Privileges at the column level
Manage column privileges in the Dashboard
Manage column privileges in migrations
Considerations when using column-level privileges
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Au

Custom Claims & Role-based Access Control (RBAC)

Custom Claims are special attributes attached to a user that you can use to control access to portions of your application. For example:

{
  "user_role": "admin",
  "plan": "TRIAL",
  "user_level": 100,
  "group_name": "Super Guild!",
  "joined_on": "2022-05-20T14:28:18.217Z",
  "group_manager": false,
  "items": ["toothpick", "string", "ring"]
}
To implement Role-Based Access Control (RBAC) with custom claims, use a Custom Access Token Auth Hook. This hook runs before a token is issued. You can use it to add additional claims to the user's JWT.

This guide uses the Slack Clone example to demonstrate how to add a user_role claim and use it in your Row Level Security (RLS) policies.

Create a table to track user roles and permissions#
In this example, you will implement two user roles with specific permissions:

moderator: A moderator can delete all messages but not channels.
admin: An admin can delete all messages and channels.
-- Custom types
create type public.app_permission as enum ('channels.delete', 'messages.delete');
create type public.app_role as enum ('admin', 'moderator');
-- USER ROLES
create table public.user_roles (
  id        bigint generated by default as identity primary key,
  user_id   uuid references auth.users on delete cascade not null,
  role      app_role not null,
  unique (user_id, role)
);
comment on table public.user_roles is 'Application roles for each user.';
-- ROLE PERMISSIONS
create table public.role_permissions (
  id           bigint generated by default as identity primary key,
  role         app_role not null,
  permission   app_permission not null,
  unique (role, permission)
);
comment on table public.role_permissions is 'Application permissions for each role.';
For the full schema, see the example application on GitHub.

You can now manage your roles and permissions in SQL. For example, to add the mentioned roles and permissions from above, run:

insert into public.role_permissions (role, permission)
values
  ('admin', 'channels.delete'),
  ('admin', 'messages.delete'),
  ('moderator', 'messages.delete');
Create Auth Hook to apply user role#
The Custom Access Token Auth Hook runs before a token is issued. You can use it to edit the JWT.


PL/pgSQL (best performance)
-- Create the auth hook function
create or replace function public.custom_access_token_hook(event jsonb)
returns jsonb
language plpgsql
stable
as $$
  declare
    claims jsonb;
    user_role public.app_role;
  begin
    -- Fetch the user role in the user_roles table
    select role into user_role from public.user_roles where user_id = (event->>'user_id')::uuid;
    claims := event->'claims';
    if user_role is not null then
      -- Set the claim
      claims := jsonb_set(claims, '{user_role}', to_jsonb(user_role));
    else
      claims := jsonb_set(claims, '{user_role}', 'null');
    end if;
    -- Update the 'claims' object in the original event
    event := jsonb_set(event, '{claims}', claims);
    -- Return the modified or original event
    return event;
  end;
$$;
grant usage on schema public to supabase_auth_admin;
grant execute
  on function public.custom_access_token_hook
  to supabase_auth_admin;
revoke execute
  on function public.custom_access_token_hook
  from authenticated, anon, public;
grant all
  on table public.user_roles
to supabase_auth_admin;
revoke all
  on table public.user_roles
  from authenticated, anon, public;
create policy "Allow auth admin to read user roles" ON public.user_roles
as permissive for select
to supabase_auth_admin
using (true)
Enable the hook#
In the dashboard, navigate to Authentication > Hooks (Beta) and select the appropriate Postgres function from the dropdown menu.

When developing locally, follow the local development instructions.

To learn more about Auth Hooks, see the Auth Hooks docs.

Accessing custom claims in RLS policies#
To utilize Role-Based Access Control (RBAC) in Row Level Security (RLS) policies, create an authorize method that reads the user's role from their JWT and checks the role's permissions:

create or replace function public.authorize(
  requested_permission app_permission
)
returns boolean as $$
declare
  bind_permissions int;
  user_role public.app_role;
begin
  -- Fetch user role once and store it to reduce number of calls
  select (auth.jwt() ->> 'user_role')::public.app_role into user_role;
  select count(*)
  into bind_permissions
  from public.role_permissions
  where role_permissions.permission = requested_permission
    and role_permissions.role = user_role;
  return bind_permissions > 0;
end;
$$ language plpgsql stable security definer set search_path = '';
You can read more about using functions in RLS policies in the RLS guide.

You can then use the authorize method within your RLS policies. For example, to enable the desired delete access, you would add the following policies:

create policy "Allow authorized delete access" on public.channels for delete to authenticated using ( (SELECT authorize('channels.delete')) );
create policy "Allow authorized delete access" on public.messages for delete to authenticated using ( (SELECT authorize('messages.delete')) );
Accessing custom claims in your application#
The auth hook will only modify the access token JWT but not the auth response. Therefore, to access the custom claims in your application, e.g. your browser client, or server-side middleware, you will need to decode the access_token JWT on the auth session.

In a JavaScript client application you can for example use the jwt-decode package:

import { jwtDecode } from 'jwt-decode'
const { subscription: authListener } = supabase.auth.onAuthStateChange(async (event, session) => {
  if (session) {
    const jwt = jwtDecode(session.access_token)
    const userRole = jwt.user_role
  }
})
For server-side logic you can use packages like express-jwt, koa-jwt, PyJWT, dart_jsonwebtoken, Microsoft.AspNetCore.Authentication.JwtBearer, etc.

Conclusion#
You now have a robust system in place to manage user roles and permissions within your database that automatically propagates to Supabase Auth.

More resources#
Auth Hooks
Row Level Security
RLS Functions
Next.js Slack Clone Example
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Create a table to track user roles and permissions
Create Auth Hook to apply user role
Enable the hook
Accessing custom claims in RLS policies
Accessing custom claims in your application
Conclusion
More resources
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
C

Auth UI

As of 7th Feb 2024, this repository is no longer maintained by the Supabase Team. At the moment, the team does not have capacity to give the expected level of care to this repository. We may revisit Auth UI in the future but regrettably have to leave it on hold for now as we focus on other priorities such as improving the Server-Side Rendering (SSR) package and advanced Auth primitives.

As an alternative you can use the Supabase UI Library which has auth ready blocks to use in your projects.

Auth UI is a pre-built React component for authenticating users.
It supports custom themes and extensible styles to match your brand and aesthetic.

Set up Auth UI#
Install the latest version of supabase-js and the Auth UI package:

npm install @supabase/supabase-js @supabase/auth-ui-react @supabase/auth-ui-shared
Import the Auth component#
Pass supabaseClient from @supabase/supabase-js as a prop to the component.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => <Auth supabaseClient={supabase} />
This renders the Auth component without any styling.
We recommend using one of the predefined themes to style the UI.
Import the theme you want to use and pass it to the appearance.theme prop.

import { Auth } from '@supabase/auth-ui-react'
import {
  // Import predefined theme
  ThemeSupa,
} from '@supabase/auth-ui-shared'
const supabase = createClient(
  '<INSERT PROJECT URL>',
  '<INSERT PROJECT ANON API KEY>'
)
const App = () => (
  <Auth
    supabaseClient={supabase}
    {/* Apply predefined theme */}
    appearance={{ theme: ThemeSupa }}
  />
)
Social providers#
The Auth component also supports login with official social providers.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
import { ThemeSupa } from '@supabase/auth-ui-shared'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => (
  <Auth
    supabaseClient={supabase}
    appearance={{ theme: ThemeSupa }}
    providers={['google', 'facebook', 'twitter']}
  />
)
Options#
Options are available via queryParams:

<Auth
  supabaseClient={supabase}
  providers={['google']}
  queryParams={{
    access_type: 'offline',
    prompt: 'consent',
    hd: 'domain.com',
  }}
  onlyThirdPartyProviders
/>
Provider scopes#
Provider Scopes can be requested through providerScope;

<Auth
  supabaseClient={supabase}
  providers={['google']}
  queryParams={{
    access_type: 'offline',
    prompt: 'consent',
    hd: 'domain.com',
  }}
  providerScopes={{
    google: 'https://www.googleapis.com/auth/calendar.readonly',
  }}
/>
Supported views#
The Auth component is currently shipped with the following views:

Email Login
Magic Link login
Social Login
Update password
Forgotten password
We are planning on adding more views in the future. Follow along on that repo.

Customization#
There are several ways to customize Auth UI:

Use one of the predefined themes that comes with Auth UI
Extend a theme by overriding the variable tokens in a theme
Create your own theme
Use your own CSS classes
Use inline styles
Use your own labels
Predefined themes#
Auth UI comes with several themes to customize the appearance. Each predefined theme comes with at least two variations, a default variation, and a dark variation. You can switch between these themes using the theme prop. Import the theme you want to use and pass it to the appearance.theme prop.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
import { ThemeSupa } from '@supabase/auth-ui-shared'
const supabase = createClient(
  '<INSERT PROJECT URL>',
  '<INSERT PROJECT ANON API KEY>'
)
const App = () => (
  <Auth
    supabaseClient={supabase}
    {/* Apply predefined theme */}
    appearance={{ theme: ThemeSupa }}
  />
)
Currently there is only one predefined theme available, but we plan to add more.

Switch theme variations#
Auth UI comes with two theme variations: default and dark. You can switch between these themes with the theme prop.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
import { ThemeSupa } from '@supabase/auth-ui-shared'
const supabase = createClient(
  '<INSERT PROJECT URL>',
  '<INSERT PROJECT ANON API KEY>'
)
const App = () => (
  <Auth
    supabaseClient={supabase}
    appearance={{ theme: ThemeSupa }}
    {/* Set theme to dark */}
    theme="dark"
  />
)
If you don't pass a value to theme it uses the "default" theme. You can pass "dark" to the theme prop to switch to the dark theme. If your theme has other variations, use the name of the variation in this prop.

Override themes#
Auth UI themes can be overridden using variable tokens. See the list of variable tokens.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
import { ThemeSupa } from '@supabase/auth-ui-shared'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => (
  <Auth
    supabaseClient={supabase}
    appearance={{
      theme: ThemeSupa,
      variables: {
        default: {
          colors: {
            brand: 'red',
            brandAccent: 'darkred',
          },
        },
      },
    }}
  />
)
If you created your own theme, you may not need to override any of them.

Create your own theme #
You can create your own theme by following the same structure within a appearance.theme property.
See the list of tokens within a theme.

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const customTheme = {
  default: {
    colors: {
      brand: 'hsl(153 60.0% 53.0%)',
      brandAccent: 'hsl(154 54.8% 45.1%)',
      brandButtonText: 'white',
      // ..
    },
  },
  dark: {
    colors: {
      brandButtonText: 'white',
      defaultButtonBackground: '#2e2e2e',
      defaultButtonBackgroundHover: '#3e3e3e',
      //..
    },
  },
  // You can also add more theme variations with different names.
  evenDarker: {
    colors: {
      brandButtonText: 'white',
      defaultButtonBackground: '#1e1e1e',
      defaultButtonBackgroundHover: '#2e2e2e',
      //..
    },
  },
}
const App = () => (
  <Auth
    supabaseClient={supabase}
    theme="default" // can also be "dark" or "evenDarker"
    appearance={{ theme: customTheme }}
  />
)
You can switch between different variations of your theme with the "theme" prop.

Custom CSS classes #
You can use custom CSS classes for the following elements:
"button", "container", "anchor", "divider", "label", "input", "loader", "message".

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => (
  <Auth
    supabaseClient={supabase}
    appearance={{
      // If you want to extend the default styles instead of overriding it, set this to true
      extend: false,
      // Your custom classes
      className: {
        anchor: 'my-awesome-anchor',
        button: 'my-awesome-button',
        //..
      },
    }}
  />
)
Custom inline CSS #
You can use custom CSS inline styles for the following elements:
"button", "container", "anchor", "divider", "label", "input", "loader", "message".

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => (
  <Auth
    supabaseClient={supabase}
    appearance={{
      style: {
        button: { background: 'red', color: 'white' },
        anchor: { color: 'blue' },
        //..
      },
    }}
  />
)
Custom labels #
You can use custom labels with localization.variables like so:

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => (
  <Auth
    supabaseClient={supabase}
    localization={{
      variables: {
        sign_in: {
          email_label: 'Your email address',
          password_label: 'Your strong password',
        },
      },
    }}
  />
)
A full list of the available variables is below:


Sign Up

Sign In

Magic Link

Forgotten Password

Update Password

Verify OTP
Label Tag	Default Label
email_label	Email address
password_label	Create a Password
email_input_placeholder	Your email address
password_input_placeholder	Your password
button_label	Sign up
loading_button_label	Signing up ...
social_provider_text	Sign in with {{provider}}
link_text	Don't have an account? Sign up
confirmation_text	Check your email for the confirmation link
Currently, translating error messages (e.g. "Invalid credentials") is not supported. Check related issue.

Hiding links #
You can hide links by setting the showLinks prop to false

import { createClient } from '@supabase/supabase-js'
import { Auth } from '@supabase/auth-ui-react'
const supabase = createClient('<INSERT PROJECT URL>', '<INSERT PROJECT ANON API KEY>')
const App = () => <Auth supabaseClient={supabase} showLinks={false} />
Setting showLinks to false will hide the following links:

Don't have an account? Sign up
Already have an account? Sign in
Send a magic link email
Forgot your password?
Sign in and sign up views#
Add sign_in or sign_up views with the view prop:

<Auth
  supabaseClient={supabase}
  view="sign_up"
/>
Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Set up Auth UI
Import the Auth component
Social providers
Options
Provider scopes
Supported views
Customization
Predefined themes
Switch theme variations
Override themes
Create your own theme
Custom CSS classes
Custom inline CSS
Custom labels
Hiding links
Sign in and sign up views
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Au

Auth
Auth UI
Flutter Auth UI
Flutter Auth UI

Flutter Auth UI is a Flutter package containing pre-built widgets for authenticating users.
It is unstyled and can match your brand and aesthetic.

Flutter Auth UI

Add Flutter Auth UI#
Add the latest version of the package supabase-auth-ui to pubspec.yaml:

flutter pub add supabase_auth_ui
Initialize the Flutter Auth package#
import 'package:flutter/material.dart';
import 'package:supabase_auth_ui/supabase_auth_ui.dart';
void main() async {
  await Supabase.initialize(
    url: dotenv.get('SUPABASE_URL'),
    anonKey: dotenv.get('SUPABASE_PUBLISHABLE_KEY'),
  );
  runApp(const MyApp());
}
Email Auth#
Use a SupaEmailAuth widget to create an email and password signin and signup form. It also contains a button to toggle to display a forgot password form.

You can pass metadataFields to add additional fields to the form to pass as metadata to Supabase.

SupaEmailAuth(
  redirectTo: kIsWeb ? null : 'io.mydomain.myapp://callback',
  onSignInComplete: (response) {},
  onSignUpComplete: (response) {},
  metadataFields: [
    MetaDataField(
    prefixIcon: const Icon(Icons.person),
    label: 'Username',
    key: 'username',
    validator: (val) {
            if (val == null || val.isEmpty) {
            return 'Please enter something';
            }
            return null;
          },
        ),
    ],
)
Magic link Auth#
Use SupaMagicAuth widget to create a magic link signIn form.

SupaMagicAuth(
  redirectUrl: kIsWeb ? null : 'io.mydomain.myapp://callback',
  onSuccess: (Session response) {},
  onError: (error) {},
)
Reset password#
Use SupaResetPassword to create a password reset form.

SupaResetPassword(
  accessToken: supabase.auth.currentSession?.accessToken,
  onSuccess: (UserResponse response) {},
  onError: (error) {},
)
Phone Auth#
Use SupaPhoneAuth to create a phone authentication form.

SupaPhoneAuth(
  authAction: SupaAuthAction.signUp,
  onSuccess: (AuthResponse response) {},
),
Social Auth#
The package supports login with official social providers.

Use SupaSocialsAuth to create list of social login buttons.

SupaSocialsAuth(
  socialProviders: [
    OAuthProvider.apple,
    OAuthProvider.google,
  ],
  colored: true,
  redirectUrl: kIsWeb
    ? null
    : 'io.mydomain.myapp://callback',
  onSuccess: (Session response) {},
  onError: (error) {},
)
Theming#
This package uses plain Flutter components allowing you to control the appearance of the components using your own theme.

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Add Flutter Auth UI
Initialize the Flutter Auth package
Email Auth
Magic link Auth
Reset password
Phone Auth
Social Auth
Theming
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contributing
Au

Glossary

Definitions for terminology and acronyms used in the Supabase documentation.

Access token#
An access token is a short-lived (usually no more than 1 hour) token that authorizes a client to access resources on a server. It comes in the form of a JSON Web Token (JWT).

Authentication#
Authentication (often abbreviated authn.) is the process of verifying the identity of a user. Verification of the identity of a user can happen in multiple ways:

Asking users for something they know. For example: password, passphrase.
Checking that users have access to something they own. For example: an email address, a phone number, a hardware key, recovery codes.
Confirming that users have some biological features. For example: a fingerprint, a certain facial structure, an iris print.
Authenticator app#
An authenticator app generates time-based one-time passwords (TOTPs). These passwords are generated based off a long and difficult to guess secret string. The secret is initially passed to the application by scanning a QR code.

Authorization#
Authorization (often abbreviated authz.) is the process of verifying if a certain identity is allowed to access resources. Authorization often occurs by verifying an access token.

Identity provider#
An identity provider is software or service that allows third-party applications to identify users without the exchange of passwords. Social login and enterprise single-sign on won't be possible without identity providers.

Social login platforms typically use the OAuth protocol, while enterprise single-sign on is based on the OIDC or SAML protocols.

JSON Web Token (JWT)#
A JSON Web Token is a type of data structure, represented as a string, that usually contains identity and authorization information about a user. It encodes information about its lifetime and is signed with cryptographic key making it tamper resistant.

Access tokens are JWTs and by inspecting the information they contain you can allow or deny access to resources. Row level security policies are based on the information present in JWTs.

JWT signing secret#
JWTs issued by Supabase are signed using the HMAC-SHA256 algorithm. The secret key used in the signing is called the JWT signing secret. You should not share this secret with someone or some thing you don't trust, nor should you post it publicly. Anyone with access to the secret can create arbitrary JWTs.

Multi-factor authentication (MFA or 2FA)#
Multi-factor authentication is the process of authenticating a user's identity by using a combination of factors: something users know, something users have or something they are.

Nonce#
Nonce means number used once. In reality though, it is a unique and difficult to guess string used to either initialize a protocol or algorithm securely, or detect abuse in various forms of replay attacks.

OAuth#
OAuth is a protocol allowing third-party applications to request and receive authorization from their users. It is typically used to implement social login, and serves as a base for enterprise single-sign on in the OIDC protocol. Applications can request different levels of access, including basic user identification information such as name, email address, and user ID.

OIDC#
OIDC stands for OpenID Connect and is a protocol that enables single-sign on for enterprises. OIDC is based on modern web technologies such as OAuth and JSON Web Tokens. It is commonly used instead of the older SAML protocol.

One-time password (OTP)#
A one-time password is a short, randomly generated and difficult to guess password or code that is sent to a device (like a phone number) or generated by a device or application.

Password hashing function#
Password hashing functions are specially-designed algorithms that allow web servers to verify a password without storing it as-is. Unlike other difficult to guess strings generated from secure random number generators, passwords are picked by users and often are easy to guess by attackers. These algorithms slow down and make it very costly for attackers to guess passwords.

There are three generally accepted password hashing functions: Argon2, bcrypt and scrypt.

Password strength#
Password strength is a measurement of how difficult a password is to guess. Simple measurement includes calculating the number of possibilities given the types of characters used in the password. For example a password of only letters has fewer variations than ones with letters and digits. Better measurements include strategies such as looking for similarity to words, phrases or already known passwords.

PKCE#
Proof Key for Code Exchange is an extension to the OAuth protocol that enables secure exchange of refresh and access tokens between an application (web app, single-page app or mobile app) and the authorization server. It is used in places where the exchange of the refresh and access token may be intercepted by third parties such as other applications running in the operating system. This is a common problem on mobile devices where the operating system may hand out URLs to other applications. This can sometimes be also exploited in single-page apps too.

Provider refresh token#
A provider refresh token is a refresh token issued by a third-party identity provider which can be used to refresh the provider token returned.

Provider tokens#
A provider token is a long-lived token issued by a third-party identity provider. These are issued by social login services (e.g., Google, Twitter, Apple, Microsoft) and uniquely identify a user on those platforms.

Refresh token#
A refresh token is a long-lived (in most cases with an indefinite lifetime) token that is meant to be stored and exchanged for a new refresh and access tokens only once. Once a refresh token is exchanged it becomes invalid, and can't be exchanged again. In practice, though, a refresh token can be exchanged multiple times but in a short time window.

Refresh token flow#
The refresh token flow is a mechanism that issues a new refresh and access token on the basis of a valid refresh token. It is used to extend authorization access for an application. An application that is being constantly used will invoke the refresh token flow just before the access token expires.

Replay attack#
A replay attack is when sensitive information is stolen or intercepted by attackers who then attempt to use it again (thus replay) in an effort to compromise a system. Commonly replay attacks can be mitigated with the proper use of nonces.

Row level security policies (RLS)#
Row level security policies are special objects within the Postgres database that limit the available operations or data returned to clients. RLS policies use information contained in a JWT to identify users and the actions and data they are allowed to perform or view.

SAML#
SAML stands for Security Assertion Markup Language and is a protocol that enables single-sign on for enterprises. SAML was invented in the early 2000s and is based on XML technology. It is the de facto standard for enabling single-sign on for enterprises, although the more recent OIDC (OpenID Connect) protocol is gaining popularity.

Session#
A session or authentication session is the concept that binds a verified user identity to a web browser. A session usually is long-lived, and can be terminated by the user logging out. An access and refresh token pair represent a session in the browser, and they are stored in local storage or as cookies.

Single-sign on (SSO)#
Single-sign on allows enterprises to centrally manage accounts and access to applications. They use identity provider software or services to organize employee information in directories and connect those accounts with applications via OIDC or SAML protocols.

Time-based one-time password (TOTP)#
A time-based one-time password is a one-time password generated at regular time intervals from a secret, usually from an application in a mobile device (e.g., Google Authenticator, 1Password).

Edit this page on GitHub
Is this helpful?

No

Yes
On this page
Access token
Authentication
Authenticator app
Authorization
Identity provider
JSON Web Token (JWT)
JWT signing secret
Multi-factor authentication (MFA or 2FA)
Nonce
OAuth
OIDC
One-time password (OTP)
Password hashing function
Password strength
PKCE
Provider refresh token
Provider tokens
Refresh token
Refresh token flow
Replay attack
Row level security policies (RLS)
SAML
Session
Single-sign on (SSO)
Time-based one-time password (TOTP)
Need some help?

Contact support
Latest product updates?

See Changelog
Something's not right?

Check system status
© Supabase Inc
—
Contrib
